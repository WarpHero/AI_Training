{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DcYm7M96CfxT",
        "-FChiqMeFLki"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMuPgdkrQteyUTIOcjYHzY8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WarpHero/AI_Training/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow Tutorial\n",
        "https://www.tensorflow.org/tutorials/keras/regression?hl=ko"
      ],
      "metadata": {
        "id": "HOHsZmazDtM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MyOmP7sBjQS",
        "outputId": "ce588ff5-c084-498a-9612-ba07e0311b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자동차 연비 예측하기"
      ],
      "metadata": {
        "id": "D0chiUouCEhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 구하기\n",
        "- 먼저 데이터셋을 다운로드 하기"
      ],
      "metadata": {
        "id": "DcYm7M96CfxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
        "dataset_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4XkQQzNsB2mG",
        "outputId": "3df08ba4-3c70-4c9a-e860-fc20f093b9fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/auto-mpg.data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\" \", skipinitialspace=True)\n",
        "\n",
        "dataset = raw_dataset.copy()\n",
        "dataset.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_UP9ztr3DAIu",
        "outputId": "1ed5135a-2a8b-4d18-88ff-4af125c47cfb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
              "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
              "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
              "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
              "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
              "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
              "\n",
              "     Model Year  Origin  \n",
              "393          82       1  \n",
              "394          82       2  \n",
              "395          82       1  \n",
              "396          82       1  \n",
              "397          82       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ca89a15-3dfa-478c-90d1-214e9969db2c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>27.0</td>\n",
              "      <td>4</td>\n",
              "      <td>140.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>2790.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>44.0</td>\n",
              "      <td>4</td>\n",
              "      <td>97.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2130.0</td>\n",
              "      <td>24.6</td>\n",
              "      <td>82</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>32.0</td>\n",
              "      <td>4</td>\n",
              "      <td>135.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>2295.0</td>\n",
              "      <td>11.6</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>28.0</td>\n",
              "      <td>4</td>\n",
              "      <td>120.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>2625.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>31.0</td>\n",
              "      <td>4</td>\n",
              "      <td>119.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>2720.0</td>\n",
              "      <td>19.4</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ca89a15-3dfa-478c-90d1-214e9969db2c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ca89a15-3dfa-478c-90d1-214e9969db2c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ca89a15-3dfa-478c-90d1-214e9969db2c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EV3xMo0eD1q2",
        "outputId": "8eb37275-744b-4bf4-822c-84ac4869ceac"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
              "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
              "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
              "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
              "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
              "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
              "\n",
              "   Model Year  Origin  \n",
              "0          70       1  \n",
              "1          70       1  \n",
              "2          70       1  \n",
              "3          70       1  \n",
              "4          70       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38519ab8-fec0-4c02-84ed-7f0f9f54c3ab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38519ab8-fec0-4c02-84ed-7f0f9f54c3ab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-38519ab8-fec0-4c02-84ed-7f0f9f54c3ab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-38519ab8-fec0-4c02-84ed-7f0f9f54c3ab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkVclSYQEeo2",
        "outputId": "829651f7-6b90-45e3-c2be-99bd56c14d4d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "398"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "비정상 데이터 제거 dropna"
      ],
      "metadata": {
        "id": "vW3LJ2xzEqh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=dataset.dropna()"
      ],
      "metadata": {
        "id": "J1BerpewEj2t"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjCxZ1YpEpfF",
        "outputId": "ee32feb3-5d08-4695-c323-82bee10df6ab"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "392"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Origin data 제거\n",
        "- \"Origin\" 열은 수치형이 아니고 범주형이므로 원-핫 인코딩(one-hot encoding)으로 변환하겠습니다:"
      ],
      "metadata": {
        "id": "xB0Z9ZIwFCb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.pop('Origin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSwKwGENExa9",
        "outputId": "8d15c40f-e2c3-4fef-bf1d-0d5edff24d4b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "393    1\n",
              "394    2\n",
              "395    1\n",
              "396    1\n",
              "397    1\n",
              "Name: Origin, Length: 392, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wzy-Pi0zE7JY",
        "outputId": "cfabd3a4-cd27-4c3e-c442-517b75b5ac23"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
              "0    18.0          8         307.0       130.0  3504.0          12.0   \n",
              "1    15.0          8         350.0       165.0  3693.0          11.5   \n",
              "2    18.0          8         318.0       150.0  3436.0          11.0   \n",
              "3    16.0          8         304.0       150.0  3433.0          12.0   \n",
              "4    17.0          8         302.0       140.0  3449.0          10.5   \n",
              "..    ...        ...           ...         ...     ...           ...   \n",
              "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
              "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
              "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
              "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
              "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
              "\n",
              "     Model Year  \n",
              "0            70  \n",
              "1            70  \n",
              "2            70  \n",
              "3            70  \n",
              "4            70  \n",
              "..          ...  \n",
              "393          82  \n",
              "394          82  \n",
              "395          82  \n",
              "396          82  \n",
              "397          82  \n",
              "\n",
              "[392 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d5b8190-6160-4a66-9381-c6ed43d21a10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>27.0</td>\n",
              "      <td>4</td>\n",
              "      <td>140.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>2790.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>44.0</td>\n",
              "      <td>4</td>\n",
              "      <td>97.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2130.0</td>\n",
              "      <td>24.6</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>32.0</td>\n",
              "      <td>4</td>\n",
              "      <td>135.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>2295.0</td>\n",
              "      <td>11.6</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>28.0</td>\n",
              "      <td>4</td>\n",
              "      <td>120.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>2625.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>31.0</td>\n",
              "      <td>4</td>\n",
              "      <td>119.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>2720.0</td>\n",
              "      <td>19.4</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>392 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d5b8190-6160-4a66-9381-c6ed43d21a10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2d5b8190-6160-4a66-9381-c6ed43d21a10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2d5b8190-6160-4a66-9381-c6ed43d21a10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋을 훈련 세트와 테스트 세트로 분할\n",
        "- 이제 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "- 테스트 세트는 모델을 최종적으로 평가할 때 사용"
      ],
      "metadata": {
        "id": "-FChiqMeFLki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "metadata": {
        "id": "tS8ToxcjFIMc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-9Lpzg2Fipm",
        "outputId": "e60c1221-050c-420d-cffe-c771be45d6a9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(314, 78)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정규화 하기\n",
        "- dataset의 숫자들의 편차가 크기 때문에 어느정도 크기를 통일시켜 줄 필요가 있음\n",
        "\n",
        "- 특성의 스케일과 범위가 다르면 정규화(normalization)하는 것이 권장됩니다. 특성을 정규화하지 않아도 모델이 수렴할 수 있지만, 훈련시키기 어렵고 입력 단위에 의존적인 모델이 만들어집니다.\n",
        "\n",
        "노트: 의도적으로 훈련 세트만 사용하여 통계치를 생성했습니다. 이 통계는 테스트 세트를 정규화할 때에도 사용됩니다. 이는 테스트 세트를 모델이 훈련에 사용했던 것과 동일한 분포로 투영하기 위해서입니다.\n",
        "\n",
        "\n",
        "-> 통계적인 값이 필요함(avg, std 같은 것들)\n",
        "\n",
        "-> describe()를 이용해 구해줌"
      ],
      "metadata": {
        "id": "XPdxW2zpF46K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop(\"MPG\")\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "m7XpHoAUFo8d",
        "outputId": "cf94ead2-122c-439c-9a35-1cad05079a85"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              count         mean         std     min      25%     50%  \\\n",
              "Cylinders     314.0     5.477707    1.699788     3.0     4.00     4.0   \n",
              "Displacement  314.0   195.318471  104.331589    68.0   105.50   151.0   \n",
              "Horsepower    314.0   104.869427   38.096214    46.0    76.25    94.5   \n",
              "Weight        314.0  2990.251592  843.898596  1649.0  2256.50  2822.5   \n",
              "Acceleration  314.0    15.559236    2.789230     8.0    13.80    15.5   \n",
              "Model Year    314.0    75.898089    3.675642    70.0    73.00    76.0   \n",
              "\n",
              "                  75%     max  \n",
              "Cylinders        8.00     8.0  \n",
              "Displacement   265.75   455.0  \n",
              "Horsepower     128.00   225.0  \n",
              "Weight        3608.00  5140.0  \n",
              "Acceleration    17.20    24.8  \n",
              "Model Year      79.00    82.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a5f23ad-dd2a-40be-8337-45d7296b4a6d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Cylinders</th>\n",
              "      <td>314.0</td>\n",
              "      <td>5.477707</td>\n",
              "      <td>1.699788</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.00</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Displacement</th>\n",
              "      <td>314.0</td>\n",
              "      <td>195.318471</td>\n",
              "      <td>104.331589</td>\n",
              "      <td>68.0</td>\n",
              "      <td>105.50</td>\n",
              "      <td>151.0</td>\n",
              "      <td>265.75</td>\n",
              "      <td>455.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Horsepower</th>\n",
              "      <td>314.0</td>\n",
              "      <td>104.869427</td>\n",
              "      <td>38.096214</td>\n",
              "      <td>46.0</td>\n",
              "      <td>76.25</td>\n",
              "      <td>94.5</td>\n",
              "      <td>128.00</td>\n",
              "      <td>225.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight</th>\n",
              "      <td>314.0</td>\n",
              "      <td>2990.251592</td>\n",
              "      <td>843.898596</td>\n",
              "      <td>1649.0</td>\n",
              "      <td>2256.50</td>\n",
              "      <td>2822.5</td>\n",
              "      <td>3608.00</td>\n",
              "      <td>5140.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Acceleration</th>\n",
              "      <td>314.0</td>\n",
              "      <td>15.559236</td>\n",
              "      <td>2.789230</td>\n",
              "      <td>8.0</td>\n",
              "      <td>13.80</td>\n",
              "      <td>15.5</td>\n",
              "      <td>17.20</td>\n",
              "      <td>24.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model Year</th>\n",
              "      <td>314.0</td>\n",
              "      <td>75.898089</td>\n",
              "      <td>3.675642</td>\n",
              "      <td>70.0</td>\n",
              "      <td>73.00</td>\n",
              "      <td>76.0</td>\n",
              "      <td>79.00</td>\n",
              "      <td>82.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a5f23ad-dd2a-40be-8337-45d7296b4a6d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9a5f23ad-dd2a-40be-8337-45d7296b4a6d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9a5f23ad-dd2a-40be-8337-45d7296b4a6d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 특성과 레이블 분리하기\n",
        "- 특성에서 타깃 값 또는 \"레이블\"을 분리합니다. 이 레이블을 예측하기 위해 모델을 훈련시킬 것입니다.\n",
        "\n",
        "ex) mpg는 정규화를 하지 말아야 하므로 제외시켜줌 "
      ],
      "metadata": {
        "id": "crBtIIUyHAbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_dataset.pop('MPG')\n",
        "test_labels = test_dataset.pop('MPG')"
      ],
      "metadata": {
        "id": "LPmD4b4CG9aq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 정규화"
      ],
      "metadata": {
        "id": "X28kdRx3HdwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)"
      ],
      "metadata": {
        "id": "RNu8Z9ufHcA9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normed_train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cs28JGVOHneh",
        "outputId": "a87f5a82-16da-4061-d996-d1c88ec32657"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Cylinders  Displacement  Horsepower    Weight  Acceleration  Model Year\n",
              "146  -0.869348     -1.009459   -0.784052 -1.025303     -0.379759   -0.516397\n",
              "282  -0.869348     -0.530218   -0.442811 -0.118796      0.624102    0.843910\n",
              "69    1.483887      1.482595    1.447140  1.736877     -0.738281   -1.060519\n",
              "378  -0.869348     -0.865687   -1.099044 -1.025303     -0.308055    1.660094\n",
              "331  -0.869348     -0.942365   -0.994047 -1.001603      0.875068    1.115971"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-587bf824-84f4-45a7-ab84-c3b1bbd6128d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>-0.869348</td>\n",
              "      <td>-1.009459</td>\n",
              "      <td>-0.784052</td>\n",
              "      <td>-1.025303</td>\n",
              "      <td>-0.379759</td>\n",
              "      <td>-0.516397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>-0.869348</td>\n",
              "      <td>-0.530218</td>\n",
              "      <td>-0.442811</td>\n",
              "      <td>-0.118796</td>\n",
              "      <td>0.624102</td>\n",
              "      <td>0.843910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>1.483887</td>\n",
              "      <td>1.482595</td>\n",
              "      <td>1.447140</td>\n",
              "      <td>1.736877</td>\n",
              "      <td>-0.738281</td>\n",
              "      <td>-1.060519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>-0.869348</td>\n",
              "      <td>-0.865687</td>\n",
              "      <td>-1.099044</td>\n",
              "      <td>-1.025303</td>\n",
              "      <td>-0.308055</td>\n",
              "      <td>1.660094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>-0.869348</td>\n",
              "      <td>-0.942365</td>\n",
              "      <td>-0.994047</td>\n",
              "      <td>-1.001603</td>\n",
              "      <td>0.875068</td>\n",
              "      <td>1.115971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-587bf824-84f4-45a7-ab84-c3b1bbd6128d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-587bf824-84f4-45a7-ab84-c3b1bbd6128d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-587bf824-84f4-45a7-ab84-c3b1bbd6128d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델"
      ],
      "metadata": {
        "id": "nTGEHP2RJ4gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델만들기\n",
        "- 모델을 구성해 보죠. 여기에서는 두 개의 완전 연결(densely connected) 은닉층으로 Sequential 모델을 만들겠습니다. 출력 층은 하나의 연속적인 값을 반환합니다. 나중에 두 번째 모델을 만들기 쉽도록 build_model 함수로 모델 구성 단계를 감싸겠습니다."
      ],
      "metadata": {
        "id": "9aygSWZYJ79e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normed_train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrxr8k8qH3Zl",
        "outputId": "3cb12371-06fa-4a54-83cc-4cb9d2143ad1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(314, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data feature의 size가 중요하기 때문에\n",
        "Input으로 넣어준다"
      ],
      "metadata": {
        "id": "7GzU4j21KWpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=keras.Input(shape=(normed_train_data.shape[1],))\n",
        "h=layers.Dense(64)(inputs) # 행렬의 feature 크기를 64로 하겠다 (중간 레이어)\n",
        "#hidden layer 추가하려면 h=layers.Dense(32)(h) : 이전 레이어가 다음 레이어의 input으로 들어가야 됨\n",
        "outputs=layers.Dense(1)(h) # 최종 output의 shape는 1로 나오도록 하겠다"
      ],
      "metadata": {
        "id": "MnswcjTMKRRF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "eTfRw7lJLWi-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(0.001))\n",
        "\n",
        "# model이라는 것\n",
        "# y(예측 output) = x(intput) * w(weight: matrix) +b(bias) : linear model\n",
        "# 처음엔 w와 b는 랜덤 값으로 정해주지만 모델을 학습시키면서 점점 w, b를 조정해가면서 model 의 정확도를 높인다\n",
        "# mse : mean squared error\n",
        "# optimizer : 오차가 났을 때 w와 b를 어떻게 둘 것인지 정하는 것."
      ],
      "metadata": {
        "id": "KPEG7H-pLf6L"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 확인\n",
        "\n",
        "- summary 메서드 사용해 모델에 대한 간단한 정보를 출력한다."
      ],
      "metadata": {
        "id": "-dSYhISfLuGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary() #input -> 중간레이어 -> output(결과값 1개)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXzhNceELr7U",
        "outputId": "b1e7e464-b599-479e-9849-1d5d0a89bca2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                448       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 513\n",
            "Trainable params: 513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- visualize 함수 plot_model()\n",
        "   레이어 별로 input output 나옴\n",
        "\n"
      ],
      "metadata": {
        "id": "GA8IsjfKMPQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "qHxNOkGqL7cu",
        "outputId": "adb2e3de-1f16-4141-c528-321df6f68b61"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 한 번 실행해 본다.\n",
        "train_set에서 10 sample을 하나의 배치로 만들어 model.predict()를 이용해 호출해본다."
      ],
      "metadata": {
        "id": "khXcaMaQOZUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normed_train_data[:10].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPvAgvzhOxrI",
        "outputId": "c5c8e3f5-44fa-4eb9-c657-ad2cbf878c80"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습이 전혀 안 된 모델이므로 기대값과 다른 값이 나오는게 정상이다"
      ],
      "metadata": {
        "id": "yfz3JtN8PHqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch = normed_train_data[:10]\n",
        "example_result = model.predict(example_batch)\n",
        "example_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSXqY-plM4DM",
        "outputId": "279f10ab-a1a5-4f7f-b9ee-412155e53dbf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 15ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.68146133],\n",
              "       [ 0.39138737],\n",
              "       [-1.0478222 ],\n",
              "       [ 0.88360834],\n",
              "       [ 0.5957187 ],\n",
              "       [-0.15938246],\n",
              "       [ 0.60448986],\n",
              "       [ 0.15210125],\n",
              "       [-0.18496037],\n",
              "       [ 0.25927064]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델훈련\n",
        "- 이 모델을 1000번의 에포크(epoch)동안 훈련함. 훈련 정확도와 검증 정확도는 history 객체에 기록 됨."
      ],
      "metadata": {
        "id": "6HpgzNACPW6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- fit(input_data, output_data, epochs,validation_split)\n",
        "- validation set : 과적합을 방지하기 위해 따로 분류해 놓은 값 (학습에 쓰이지 않는 값)"
      ],
      "metadata": {
        "id": "ijKJyhbyPqox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "history = model.fit(\n",
        "  normed_train_data, train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M_MlNclMrX1",
        "outputId": "01b19822-17ef-408e-9490-91a5800aca6f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "8/8 [==============================] - 1s 21ms/step - loss: 583.3276 - val_loss: 600.9270\n",
            "Epoch 2/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 572.6133 - val_loss: 591.0014\n",
            "Epoch 3/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 564.1765 - val_loss: 582.2165\n",
            "Epoch 4/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 556.5971 - val_loss: 574.1881\n",
            "Epoch 5/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 548.9046 - val_loss: 565.7850\n",
            "Epoch 6/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 541.3668 - val_loss: 557.6788\n",
            "Epoch 7/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 533.6349 - val_loss: 549.4427\n",
            "Epoch 8/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 525.8829 - val_loss: 541.2386\n",
            "Epoch 9/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 518.1856 - val_loss: 533.1042\n",
            "Epoch 10/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 510.4417 - val_loss: 524.8291\n",
            "Epoch 11/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 502.5612 - val_loss: 516.5508\n",
            "Epoch 12/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 494.4543 - val_loss: 508.2794\n",
            "Epoch 13/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 486.4854 - val_loss: 500.1738\n",
            "Epoch 14/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 478.6535 - val_loss: 491.7428\n",
            "Epoch 15/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 470.3337 - val_loss: 483.4164\n",
            "Epoch 16/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 462.1395 - val_loss: 474.9563\n",
            "Epoch 17/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 453.6947 - val_loss: 466.4139\n",
            "Epoch 18/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 445.3009 - val_loss: 457.5884\n",
            "Epoch 19/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 436.4717 - val_loss: 448.5810\n",
            "Epoch 20/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 427.6928 - val_loss: 439.7323\n",
            "Epoch 21/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 418.7765 - val_loss: 430.6427\n",
            "Epoch 22/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 409.5245 - val_loss: 421.2031\n",
            "Epoch 23/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 400.2196 - val_loss: 411.5790\n",
            "Epoch 24/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 390.7370 - val_loss: 401.8067\n",
            "Epoch 25/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 380.8313 - val_loss: 391.8675\n",
            "Epoch 26/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 370.9895 - val_loss: 381.8606\n",
            "Epoch 27/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 361.1488 - val_loss: 371.6949\n",
            "Epoch 28/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 350.9580 - val_loss: 361.4171\n",
            "Epoch 29/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 340.9531 - val_loss: 350.9241\n",
            "Epoch 30/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 330.3809 - val_loss: 340.4666\n",
            "Epoch 31/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 319.8196 - val_loss: 329.8748\n",
            "Epoch 32/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 309.2114 - val_loss: 318.9940\n",
            "Epoch 33/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 298.3968 - val_loss: 308.0300\n",
            "Epoch 34/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 287.5067 - val_loss: 296.8440\n",
            "Epoch 35/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 276.6241 - val_loss: 285.7935\n",
            "Epoch 36/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 265.5766 - val_loss: 274.7158\n",
            "Epoch 37/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 254.7333 - val_loss: 263.6341\n",
            "Epoch 38/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 243.4429 - val_loss: 252.4126\n",
            "Epoch 39/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 232.5971 - val_loss: 241.2855\n",
            "Epoch 40/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 221.8854 - val_loss: 230.2151\n",
            "Epoch 41/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 210.8750 - val_loss: 219.1579\n",
            "Epoch 42/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 200.1494 - val_loss: 208.3265\n",
            "Epoch 43/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 189.5989 - val_loss: 197.5165\n",
            "Epoch 44/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 179.3776 - val_loss: 186.9955\n",
            "Epoch 45/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 168.8959 - val_loss: 176.4639\n",
            "Epoch 46/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 158.5727 - val_loss: 166.1039\n",
            "Epoch 47/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 148.6409 - val_loss: 155.9219\n",
            "Epoch 48/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 138.8185 - val_loss: 145.9222\n",
            "Epoch 49/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 129.3786 - val_loss: 135.9632\n",
            "Epoch 50/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 119.8202 - val_loss: 126.3998\n",
            "Epoch 51/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 111.0156 - val_loss: 117.0424\n",
            "Epoch 52/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 102.1612 - val_loss: 108.0320\n",
            "Epoch 53/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 93.8118 - val_loss: 99.2623\n",
            "Epoch 54/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 85.4785 - val_loss: 90.9400\n",
            "Epoch 55/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 77.7048 - val_loss: 82.7233\n",
            "Epoch 56/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 70.4949 - val_loss: 75.0955\n",
            "Epoch 57/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 63.2989 - val_loss: 67.8137\n",
            "Epoch 58/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 56.8258 - val_loss: 60.8826\n",
            "Epoch 59/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 50.4378 - val_loss: 54.1477\n",
            "Epoch 60/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 44.7033 - val_loss: 47.9887\n",
            "Epoch 61/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 39.3340 - val_loss: 42.3939\n",
            "Epoch 62/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 34.3942 - val_loss: 37.3406\n",
            "Epoch 63/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 30.1493 - val_loss: 32.6009\n",
            "Epoch 64/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 26.2624 - val_loss: 28.3925\n",
            "Epoch 65/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 22.8826 - val_loss: 24.6257\n",
            "Epoch 66/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 20.3047 - val_loss: 21.5732\n",
            "Epoch 67/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 17.7575 - val_loss: 18.9675\n",
            "Epoch 68/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 16.0391 - val_loss: 16.8321\n",
            "Epoch 69/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 14.5976 - val_loss: 15.2095\n",
            "Epoch 70/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 13.5579 - val_loss: 13.9694\n",
            "Epoch 71/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13.0080 - val_loss: 13.0557\n",
            "Epoch 72/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.7950 - val_loss: 12.6749\n",
            "Epoch 73/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.5757 - val_loss: 12.5194\n",
            "Epoch 74/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.5350 - val_loss: 12.4202\n",
            "Epoch 75/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.5751 - val_loss: 12.1366\n",
            "Epoch 76/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.4533 - val_loss: 12.1107\n",
            "Epoch 77/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.4865 - val_loss: 12.1023\n",
            "Epoch 78/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.4950 - val_loss: 12.2482\n",
            "Epoch 79/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3800 - val_loss: 11.9828\n",
            "Epoch 80/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.4735 - val_loss: 12.0114\n",
            "Epoch 81/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.5327 - val_loss: 12.0493\n",
            "Epoch 82/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3456 - val_loss: 12.0097\n",
            "Epoch 83/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3815 - val_loss: 12.0050\n",
            "Epoch 84/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.5607 - val_loss: 11.9876\n",
            "Epoch 85/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3693 - val_loss: 11.9758\n",
            "Epoch 86/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.4357 - val_loss: 11.9415\n",
            "Epoch 87/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2865 - val_loss: 12.2086\n",
            "Epoch 88/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3179 - val_loss: 12.0076\n",
            "Epoch 89/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3324 - val_loss: 11.9768\n",
            "Epoch 90/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3540 - val_loss: 12.0058\n",
            "Epoch 91/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3987 - val_loss: 12.0612\n",
            "Epoch 92/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3179 - val_loss: 11.9725\n",
            "Epoch 93/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3901 - val_loss: 11.9634\n",
            "Epoch 94/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2881 - val_loss: 11.8944\n",
            "Epoch 95/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2836 - val_loss: 12.0131\n",
            "Epoch 96/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3212 - val_loss: 12.0785\n",
            "Epoch 97/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.4090 - val_loss: 12.0027\n",
            "Epoch 98/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3401 - val_loss: 11.9637\n",
            "Epoch 99/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3190 - val_loss: 11.8392\n",
            "Epoch 100/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2467 - val_loss: 11.9820\n",
            "Epoch 101/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2424 - val_loss: 11.9164\n",
            "Epoch 102/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2058 - val_loss: 11.9472\n",
            "Epoch 103/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2659 - val_loss: 11.8480\n",
            "Epoch 104/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3382 - val_loss: 11.9063\n",
            "Epoch 105/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2704 - val_loss: 11.9803\n",
            "Epoch 106/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1888 - val_loss: 11.8553\n",
            "Epoch 107/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2409 - val_loss: 11.9213\n",
            "Epoch 108/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2709 - val_loss: 11.8962\n",
            "Epoch 109/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3640 - val_loss: 11.9159\n",
            "Epoch 110/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2346 - val_loss: 11.9399\n",
            "Epoch 111/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2184 - val_loss: 11.9800\n",
            "Epoch 112/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2452 - val_loss: 11.8915\n",
            "Epoch 113/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2997 - val_loss: 11.8018\n",
            "Epoch 114/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2929 - val_loss: 11.8341\n",
            "Epoch 115/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2268 - val_loss: 11.8118\n",
            "Epoch 116/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2926 - val_loss: 11.8374\n",
            "Epoch 117/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1898 - val_loss: 12.0054\n",
            "Epoch 118/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2771 - val_loss: 11.8901\n",
            "Epoch 119/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2257 - val_loss: 11.9791\n",
            "Epoch 120/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2214 - val_loss: 11.8836\n",
            "Epoch 121/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2680 - val_loss: 11.8946\n",
            "Epoch 122/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1599 - val_loss: 12.0673\n",
            "Epoch 123/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2503 - val_loss: 11.9255\n",
            "Epoch 124/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2389 - val_loss: 11.9455\n",
            "Epoch 125/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2016 - val_loss: 11.9695\n",
            "Epoch 126/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2902 - val_loss: 11.9505\n",
            "Epoch 127/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2097 - val_loss: 11.9806\n",
            "Epoch 128/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2687 - val_loss: 11.8518\n",
            "Epoch 129/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2879 - val_loss: 11.9272\n",
            "Epoch 130/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1967 - val_loss: 11.8709\n",
            "Epoch 131/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3070 - val_loss: 11.9662\n",
            "Epoch 132/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2125 - val_loss: 11.9932\n",
            "Epoch 133/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2099 - val_loss: 11.9817\n",
            "Epoch 134/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1957 - val_loss: 11.9480\n",
            "Epoch 135/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2825 - val_loss: 11.8660\n",
            "Epoch 136/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2256 - val_loss: 11.9128\n",
            "Epoch 137/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2910 - val_loss: 11.8163\n",
            "Epoch 138/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1938 - val_loss: 12.0004\n",
            "Epoch 139/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2647 - val_loss: 11.8759\n",
            "Epoch 140/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1606 - val_loss: 12.1235\n",
            "Epoch 141/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2373 - val_loss: 11.9048\n",
            "Epoch 142/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1927 - val_loss: 12.0041\n",
            "Epoch 143/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2062 - val_loss: 11.8601\n",
            "Epoch 144/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1925 - val_loss: 11.8739\n",
            "Epoch 145/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2187 - val_loss: 12.0052\n",
            "Epoch 146/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3076 - val_loss: 11.9017\n",
            "Epoch 147/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2003 - val_loss: 11.9187\n",
            "Epoch 148/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1727 - val_loss: 11.8319\n",
            "Epoch 149/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1454 - val_loss: 11.9080\n",
            "Epoch 150/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2563 - val_loss: 12.0378\n",
            "Epoch 151/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2850 - val_loss: 11.9604\n",
            "Epoch 152/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1879 - val_loss: 12.0233\n",
            "Epoch 153/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3051 - val_loss: 11.9530\n",
            "Epoch 154/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2493 - val_loss: 11.9333\n",
            "Epoch 155/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1464 - val_loss: 12.0937\n",
            "Epoch 156/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2657 - val_loss: 11.9707\n",
            "Epoch 157/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1555 - val_loss: 12.0258\n",
            "Epoch 158/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2838 - val_loss: 11.9561\n",
            "Epoch 159/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1718 - val_loss: 11.9428\n",
            "Epoch 160/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1375 - val_loss: 11.9554\n",
            "Epoch 161/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1928 - val_loss: 11.9093\n",
            "Epoch 162/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3342 - val_loss: 11.8957\n",
            "Epoch 163/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2025 - val_loss: 11.9966\n",
            "Epoch 164/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2270 - val_loss: 12.1069\n",
            "Epoch 165/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2733 - val_loss: 12.1116\n",
            "Epoch 166/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2610 - val_loss: 11.9439\n",
            "Epoch 167/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1914 - val_loss: 12.0165\n",
            "Epoch 168/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2814 - val_loss: 11.9423\n",
            "Epoch 169/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3154 - val_loss: 11.8673\n",
            "Epoch 170/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1718 - val_loss: 11.9118\n",
            "Epoch 171/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2189 - val_loss: 11.8335\n",
            "Epoch 172/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3203 - val_loss: 11.8631\n",
            "Epoch 173/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2242 - val_loss: 11.8689\n",
            "Epoch 174/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2219 - val_loss: 11.9595\n",
            "Epoch 175/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1314 - val_loss: 12.0100\n",
            "Epoch 176/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2804 - val_loss: 11.8939\n",
            "Epoch 177/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1438 - val_loss: 11.8540\n",
            "Epoch 178/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2599 - val_loss: 11.9839\n",
            "Epoch 179/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2407 - val_loss: 12.0268\n",
            "Epoch 180/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3630 - val_loss: 11.9619\n",
            "Epoch 181/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1702 - val_loss: 11.9226\n",
            "Epoch 182/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2603 - val_loss: 11.9976\n",
            "Epoch 183/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2543 - val_loss: 11.9804\n",
            "Epoch 184/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2042 - val_loss: 11.9874\n",
            "Epoch 185/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3627 - val_loss: 12.0651\n",
            "Epoch 186/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2293 - val_loss: 11.9703\n",
            "Epoch 187/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2628 - val_loss: 11.9599\n",
            "Epoch 188/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2055 - val_loss: 11.9532\n",
            "Epoch 189/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2666 - val_loss: 11.9546\n",
            "Epoch 190/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2511 - val_loss: 12.0271\n",
            "Epoch 191/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2277 - val_loss: 12.0203\n",
            "Epoch 192/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3322 - val_loss: 11.9913\n",
            "Epoch 193/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2265 - val_loss: 11.9408\n",
            "Epoch 194/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1724 - val_loss: 11.9837\n",
            "Epoch 195/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2462 - val_loss: 11.9574\n",
            "Epoch 196/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2575 - val_loss: 11.9248\n",
            "Epoch 197/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1366 - val_loss: 11.8817\n",
            "Epoch 198/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1904 - val_loss: 12.0077\n",
            "Epoch 199/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3134 - val_loss: 11.8917\n",
            "Epoch 200/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2506 - val_loss: 11.8631\n",
            "Epoch 201/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1725 - val_loss: 11.9232\n",
            "Epoch 202/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2420 - val_loss: 11.9472\n",
            "Epoch 203/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1792 - val_loss: 11.8841\n",
            "Epoch 204/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2047 - val_loss: 11.9448\n",
            "Epoch 205/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1536 - val_loss: 12.0243\n",
            "Epoch 206/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1732 - val_loss: 11.8712\n",
            "Epoch 207/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2277 - val_loss: 11.8762\n",
            "Epoch 208/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2586 - val_loss: 11.9233\n",
            "Epoch 209/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1497 - val_loss: 11.8879\n",
            "Epoch 210/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2032 - val_loss: 11.8579\n",
            "Epoch 211/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1586 - val_loss: 11.9399\n",
            "Epoch 212/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2400 - val_loss: 11.9354\n",
            "Epoch 213/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2271 - val_loss: 12.0004\n",
            "Epoch 214/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2171 - val_loss: 12.0083\n",
            "Epoch 215/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3085 - val_loss: 11.8477\n",
            "Epoch 216/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2700 - val_loss: 11.9300\n",
            "Epoch 217/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1649 - val_loss: 11.9204\n",
            "Epoch 218/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1837 - val_loss: 11.8614\n",
            "Epoch 219/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1778 - val_loss: 11.8912\n",
            "Epoch 220/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1566 - val_loss: 12.0627\n",
            "Epoch 221/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2211 - val_loss: 11.9890\n",
            "Epoch 222/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2409 - val_loss: 11.8897\n",
            "Epoch 223/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2759 - val_loss: 11.8915\n",
            "Epoch 224/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1236 - val_loss: 11.8714\n",
            "Epoch 225/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2795 - val_loss: 11.8282\n",
            "Epoch 226/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1484 - val_loss: 11.8364\n",
            "Epoch 227/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1909 - val_loss: 11.9469\n",
            "Epoch 228/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3090 - val_loss: 11.7791\n",
            "Epoch 229/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3181 - val_loss: 11.8692\n",
            "Epoch 230/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1687 - val_loss: 12.1371\n",
            "Epoch 231/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1100 - val_loss: 11.9041\n",
            "Epoch 232/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1940 - val_loss: 11.8765\n",
            "Epoch 233/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1525 - val_loss: 11.9384\n",
            "Epoch 234/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2019 - val_loss: 11.9478\n",
            "Epoch 235/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2742 - val_loss: 12.0398\n",
            "Epoch 236/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1635 - val_loss: 11.9125\n",
            "Epoch 237/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2770 - val_loss: 11.8432\n",
            "Epoch 238/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1246 - val_loss: 11.9431\n",
            "Epoch 239/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2484 - val_loss: 11.9537\n",
            "Epoch 240/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2223 - val_loss: 11.9563\n",
            "Epoch 241/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2653 - val_loss: 12.0186\n",
            "Epoch 242/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1559 - val_loss: 11.9285\n",
            "Epoch 243/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2722 - val_loss: 12.0172\n",
            "Epoch 244/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2458 - val_loss: 11.9885\n",
            "Epoch 245/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2543 - val_loss: 11.8782\n",
            "Epoch 246/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1937 - val_loss: 11.9004\n",
            "Epoch 247/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2716 - val_loss: 11.8835\n",
            "Epoch 248/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1318 - val_loss: 11.8413\n",
            "Epoch 249/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2161 - val_loss: 11.8843\n",
            "Epoch 250/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2076 - val_loss: 11.9984\n",
            "Epoch 251/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2674 - val_loss: 11.9419\n",
            "Epoch 252/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2483 - val_loss: 11.8944\n",
            "Epoch 253/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1340 - val_loss: 12.0367\n",
            "Epoch 254/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1476 - val_loss: 11.9326\n",
            "Epoch 255/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1581 - val_loss: 11.8381\n",
            "Epoch 256/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2623 - val_loss: 11.9070\n",
            "Epoch 257/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1667 - val_loss: 12.0845\n",
            "Epoch 258/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1439 - val_loss: 11.9342\n",
            "Epoch 259/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2991 - val_loss: 11.9388\n",
            "Epoch 260/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1599 - val_loss: 12.1669\n",
            "Epoch 261/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3145 - val_loss: 11.9937\n",
            "Epoch 262/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2263 - val_loss: 11.9437\n",
            "Epoch 263/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1610 - val_loss: 12.0886\n",
            "Epoch 264/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1560 - val_loss: 11.9530\n",
            "Epoch 265/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2942 - val_loss: 11.9513\n",
            "Epoch 266/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2452 - val_loss: 11.9581\n",
            "Epoch 267/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2557 - val_loss: 11.9196\n",
            "Epoch 268/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1898 - val_loss: 11.8758\n",
            "Epoch 269/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1877 - val_loss: 11.9278\n",
            "Epoch 270/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2358 - val_loss: 11.9840\n",
            "Epoch 271/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1272 - val_loss: 12.2725\n",
            "Epoch 272/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3706 - val_loss: 11.9085\n",
            "Epoch 273/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2321 - val_loss: 11.9130\n",
            "Epoch 274/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1463 - val_loss: 12.0499\n",
            "Epoch 275/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1577 - val_loss: 11.9947\n",
            "Epoch 276/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1761 - val_loss: 11.9813\n",
            "Epoch 277/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2078 - val_loss: 12.0640\n",
            "Epoch 278/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1651 - val_loss: 11.8881\n",
            "Epoch 279/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2438 - val_loss: 11.9225\n",
            "Epoch 280/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2026 - val_loss: 11.9499\n",
            "Epoch 281/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2932 - val_loss: 11.8709\n",
            "Epoch 282/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2598 - val_loss: 11.8837\n",
            "Epoch 283/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1506 - val_loss: 11.9292\n",
            "Epoch 284/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1768 - val_loss: 11.9117\n",
            "Epoch 285/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2098 - val_loss: 11.8462\n",
            "Epoch 286/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1991 - val_loss: 11.9048\n",
            "Epoch 287/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1888 - val_loss: 11.8701\n",
            "Epoch 288/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1685 - val_loss: 11.9453\n",
            "Epoch 289/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2376 - val_loss: 12.0075\n",
            "Epoch 290/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2338 - val_loss: 11.9131\n",
            "Epoch 291/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3679 - val_loss: 11.9089\n",
            "Epoch 292/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2113 - val_loss: 11.8746\n",
            "Epoch 293/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1645 - val_loss: 11.9052\n",
            "Epoch 294/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1424 - val_loss: 11.9735\n",
            "Epoch 295/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1903 - val_loss: 11.8551\n",
            "Epoch 296/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2845 - val_loss: 11.9112\n",
            "Epoch 297/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1436 - val_loss: 12.0786\n",
            "Epoch 298/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1769 - val_loss: 11.9656\n",
            "Epoch 299/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2092 - val_loss: 11.9056\n",
            "Epoch 300/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1667 - val_loss: 11.8109\n",
            "Epoch 301/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1813 - val_loss: 11.8505\n",
            "Epoch 302/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2070 - val_loss: 11.9587\n",
            "Epoch 303/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1913 - val_loss: 11.9484\n",
            "Epoch 304/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1997 - val_loss: 11.8646\n",
            "Epoch 305/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2383 - val_loss: 11.9775\n",
            "Epoch 306/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2294 - val_loss: 11.8447\n",
            "Epoch 307/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1624 - val_loss: 11.9591\n",
            "Epoch 308/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2117 - val_loss: 11.9992\n",
            "Epoch 309/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1398 - val_loss: 11.8941\n",
            "Epoch 310/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1136 - val_loss: 11.9853\n",
            "Epoch 311/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2345 - val_loss: 11.9210\n",
            "Epoch 312/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2358 - val_loss: 11.9110\n",
            "Epoch 313/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1815 - val_loss: 11.9400\n",
            "Epoch 314/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2274 - val_loss: 11.8969\n",
            "Epoch 315/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2829 - val_loss: 11.8460\n",
            "Epoch 316/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2098 - val_loss: 11.8987\n",
            "Epoch 317/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2570 - val_loss: 11.8991\n",
            "Epoch 318/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1612 - val_loss: 11.9814\n",
            "Epoch 319/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2130 - val_loss: 11.8691\n",
            "Epoch 320/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1965 - val_loss: 11.9472\n",
            "Epoch 321/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2000 - val_loss: 11.9612\n",
            "Epoch 322/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.4002 - val_loss: 11.8642\n",
            "Epoch 323/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1391 - val_loss: 11.8428\n",
            "Epoch 324/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2848 - val_loss: 11.9084\n",
            "Epoch 325/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3256 - val_loss: 11.9182\n",
            "Epoch 326/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1655 - val_loss: 11.9402\n",
            "Epoch 327/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1617 - val_loss: 11.9069\n",
            "Epoch 328/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1827 - val_loss: 11.9436\n",
            "Epoch 329/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2096 - val_loss: 11.8457\n",
            "Epoch 330/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2184 - val_loss: 11.8520\n",
            "Epoch 331/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3153 - val_loss: 11.9157\n",
            "Epoch 332/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2626 - val_loss: 11.9422\n",
            "Epoch 333/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1380 - val_loss: 11.8734\n",
            "Epoch 334/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2161 - val_loss: 11.9194\n",
            "Epoch 335/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1715 - val_loss: 11.8362\n",
            "Epoch 336/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2833 - val_loss: 11.8577\n",
            "Epoch 337/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1682 - val_loss: 11.9430\n",
            "Epoch 338/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2276 - val_loss: 11.9354\n",
            "Epoch 339/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1861 - val_loss: 11.9588\n",
            "Epoch 340/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2020 - val_loss: 11.8846\n",
            "Epoch 341/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2081 - val_loss: 11.9825\n",
            "Epoch 342/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2226 - val_loss: 11.8735\n",
            "Epoch 343/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2669 - val_loss: 11.8355\n",
            "Epoch 344/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1549 - val_loss: 11.8946\n",
            "Epoch 345/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1403 - val_loss: 11.9329\n",
            "Epoch 346/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2651 - val_loss: 11.8883\n",
            "Epoch 347/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1556 - val_loss: 11.9524\n",
            "Epoch 348/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1932 - val_loss: 11.9369\n",
            "Epoch 349/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2184 - val_loss: 11.9232\n",
            "Epoch 350/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2852 - val_loss: 11.8389\n",
            "Epoch 351/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1764 - val_loss: 11.9817\n",
            "Epoch 352/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3584 - val_loss: 11.8395\n",
            "Epoch 353/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2294 - val_loss: 11.8828\n",
            "Epoch 354/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1971 - val_loss: 11.9945\n",
            "Epoch 355/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1404 - val_loss: 11.9819\n",
            "Epoch 356/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2274 - val_loss: 11.9359\n",
            "Epoch 357/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2121 - val_loss: 11.8827\n",
            "Epoch 358/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2728 - val_loss: 11.8344\n",
            "Epoch 359/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2911 - val_loss: 11.8605\n",
            "Epoch 360/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1888 - val_loss: 11.8821\n",
            "Epoch 361/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1752 - val_loss: 11.9445\n",
            "Epoch 362/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1344 - val_loss: 12.0182\n",
            "Epoch 363/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2863 - val_loss: 12.0771\n",
            "Epoch 364/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2402 - val_loss: 11.9981\n",
            "Epoch 365/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2282 - val_loss: 12.0388\n",
            "Epoch 366/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2950 - val_loss: 11.9387\n",
            "Epoch 367/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2365 - val_loss: 11.9441\n",
            "Epoch 368/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3004 - val_loss: 11.9415\n",
            "Epoch 369/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2830 - val_loss: 11.9801\n",
            "Epoch 370/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2003 - val_loss: 11.8494\n",
            "Epoch 371/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2081 - val_loss: 11.9115\n",
            "Epoch 372/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1973 - val_loss: 11.8573\n",
            "Epoch 373/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2683 - val_loss: 11.8853\n",
            "Epoch 374/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1667 - val_loss: 11.8536\n",
            "Epoch 375/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1625 - val_loss: 12.0335\n",
            "Epoch 376/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1677 - val_loss: 11.8878\n",
            "Epoch 377/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1926 - val_loss: 11.9471\n",
            "Epoch 378/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2000 - val_loss: 11.8487\n",
            "Epoch 379/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2308 - val_loss: 11.9224\n",
            "Epoch 380/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2232 - val_loss: 11.9597\n",
            "Epoch 381/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3258 - val_loss: 11.8583\n",
            "Epoch 382/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2108 - val_loss: 11.8798\n",
            "Epoch 383/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2018 - val_loss: 11.8855\n",
            "Epoch 384/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3001 - val_loss: 11.9115\n",
            "Epoch 385/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2103 - val_loss: 11.8683\n",
            "Epoch 386/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1986 - val_loss: 11.8782\n",
            "Epoch 387/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2086 - val_loss: 11.7986\n",
            "Epoch 388/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2208 - val_loss: 11.9032\n",
            "Epoch 389/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1564 - val_loss: 11.8491\n",
            "Epoch 390/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2557 - val_loss: 11.8734\n",
            "Epoch 391/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1767 - val_loss: 11.9099\n",
            "Epoch 392/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2899 - val_loss: 11.9720\n",
            "Epoch 393/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1428 - val_loss: 11.9710\n",
            "Epoch 394/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1874 - val_loss: 11.9842\n",
            "Epoch 395/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2301 - val_loss: 11.8900\n",
            "Epoch 396/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2508 - val_loss: 11.8874\n",
            "Epoch 397/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1328 - val_loss: 11.9143\n",
            "Epoch 398/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1953 - val_loss: 11.9874\n",
            "Epoch 399/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2943 - val_loss: 11.9967\n",
            "Epoch 400/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1667 - val_loss: 11.9822\n",
            "Epoch 401/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2342 - val_loss: 12.0071\n",
            "Epoch 402/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2494 - val_loss: 11.9173\n",
            "Epoch 403/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1991 - val_loss: 11.9233\n",
            "Epoch 404/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2463 - val_loss: 11.8996\n",
            "Epoch 405/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3130 - val_loss: 11.9374\n",
            "Epoch 406/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1289 - val_loss: 11.9547\n",
            "Epoch 407/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1409 - val_loss: 11.9317\n",
            "Epoch 408/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1964 - val_loss: 12.1252\n",
            "Epoch 409/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2336 - val_loss: 12.0218\n",
            "Epoch 410/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2261 - val_loss: 12.0152\n",
            "Epoch 411/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1725 - val_loss: 12.0521\n",
            "Epoch 412/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1818 - val_loss: 12.1126\n",
            "Epoch 413/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2512 - val_loss: 11.9381\n",
            "Epoch 414/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2346 - val_loss: 12.0158\n",
            "Epoch 415/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1905 - val_loss: 11.8530\n",
            "Epoch 416/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2152 - val_loss: 12.0767\n",
            "Epoch 417/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2874 - val_loss: 11.9551\n",
            "Epoch 418/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1631 - val_loss: 11.9774\n",
            "Epoch 419/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1972 - val_loss: 11.8516\n",
            "Epoch 420/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2205 - val_loss: 11.8498\n",
            "Epoch 421/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2219 - val_loss: 11.8810\n",
            "Epoch 422/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2301 - val_loss: 11.9669\n",
            "Epoch 423/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1570 - val_loss: 11.9331\n",
            "Epoch 424/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2616 - val_loss: 11.9517\n",
            "Epoch 425/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1383 - val_loss: 11.9707\n",
            "Epoch 426/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1533 - val_loss: 12.1027\n",
            "Epoch 427/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3877 - val_loss: 12.0693\n",
            "Epoch 428/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2124 - val_loss: 12.0860\n",
            "Epoch 429/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1972 - val_loss: 11.9763\n",
            "Epoch 430/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1828 - val_loss: 12.0507\n",
            "Epoch 431/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2133 - val_loss: 11.9892\n",
            "Epoch 432/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2395 - val_loss: 12.0065\n",
            "Epoch 433/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1983 - val_loss: 11.9536\n",
            "Epoch 434/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1678 - val_loss: 12.0080\n",
            "Epoch 435/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1703 - val_loss: 11.9199\n",
            "Epoch 436/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1814 - val_loss: 11.9253\n",
            "Epoch 437/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1763 - val_loss: 12.0464\n",
            "Epoch 438/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2639 - val_loss: 11.9155\n",
            "Epoch 439/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2451 - val_loss: 11.9060\n",
            "Epoch 440/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1958 - val_loss: 11.9872\n",
            "Epoch 441/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2752 - val_loss: 11.8609\n",
            "Epoch 442/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2158 - val_loss: 11.9671\n",
            "Epoch 443/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1458 - val_loss: 11.8940\n",
            "Epoch 444/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2136 - val_loss: 12.0334\n",
            "Epoch 445/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2545 - val_loss: 11.9060\n",
            "Epoch 446/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2014 - val_loss: 11.8827\n",
            "Epoch 447/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2197 - val_loss: 11.8936\n",
            "Epoch 448/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1433 - val_loss: 11.8539\n",
            "Epoch 449/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1630 - val_loss: 11.8307\n",
            "Epoch 450/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2010 - val_loss: 11.8777\n",
            "Epoch 451/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.4591 - val_loss: 11.8711\n",
            "Epoch 452/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1501 - val_loss: 11.8905\n",
            "Epoch 453/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2698 - val_loss: 11.8532\n",
            "Epoch 454/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2334 - val_loss: 11.9169\n",
            "Epoch 455/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2078 - val_loss: 11.8829\n",
            "Epoch 456/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1305 - val_loss: 11.8703\n",
            "Epoch 457/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2432 - val_loss: 11.9586\n",
            "Epoch 458/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1775 - val_loss: 11.9327\n",
            "Epoch 459/1000\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 12.1836 - val_loss: 12.0072\n",
            "Epoch 460/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.1929 - val_loss: 11.9417\n",
            "Epoch 461/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.0912 - val_loss: 12.0064\n",
            "Epoch 462/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.3612 - val_loss: 11.8764\n",
            "Epoch 463/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.2043 - val_loss: 12.0218\n",
            "Epoch 464/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.2456 - val_loss: 11.9356\n",
            "Epoch 465/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1954 - val_loss: 12.0845\n",
            "Epoch 466/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1814 - val_loss: 11.9994\n",
            "Epoch 467/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2046 - val_loss: 11.9378\n",
            "Epoch 468/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2739 - val_loss: 12.0673\n",
            "Epoch 469/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1625 - val_loss: 11.9730\n",
            "Epoch 470/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1245 - val_loss: 11.9231\n",
            "Epoch 471/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2110 - val_loss: 11.9193\n",
            "Epoch 472/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1930 - val_loss: 11.8879\n",
            "Epoch 473/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2184 - val_loss: 11.8314\n",
            "Epoch 474/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2049 - val_loss: 11.9053\n",
            "Epoch 475/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3036 - val_loss: 12.0330\n",
            "Epoch 476/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2789 - val_loss: 11.9694\n",
            "Epoch 477/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1784 - val_loss: 11.9231\n",
            "Epoch 478/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1568 - val_loss: 11.8594\n",
            "Epoch 479/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1767 - val_loss: 11.9959\n",
            "Epoch 480/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2415 - val_loss: 11.8620\n",
            "Epoch 481/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2026 - val_loss: 11.9179\n",
            "Epoch 482/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1811 - val_loss: 11.8804\n",
            "Epoch 483/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1522 - val_loss: 12.0296\n",
            "Epoch 484/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2135 - val_loss: 12.0045\n",
            "Epoch 485/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1114 - val_loss: 11.9954\n",
            "Epoch 486/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1545 - val_loss: 12.0996\n",
            "Epoch 487/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1907 - val_loss: 11.9663\n",
            "Epoch 488/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1883 - val_loss: 11.9672\n",
            "Epoch 489/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2997 - val_loss: 11.8346\n",
            "Epoch 490/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1737 - val_loss: 11.9389\n",
            "Epoch 491/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1612 - val_loss: 11.8835\n",
            "Epoch 492/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2385 - val_loss: 11.8698\n",
            "Epoch 493/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1871 - val_loss: 11.8972\n",
            "Epoch 494/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.4261 - val_loss: 11.9108\n",
            "Epoch 495/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1589 - val_loss: 11.9047\n",
            "Epoch 496/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1604 - val_loss: 11.9029\n",
            "Epoch 497/1000\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 12.1617 - val_loss: 12.0019\n",
            "Epoch 498/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2267 - val_loss: 12.0359\n",
            "Epoch 499/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1960 - val_loss: 11.9054\n",
            "Epoch 500/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2129 - val_loss: 12.0001\n",
            "Epoch 501/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2726 - val_loss: 11.9141\n",
            "Epoch 502/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2247 - val_loss: 11.9537\n",
            "Epoch 503/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1237 - val_loss: 11.9224\n",
            "Epoch 504/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2122 - val_loss: 12.2083\n",
            "Epoch 505/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2026 - val_loss: 11.9912\n",
            "Epoch 506/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1176 - val_loss: 12.0159\n",
            "Epoch 507/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.2513 - val_loss: 11.9193\n",
            "Epoch 508/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2060 - val_loss: 11.9424\n",
            "Epoch 509/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2694 - val_loss: 11.9343\n",
            "Epoch 510/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1663 - val_loss: 11.8753\n",
            "Epoch 511/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2553 - val_loss: 11.8586\n",
            "Epoch 512/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1000 - val_loss: 11.8615\n",
            "Epoch 513/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3734 - val_loss: 11.9037\n",
            "Epoch 514/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1435 - val_loss: 11.8547\n",
            "Epoch 515/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1578 - val_loss: 12.1182\n",
            "Epoch 516/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2128 - val_loss: 11.8680\n",
            "Epoch 517/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2440 - val_loss: 11.8753\n",
            "Epoch 518/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1612 - val_loss: 12.0381\n",
            "Epoch 519/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1691 - val_loss: 11.8334\n",
            "Epoch 520/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2616 - val_loss: 11.8592\n",
            "Epoch 521/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2508 - val_loss: 11.9621\n",
            "Epoch 522/1000\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 12.1629 - val_loss: 11.9779\n",
            "Epoch 523/1000\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 12.2969 - val_loss: 11.9567\n",
            "Epoch 524/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1540 - val_loss: 12.0426\n",
            "Epoch 525/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2441 - val_loss: 11.8728\n",
            "Epoch 526/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1908 - val_loss: 11.8948\n",
            "Epoch 527/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.0982 - val_loss: 12.1060\n",
            "Epoch 528/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2641 - val_loss: 11.8471\n",
            "Epoch 529/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1770 - val_loss: 11.8575\n",
            "Epoch 530/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1797 - val_loss: 11.8361\n",
            "Epoch 531/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3354 - val_loss: 11.8517\n",
            "Epoch 532/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1780 - val_loss: 11.9169\n",
            "Epoch 533/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1601 - val_loss: 11.9320\n",
            "Epoch 534/1000\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 12.1628 - val_loss: 11.9532\n",
            "Epoch 535/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2092 - val_loss: 11.9229\n",
            "Epoch 536/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1935 - val_loss: 11.9119\n",
            "Epoch 537/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1763 - val_loss: 11.8872\n",
            "Epoch 538/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1338 - val_loss: 11.9510\n",
            "Epoch 539/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2250 - val_loss: 11.9996\n",
            "Epoch 540/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2017 - val_loss: 11.9514\n",
            "Epoch 541/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3063 - val_loss: 11.9860\n",
            "Epoch 542/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1925 - val_loss: 12.0116\n",
            "Epoch 543/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1654 - val_loss: 11.8556\n",
            "Epoch 544/1000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 12.1853 - val_loss: 11.8494\n",
            "Epoch 545/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.2583 - val_loss: 11.8279\n",
            "Epoch 546/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1529 - val_loss: 11.8897\n",
            "Epoch 547/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2458 - val_loss: 11.8913\n",
            "Epoch 548/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2956 - val_loss: 11.8758\n",
            "Epoch 549/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1053 - val_loss: 12.0612\n",
            "Epoch 550/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2275 - val_loss: 11.9012\n",
            "Epoch 551/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1590 - val_loss: 12.0022\n",
            "Epoch 552/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2322 - val_loss: 11.9975\n",
            "Epoch 553/1000\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 12.1845 - val_loss: 11.9535\n",
            "Epoch 554/1000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 12.2341 - val_loss: 11.9193\n",
            "Epoch 555/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.3743 - val_loss: 11.9430\n",
            "Epoch 556/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1456 - val_loss: 12.0270\n",
            "Epoch 557/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2388 - val_loss: 12.0850\n",
            "Epoch 558/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1591 - val_loss: 11.8701\n",
            "Epoch 559/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2469 - val_loss: 11.8238\n",
            "Epoch 560/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2102 - val_loss: 11.8267\n",
            "Epoch 561/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1714 - val_loss: 11.8355\n",
            "Epoch 562/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1288 - val_loss: 11.9976\n",
            "Epoch 563/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2529 - val_loss: 11.8557\n",
            "Epoch 564/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2096 - val_loss: 11.9114\n",
            "Epoch 565/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2245 - val_loss: 11.9943\n",
            "Epoch 566/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2384 - val_loss: 11.9851\n",
            "Epoch 567/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2159 - val_loss: 11.8401\n",
            "Epoch 568/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1756 - val_loss: 11.8639\n",
            "Epoch 569/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1765 - val_loss: 11.8806\n",
            "Epoch 570/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2285 - val_loss: 11.8905\n",
            "Epoch 571/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.0901 - val_loss: 11.9742\n",
            "Epoch 572/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2943 - val_loss: 11.9303\n",
            "Epoch 573/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2131 - val_loss: 11.8613\n",
            "Epoch 574/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1880 - val_loss: 11.9912\n",
            "Epoch 575/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2164 - val_loss: 12.0895\n",
            "Epoch 576/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2388 - val_loss: 12.0012\n",
            "Epoch 577/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1427 - val_loss: 12.1089\n",
            "Epoch 578/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1614 - val_loss: 11.9510\n",
            "Epoch 579/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2098 - val_loss: 11.9944\n",
            "Epoch 580/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2355 - val_loss: 12.0498\n",
            "Epoch 581/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2235 - val_loss: 11.9577\n",
            "Epoch 582/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2155 - val_loss: 11.8099\n",
            "Epoch 583/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1431 - val_loss: 11.9806\n",
            "Epoch 584/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.3517 - val_loss: 11.8482\n",
            "Epoch 585/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1813 - val_loss: 11.9105\n",
            "Epoch 586/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1536 - val_loss: 11.9526\n",
            "Epoch 587/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2236 - val_loss: 11.9216\n",
            "Epoch 588/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1554 - val_loss: 11.9457\n",
            "Epoch 589/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3192 - val_loss: 11.9517\n",
            "Epoch 590/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1295 - val_loss: 11.8778\n",
            "Epoch 591/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1766 - val_loss: 11.9791\n",
            "Epoch 592/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1914 - val_loss: 12.0666\n",
            "Epoch 593/1000\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 12.2875 - val_loss: 11.9684\n",
            "Epoch 594/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1139 - val_loss: 11.9667\n",
            "Epoch 595/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1775 - val_loss: 12.0488\n",
            "Epoch 596/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.2254 - val_loss: 12.0814\n",
            "Epoch 597/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2521 - val_loss: 11.9955\n",
            "Epoch 598/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2246 - val_loss: 11.9408\n",
            "Epoch 599/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1772 - val_loss: 11.9992\n",
            "Epoch 600/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2011 - val_loss: 12.0250\n",
            "Epoch 601/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2611 - val_loss: 11.8973\n",
            "Epoch 602/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3157 - val_loss: 11.9860\n",
            "Epoch 603/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1848 - val_loss: 11.9345\n",
            "Epoch 604/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.0936 - val_loss: 11.9388\n",
            "Epoch 605/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2775 - val_loss: 11.9452\n",
            "Epoch 606/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1815 - val_loss: 11.9306\n",
            "Epoch 607/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1852 - val_loss: 11.9658\n",
            "Epoch 608/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1832 - val_loss: 12.0868\n",
            "Epoch 609/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2357 - val_loss: 12.1079\n",
            "Epoch 610/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1961 - val_loss: 12.0001\n",
            "Epoch 611/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.3075 - val_loss: 11.9602\n",
            "Epoch 612/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2419 - val_loss: 11.9855\n",
            "Epoch 613/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.1476 - val_loss: 12.1224\n",
            "Epoch 614/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2634 - val_loss: 12.0539\n",
            "Epoch 615/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1962 - val_loss: 12.0153\n",
            "Epoch 616/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2152 - val_loss: 12.0339\n",
            "Epoch 617/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1587 - val_loss: 11.8935\n",
            "Epoch 618/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2861 - val_loss: 11.9396\n",
            "Epoch 619/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1952 - val_loss: 11.9507\n",
            "Epoch 620/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1766 - val_loss: 11.9402\n",
            "Epoch 621/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.2529 - val_loss: 11.9376\n",
            "Epoch 622/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1626 - val_loss: 11.8213\n",
            "Epoch 623/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1680 - val_loss: 11.9422\n",
            "Epoch 624/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2874 - val_loss: 11.8839\n",
            "Epoch 625/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1968 - val_loss: 11.8989\n",
            "Epoch 626/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1245 - val_loss: 11.9374\n",
            "Epoch 627/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2409 - val_loss: 11.9412\n",
            "Epoch 628/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2226 - val_loss: 12.0644\n",
            "Epoch 629/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1858 - val_loss: 11.9415\n",
            "Epoch 630/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.3348 - val_loss: 11.9092\n",
            "Epoch 631/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2068 - val_loss: 12.0064\n",
            "Epoch 632/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2443 - val_loss: 11.9390\n",
            "Epoch 633/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.2898 - val_loss: 11.8979\n",
            "Epoch 634/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2148 - val_loss: 11.8888\n",
            "Epoch 635/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1520 - val_loss: 11.8129\n",
            "Epoch 636/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3831 - val_loss: 11.8539\n",
            "Epoch 637/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3416 - val_loss: 11.8643\n",
            "Epoch 638/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1990 - val_loss: 11.8507\n",
            "Epoch 639/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1628 - val_loss: 11.9437\n",
            "Epoch 640/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1267 - val_loss: 11.8483\n",
            "Epoch 641/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2540 - val_loss: 11.9106\n",
            "Epoch 642/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2032 - val_loss: 11.9083\n",
            "Epoch 643/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2089 - val_loss: 11.8900\n",
            "Epoch 644/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1521 - val_loss: 11.9030\n",
            "Epoch 645/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1540 - val_loss: 12.0389\n",
            "Epoch 646/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.2719 - val_loss: 12.0028\n",
            "Epoch 647/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2129 - val_loss: 11.9747\n",
            "Epoch 648/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1732 - val_loss: 12.0659\n",
            "Epoch 649/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2359 - val_loss: 11.9327\n",
            "Epoch 650/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1939 - val_loss: 11.9400\n",
            "Epoch 651/1000\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 12.2024 - val_loss: 11.9160\n",
            "Epoch 652/1000\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 12.2662 - val_loss: 11.8719\n",
            "Epoch 653/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.2072 - val_loss: 11.9769\n",
            "Epoch 654/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1772 - val_loss: 12.0261\n",
            "Epoch 655/1000\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 12.2985 - val_loss: 11.8976\n",
            "Epoch 656/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1589 - val_loss: 11.8813\n",
            "Epoch 657/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1428 - val_loss: 11.8972\n",
            "Epoch 658/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3644 - val_loss: 11.9510\n",
            "Epoch 659/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2174 - val_loss: 11.9396\n",
            "Epoch 660/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1465 - val_loss: 11.8352\n",
            "Epoch 661/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2160 - val_loss: 11.8436\n",
            "Epoch 662/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2747 - val_loss: 11.7931\n",
            "Epoch 663/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2891 - val_loss: 11.9504\n",
            "Epoch 664/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3549 - val_loss: 11.9263\n",
            "Epoch 665/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1423 - val_loss: 11.9196\n",
            "Epoch 666/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1932 - val_loss: 11.8902\n",
            "Epoch 667/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2438 - val_loss: 11.9666\n",
            "Epoch 668/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2059 - val_loss: 11.9577\n",
            "Epoch 669/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1519 - val_loss: 11.9561\n",
            "Epoch 670/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.1548 - val_loss: 11.9788\n",
            "Epoch 671/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2578 - val_loss: 11.8979\n",
            "Epoch 672/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2596 - val_loss: 11.8963\n",
            "Epoch 673/1000\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 12.2716 - val_loss: 11.9159\n",
            "Epoch 674/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2132 - val_loss: 11.9446\n",
            "Epoch 675/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2243 - val_loss: 11.8658\n",
            "Epoch 676/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2629 - val_loss: 11.9999\n",
            "Epoch 677/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1984 - val_loss: 11.8607\n",
            "Epoch 678/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2635 - val_loss: 11.9599\n",
            "Epoch 679/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2174 - val_loss: 11.9605\n",
            "Epoch 680/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2131 - val_loss: 12.0337\n",
            "Epoch 681/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1720 - val_loss: 11.8722\n",
            "Epoch 682/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2341 - val_loss: 11.8508\n",
            "Epoch 683/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3196 - val_loss: 11.8774\n",
            "Epoch 684/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2004 - val_loss: 11.9205\n",
            "Epoch 685/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1928 - val_loss: 11.9440\n",
            "Epoch 686/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1884 - val_loss: 11.9166\n",
            "Epoch 687/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2574 - val_loss: 12.0241\n",
            "Epoch 688/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1990 - val_loss: 11.9397\n",
            "Epoch 689/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2007 - val_loss: 11.9194\n",
            "Epoch 690/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2243 - val_loss: 11.9148\n",
            "Epoch 691/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1732 - val_loss: 11.9306\n",
            "Epoch 692/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1679 - val_loss: 11.8427\n",
            "Epoch 693/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1283 - val_loss: 12.1237\n",
            "Epoch 694/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2230 - val_loss: 11.8367\n",
            "Epoch 695/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2234 - val_loss: 11.9612\n",
            "Epoch 696/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2026 - val_loss: 11.9320\n",
            "Epoch 697/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1532 - val_loss: 11.9757\n",
            "Epoch 698/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2399 - val_loss: 11.9872\n",
            "Epoch 699/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.2079 - val_loss: 11.9102\n",
            "Epoch 700/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2641 - val_loss: 11.9051\n",
            "Epoch 701/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1731 - val_loss: 11.9509\n",
            "Epoch 702/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1302 - val_loss: 11.8484\n",
            "Epoch 703/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1801 - val_loss: 11.9545\n",
            "Epoch 704/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2077 - val_loss: 11.8834\n",
            "Epoch 705/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1819 - val_loss: 11.9301\n",
            "Epoch 706/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2619 - val_loss: 11.9133\n",
            "Epoch 707/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2121 - val_loss: 11.9294\n",
            "Epoch 708/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1935 - val_loss: 11.8544\n",
            "Epoch 709/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2422 - val_loss: 11.8920\n",
            "Epoch 710/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1799 - val_loss: 11.8845\n",
            "Epoch 711/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3102 - val_loss: 11.8227\n",
            "Epoch 712/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1509 - val_loss: 11.8863\n",
            "Epoch 713/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2876 - val_loss: 11.8544\n",
            "Epoch 714/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1572 - val_loss: 11.8469\n",
            "Epoch 715/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3133 - val_loss: 11.8705\n",
            "Epoch 716/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2246 - val_loss: 11.9116\n",
            "Epoch 717/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1404 - val_loss: 11.9432\n",
            "Epoch 718/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1802 - val_loss: 11.9778\n",
            "Epoch 719/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2101 - val_loss: 11.8846\n",
            "Epoch 720/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1702 - val_loss: 12.0088\n",
            "Epoch 721/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1560 - val_loss: 12.0707\n",
            "Epoch 722/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3247 - val_loss: 12.0050\n",
            "Epoch 723/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1853 - val_loss: 11.9960\n",
            "Epoch 724/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2358 - val_loss: 11.9323\n",
            "Epoch 725/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2230 - val_loss: 11.9598\n",
            "Epoch 726/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1394 - val_loss: 11.9293\n",
            "Epoch 727/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1517 - val_loss: 11.9237\n",
            "Epoch 728/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1694 - val_loss: 11.9409\n",
            "Epoch 729/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2564 - val_loss: 11.8238\n",
            "Epoch 730/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2859 - val_loss: 11.9196\n",
            "Epoch 731/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2207 - val_loss: 12.0135\n",
            "Epoch 732/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1568 - val_loss: 11.8723\n",
            "Epoch 733/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2015 - val_loss: 11.9756\n",
            "Epoch 734/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2114 - val_loss: 11.9867\n",
            "Epoch 735/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3591 - val_loss: 11.9815\n",
            "Epoch 736/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2721 - val_loss: 11.9016\n",
            "Epoch 737/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2063 - val_loss: 11.9304\n",
            "Epoch 738/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2428 - val_loss: 11.9814\n",
            "Epoch 739/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2298 - val_loss: 11.8380\n",
            "Epoch 740/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2131 - val_loss: 11.8902\n",
            "Epoch 741/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1429 - val_loss: 11.9143\n",
            "Epoch 742/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1354 - val_loss: 11.9195\n",
            "Epoch 743/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2360 - val_loss: 12.0284\n",
            "Epoch 744/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2084 - val_loss: 11.8521\n",
            "Epoch 745/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1642 - val_loss: 11.8706\n",
            "Epoch 746/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2576 - val_loss: 12.0014\n",
            "Epoch 747/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2324 - val_loss: 11.8626\n",
            "Epoch 748/1000\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 12.2417 - val_loss: 11.8925\n",
            "Epoch 749/1000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 12.2652 - val_loss: 11.8662\n",
            "Epoch 750/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2235 - val_loss: 11.8786\n",
            "Epoch 751/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2977 - val_loss: 11.8605\n",
            "Epoch 752/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1301 - val_loss: 12.0158\n",
            "Epoch 753/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1631 - val_loss: 11.9077\n",
            "Epoch 754/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2087 - val_loss: 11.9530\n",
            "Epoch 755/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1999 - val_loss: 11.8547\n",
            "Epoch 756/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2391 - val_loss: 11.8398\n",
            "Epoch 757/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1991 - val_loss: 11.8491\n",
            "Epoch 758/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2168 - val_loss: 11.9292\n",
            "Epoch 759/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1491 - val_loss: 11.8970\n",
            "Epoch 760/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2761 - val_loss: 11.8322\n",
            "Epoch 761/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2223 - val_loss: 11.9317\n",
            "Epoch 762/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1716 - val_loss: 11.8967\n",
            "Epoch 763/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2987 - val_loss: 11.9528\n",
            "Epoch 764/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1500 - val_loss: 11.9775\n",
            "Epoch 765/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3695 - val_loss: 11.8485\n",
            "Epoch 766/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1228 - val_loss: 11.9116\n",
            "Epoch 767/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3446 - val_loss: 11.8620\n",
            "Epoch 768/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1914 - val_loss: 11.8771\n",
            "Epoch 769/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1636 - val_loss: 11.8751\n",
            "Epoch 770/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2714 - val_loss: 11.8648\n",
            "Epoch 771/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1626 - val_loss: 11.8960\n",
            "Epoch 772/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2964 - val_loss: 11.9133\n",
            "Epoch 773/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2039 - val_loss: 11.9102\n",
            "Epoch 774/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3096 - val_loss: 11.9714\n",
            "Epoch 775/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1321 - val_loss: 11.9082\n",
            "Epoch 776/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1871 - val_loss: 11.9804\n",
            "Epoch 777/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2086 - val_loss: 12.0384\n",
            "Epoch 778/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2097 - val_loss: 12.0426\n",
            "Epoch 779/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1753 - val_loss: 11.9814\n",
            "Epoch 780/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1433 - val_loss: 11.9478\n",
            "Epoch 781/1000\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 12.1906 - val_loss: 11.9422\n",
            "Epoch 782/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2418 - val_loss: 11.9816\n",
            "Epoch 783/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3138 - val_loss: 11.9402\n",
            "Epoch 784/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2659 - val_loss: 11.9443\n",
            "Epoch 785/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2070 - val_loss: 12.0430\n",
            "Epoch 786/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2027 - val_loss: 11.9218\n",
            "Epoch 787/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1449 - val_loss: 11.8952\n",
            "Epoch 788/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1587 - val_loss: 12.1142\n",
            "Epoch 789/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2024 - val_loss: 12.2248\n",
            "Epoch 790/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2814 - val_loss: 11.8981\n",
            "Epoch 791/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2784 - val_loss: 11.9083\n",
            "Epoch 792/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1862 - val_loss: 11.9803\n",
            "Epoch 793/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2476 - val_loss: 11.9876\n",
            "Epoch 794/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1898 - val_loss: 12.0122\n",
            "Epoch 795/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2086 - val_loss: 11.9482\n",
            "Epoch 796/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.2455 - val_loss: 11.9031\n",
            "Epoch 797/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2289 - val_loss: 12.0769\n",
            "Epoch 798/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2046 - val_loss: 11.9529\n",
            "Epoch 799/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1539 - val_loss: 11.9582\n",
            "Epoch 800/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.2528 - val_loss: 11.9658\n",
            "Epoch 801/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1500 - val_loss: 11.9222\n",
            "Epoch 802/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2530 - val_loss: 12.0151\n",
            "Epoch 803/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2844 - val_loss: 11.9707\n",
            "Epoch 804/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1485 - val_loss: 11.9465\n",
            "Epoch 805/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2319 - val_loss: 11.8961\n",
            "Epoch 806/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1980 - val_loss: 11.9574\n",
            "Epoch 807/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2498 - val_loss: 12.0231\n",
            "Epoch 808/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.1758 - val_loss: 12.0248\n",
            "Epoch 809/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 12.1844 - val_loss: 11.8895\n",
            "Epoch 810/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12.2356 - val_loss: 11.8742\n",
            "Epoch 811/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.2294 - val_loss: 11.8142\n",
            "Epoch 812/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2074 - val_loss: 11.9563\n",
            "Epoch 813/1000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 12.2004 - val_loss: 11.9614\n",
            "Epoch 814/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2251 - val_loss: 11.9028\n",
            "Epoch 815/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2459 - val_loss: 11.9626\n",
            "Epoch 816/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1776 - val_loss: 12.1109\n",
            "Epoch 817/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2023 - val_loss: 12.0103\n",
            "Epoch 818/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1780 - val_loss: 11.9907\n",
            "Epoch 819/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1654 - val_loss: 11.9525\n",
            "Epoch 820/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1969 - val_loss: 12.0148\n",
            "Epoch 821/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2616 - val_loss: 12.0058\n",
            "Epoch 822/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1180 - val_loss: 11.9200\n",
            "Epoch 823/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2959 - val_loss: 11.9044\n",
            "Epoch 824/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2666 - val_loss: 11.9227\n",
            "Epoch 825/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1918 - val_loss: 12.1076\n",
            "Epoch 826/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1812 - val_loss: 11.9485\n",
            "Epoch 827/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1759 - val_loss: 11.9313\n",
            "Epoch 828/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2541 - val_loss: 11.9062\n",
            "Epoch 829/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1952 - val_loss: 11.9736\n",
            "Epoch 830/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2335 - val_loss: 11.8984\n",
            "Epoch 831/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2270 - val_loss: 11.9154\n",
            "Epoch 832/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2362 - val_loss: 11.9057\n",
            "Epoch 833/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3356 - val_loss: 11.8943\n",
            "Epoch 834/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1595 - val_loss: 11.9381\n",
            "Epoch 835/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1863 - val_loss: 11.8667\n",
            "Epoch 836/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1868 - val_loss: 11.8678\n",
            "Epoch 837/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1677 - val_loss: 12.0294\n",
            "Epoch 838/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1788 - val_loss: 11.9867\n",
            "Epoch 839/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2805 - val_loss: 12.0188\n",
            "Epoch 840/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1920 - val_loss: 11.8737\n",
            "Epoch 841/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2596 - val_loss: 11.8795\n",
            "Epoch 842/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2515 - val_loss: 11.8948\n",
            "Epoch 843/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1956 - val_loss: 11.9860\n",
            "Epoch 844/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2940 - val_loss: 11.9196\n",
            "Epoch 845/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2388 - val_loss: 11.9560\n",
            "Epoch 846/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2372 - val_loss: 11.9363\n",
            "Epoch 847/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2072 - val_loss: 11.9254\n",
            "Epoch 848/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2770 - val_loss: 11.9124\n",
            "Epoch 849/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1694 - val_loss: 11.9752\n",
            "Epoch 850/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1554 - val_loss: 11.9242\n",
            "Epoch 851/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1781 - val_loss: 11.9567\n",
            "Epoch 852/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2610 - val_loss: 11.8868\n",
            "Epoch 853/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1354 - val_loss: 12.0128\n",
            "Epoch 854/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3271 - val_loss: 11.9965\n",
            "Epoch 855/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1930 - val_loss: 11.8797\n",
            "Epoch 856/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2103 - val_loss: 11.9841\n",
            "Epoch 857/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2311 - val_loss: 11.9973\n",
            "Epoch 858/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2400 - val_loss: 11.9767\n",
            "Epoch 859/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1396 - val_loss: 11.9175\n",
            "Epoch 860/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1338 - val_loss: 12.0440\n",
            "Epoch 861/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2972 - val_loss: 11.8945\n",
            "Epoch 862/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2322 - val_loss: 11.9619\n",
            "Epoch 863/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1898 - val_loss: 11.8991\n",
            "Epoch 864/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2066 - val_loss: 11.8743\n",
            "Epoch 865/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1953 - val_loss: 11.8824\n",
            "Epoch 866/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1689 - val_loss: 11.8785\n",
            "Epoch 867/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1740 - val_loss: 11.8565\n",
            "Epoch 868/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1737 - val_loss: 11.9275\n",
            "Epoch 869/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.3278 - val_loss: 11.8897\n",
            "Epoch 870/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2728 - val_loss: 11.9269\n",
            "Epoch 871/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2785 - val_loss: 11.9482\n",
            "Epoch 872/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1617 - val_loss: 11.9707\n",
            "Epoch 873/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2423 - val_loss: 11.9485\n",
            "Epoch 874/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2129 - val_loss: 11.9253\n",
            "Epoch 875/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2526 - val_loss: 11.8480\n",
            "Epoch 876/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1800 - val_loss: 12.0774\n",
            "Epoch 877/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1370 - val_loss: 11.9261\n",
            "Epoch 878/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3383 - val_loss: 11.9276\n",
            "Epoch 879/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2032 - val_loss: 11.8528\n",
            "Epoch 880/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2798 - val_loss: 11.8423\n",
            "Epoch 881/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1819 - val_loss: 11.7811\n",
            "Epoch 882/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2454 - val_loss: 11.8623\n",
            "Epoch 883/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2196 - val_loss: 11.9022\n",
            "Epoch 884/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1099 - val_loss: 11.9704\n",
            "Epoch 885/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2336 - val_loss: 11.8747\n",
            "Epoch 886/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1484 - val_loss: 11.8910\n",
            "Epoch 887/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2054 - val_loss: 11.9684\n",
            "Epoch 888/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1552 - val_loss: 11.9377\n",
            "Epoch 889/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2463 - val_loss: 11.8137\n",
            "Epoch 890/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1844 - val_loss: 11.8642\n",
            "Epoch 891/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2483 - val_loss: 11.8679\n",
            "Epoch 892/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1932 - val_loss: 11.8872\n",
            "Epoch 893/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.0698 - val_loss: 12.0495\n",
            "Epoch 894/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.3721 - val_loss: 11.9508\n",
            "Epoch 895/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1165 - val_loss: 11.8897\n",
            "Epoch 896/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1965 - val_loss: 11.9759\n",
            "Epoch 897/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1794 - val_loss: 11.9940\n",
            "Epoch 898/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1680 - val_loss: 11.8871\n",
            "Epoch 899/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2011 - val_loss: 11.8207\n",
            "Epoch 900/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2240 - val_loss: 11.8007\n",
            "Epoch 901/1000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 12.1748 - val_loss: 11.9454\n",
            "Epoch 902/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2621 - val_loss: 11.9164\n",
            "Epoch 903/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2315 - val_loss: 11.8786\n",
            "Epoch 904/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12.1653 - val_loss: 11.8959\n",
            "Epoch 905/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2233 - val_loss: 11.8658\n",
            "Epoch 906/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 12.2330 - val_loss: 11.9049\n",
            "Epoch 907/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1959 - val_loss: 11.8567\n",
            "Epoch 908/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1170 - val_loss: 11.9302\n",
            "Epoch 909/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1929 - val_loss: 11.8931\n",
            "Epoch 910/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1965 - val_loss: 11.9327\n",
            "Epoch 911/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2106 - val_loss: 11.8384\n",
            "Epoch 912/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2049 - val_loss: 11.8441\n",
            "Epoch 913/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2563 - val_loss: 11.8836\n",
            "Epoch 914/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1902 - val_loss: 11.9399\n",
            "Epoch 915/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1849 - val_loss: 12.0150\n",
            "Epoch 916/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1647 - val_loss: 11.9577\n",
            "Epoch 917/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2361 - val_loss: 12.0150\n",
            "Epoch 918/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1598 - val_loss: 11.9327\n",
            "Epoch 919/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2933 - val_loss: 11.8876\n",
            "Epoch 920/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2121 - val_loss: 11.9557\n",
            "Epoch 921/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1444 - val_loss: 11.8924\n",
            "Epoch 922/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2551 - val_loss: 11.8711\n",
            "Epoch 923/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2626 - val_loss: 11.9290\n",
            "Epoch 924/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1921 - val_loss: 11.9540\n",
            "Epoch 925/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1878 - val_loss: 11.8865\n",
            "Epoch 926/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1998 - val_loss: 11.8769\n",
            "Epoch 927/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2234 - val_loss: 12.0078\n",
            "Epoch 928/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2108 - val_loss: 11.9822\n",
            "Epoch 929/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1199 - val_loss: 11.9347\n",
            "Epoch 930/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1891 - val_loss: 11.9024\n",
            "Epoch 931/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2610 - val_loss: 11.9167\n",
            "Epoch 932/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1547 - val_loss: 11.8303\n",
            "Epoch 933/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3304 - val_loss: 11.8944\n",
            "Epoch 934/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2579 - val_loss: 11.8848\n",
            "Epoch 935/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1844 - val_loss: 11.9101\n",
            "Epoch 936/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.1585 - val_loss: 11.9753\n",
            "Epoch 937/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2036 - val_loss: 11.9832\n",
            "Epoch 938/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2557 - val_loss: 11.9429\n",
            "Epoch 939/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2036 - val_loss: 11.8851\n",
            "Epoch 940/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2440 - val_loss: 11.8584\n",
            "Epoch 941/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1839 - val_loss: 11.8501\n",
            "Epoch 942/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1941 - val_loss: 12.1731\n",
            "Epoch 943/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2028 - val_loss: 11.9005\n",
            "Epoch 944/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1761 - val_loss: 12.0605\n",
            "Epoch 945/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2098 - val_loss: 11.9182\n",
            "Epoch 946/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1480 - val_loss: 11.9514\n",
            "Epoch 947/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2373 - val_loss: 11.8847\n",
            "Epoch 948/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2353 - val_loss: 11.9463\n",
            "Epoch 949/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1942 - val_loss: 11.9123\n",
            "Epoch 950/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2330 - val_loss: 11.9042\n",
            "Epoch 951/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2438 - val_loss: 11.9120\n",
            "Epoch 952/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1919 - val_loss: 11.9398\n",
            "Epoch 953/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1735 - val_loss: 11.9286\n",
            "Epoch 954/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2298 - val_loss: 11.8434\n",
            "Epoch 955/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1794 - val_loss: 11.9183\n",
            "Epoch 956/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2045 - val_loss: 11.9535\n",
            "Epoch 957/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2357 - val_loss: 11.9223\n",
            "Epoch 958/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2243 - val_loss: 11.9819\n",
            "Epoch 959/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2887 - val_loss: 11.9701\n",
            "Epoch 960/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2456 - val_loss: 11.9005\n",
            "Epoch 961/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2343 - val_loss: 11.8279\n",
            "Epoch 962/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2207 - val_loss: 11.8024\n",
            "Epoch 963/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2042 - val_loss: 11.7969\n",
            "Epoch 964/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1850 - val_loss: 11.8661\n",
            "Epoch 965/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1606 - val_loss: 11.9001\n",
            "Epoch 966/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1313 - val_loss: 12.0847\n",
            "Epoch 967/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1987 - val_loss: 11.8934\n",
            "Epoch 968/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1605 - val_loss: 11.9916\n",
            "Epoch 969/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1630 - val_loss: 11.9189\n",
            "Epoch 970/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1933 - val_loss: 11.9771\n",
            "Epoch 971/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1980 - val_loss: 11.9920\n",
            "Epoch 972/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1660 - val_loss: 11.8776\n",
            "Epoch 973/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2526 - val_loss: 11.9190\n",
            "Epoch 974/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2197 - val_loss: 11.8977\n",
            "Epoch 975/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3688 - val_loss: 11.8805\n",
            "Epoch 976/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1386 - val_loss: 11.8627\n",
            "Epoch 977/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.0788 - val_loss: 11.9803\n",
            "Epoch 978/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2763 - val_loss: 11.9061\n",
            "Epoch 979/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1756 - val_loss: 12.0027\n",
            "Epoch 980/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.2683 - val_loss: 11.9260\n",
            "Epoch 981/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2357 - val_loss: 11.8676\n",
            "Epoch 982/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1985 - val_loss: 12.0079\n",
            "Epoch 983/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1752 - val_loss: 11.9956\n",
            "Epoch 984/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1348 - val_loss: 12.0232\n",
            "Epoch 985/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.2321 - val_loss: 12.1405\n",
            "Epoch 986/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1932 - val_loss: 11.9146\n",
            "Epoch 987/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12.1858 - val_loss: 11.9430\n",
            "Epoch 988/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2038 - val_loss: 11.9694\n",
            "Epoch 989/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2485 - val_loss: 11.9143\n",
            "Epoch 990/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1840 - val_loss: 11.8547\n",
            "Epoch 991/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1879 - val_loss: 12.0074\n",
            "Epoch 992/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1494 - val_loss: 12.0250\n",
            "Epoch 993/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1275 - val_loss: 11.9219\n",
            "Epoch 994/1000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 12.1364 - val_loss: 11.8785\n",
            "Epoch 995/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1639 - val_loss: 12.0090\n",
            "Epoch 996/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.3622 - val_loss: 11.9277\n",
            "Epoch 997/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.1633 - val_loss: 11.9452\n",
            "Epoch 998/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.2506 - val_loss: 11.9141\n",
            "Epoch 999/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 12.2580 - val_loss: 11.9135\n",
            "Epoch 1000/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12.0898 - val_loss: 11.8973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# patience 매개변수는 성능 향상을 체크할 에포크 횟수.\n",
        "# EarlyStopping() : val loss가 어느 값 이후로 줄어들지 않으면 학습을 종료시키는 함수\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
        "                    validation_split = 0.2, verbose=0, callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1zCztSPPlnk",
        "outputId": "05ebcd46-eb1a-49cb-b39c-d50c8e5d9d67"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f3759dee0>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=model.evaluate(normed_test_data, test_labels, verbose=2) #평가 함수 evaluate()\n",
        "print('테스트 세트의 평균 절대 오차: {:5.2f} MPG'.format(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8W4fsqBRLT1",
        "outputId": "18bc30da-de2a-4667-9d83-2106ddd96690"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 0s - loss: 10.8096 - 17ms/epoch - 6ms/step\n",
            "테스트 세트의 평균 절대 오차: 10.81 MPG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측\n",
        "- 셈플을 사용해 MPG 값 예측"
      ],
      "metadata": {
        "id": "FU_ddJUuRwTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(normed_test_data).flatten()\n",
        "\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0,plt.xlim()[1]])\n",
        "plt.ylim([0,plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "s3frasgkQ-sn",
        "outputId": "6f094602-3a2d-4fa8-8c69-e1eb82c6b54a"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cn-zFfQhR4qt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}