{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DcYm7M96CfxT",
        "-FChiqMeFLki"
      ],
      "authorship_tag": "ABX9TyPjdgPU6O4Q9pp6IQR6TJR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WarpHero/AI_Training/blob/main/Non_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow Tutorial\n",
        "https://www.tensorflow.org/tutorials/keras/regression?hl=ko"
      ],
      "metadata": {
        "id": "HOHsZmazDtM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MyOmP7sBjQS",
        "outputId": "644c2ff9-d6e7-4181-e01e-9ec9d3022400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자동차 연비 예측하기"
      ],
      "metadata": {
        "id": "D0chiUouCEhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 구하기\n",
        "- 먼저 데이터셋을 다운로드 하기"
      ],
      "metadata": {
        "id": "DcYm7M96CfxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
        "dataset_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "4XkQQzNsB2mG",
        "outputId": "ff54b575-0982-4eb3-9246-f7c89365e24a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\n",
            "30286/30286 [==============================] - 0s 2us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/auto-mpg.data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
        "                      na_values = \"?\", comment='\\t',\n",
        "                      sep=\" \", skipinitialspace=True)\n",
        "\n",
        "dataset = raw_dataset.copy()\n",
        "dataset.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_UP9ztr3DAIu",
        "outputId": "5d1c7318-5dfd-44c1-c64f-a31de4a60dc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
              "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
              "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
              "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
              "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
              "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
              "\n",
              "     Model Year  Origin  \n",
              "393          82       1  \n",
              "394          82       2  \n",
              "395          82       1  \n",
              "396          82       1  \n",
              "397          82       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-062594e7-e283-4da8-bc6a-51e7fe44cce1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>27.0</td>\n",
              "      <td>4</td>\n",
              "      <td>140.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>2790.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>44.0</td>\n",
              "      <td>4</td>\n",
              "      <td>97.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2130.0</td>\n",
              "      <td>24.6</td>\n",
              "      <td>82</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>32.0</td>\n",
              "      <td>4</td>\n",
              "      <td>135.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>2295.0</td>\n",
              "      <td>11.6</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>28.0</td>\n",
              "      <td>4</td>\n",
              "      <td>120.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>2625.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>31.0</td>\n",
              "      <td>4</td>\n",
              "      <td>119.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>2720.0</td>\n",
              "      <td>19.4</td>\n",
              "      <td>82</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-062594e7-e283-4da8-bc6a-51e7fe44cce1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-062594e7-e283-4da8-bc6a-51e7fe44cce1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-062594e7-e283-4da8-bc6a-51e7fe44cce1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EV3xMo0eD1q2",
        "outputId": "ac7345d3-6cae-4953-f2ca-1b7db75188ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
              "0  18.0          8         307.0       130.0  3504.0          12.0   \n",
              "1  15.0          8         350.0       165.0  3693.0          11.5   \n",
              "2  18.0          8         318.0       150.0  3436.0          11.0   \n",
              "3  16.0          8         304.0       150.0  3433.0          12.0   \n",
              "4  17.0          8         302.0       140.0  3449.0          10.5   \n",
              "\n",
              "   Model Year  Origin  \n",
              "0          70       1  \n",
              "1          70       1  \n",
              "2          70       1  \n",
              "3          70       1  \n",
              "4          70       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b24a60c-2f67-44be-b55c-40899161908d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "      <th>Origin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b24a60c-2f67-44be-b55c-40899161908d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5b24a60c-2f67-44be-b55c-40899161908d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5b24a60c-2f67-44be-b55c-40899161908d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkVclSYQEeo2",
        "outputId": "829651f7-6b90-45e3-c2be-99bd56c14d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "398"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "비정상 데이터 제거 dropna"
      ],
      "metadata": {
        "id": "vW3LJ2xzEqh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=dataset.dropna()"
      ],
      "metadata": {
        "id": "J1BerpewEj2t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjCxZ1YpEpfF",
        "outputId": "6787a40d-a067-4954-dbca-9530db3f396f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "392"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Origin data 제거\n",
        "- \"Origin\" 열은 수치형이 아니고 범주형이므로 원-핫 인코딩(one-hot encoding)으로 변환하겠습니다:"
      ],
      "metadata": {
        "id": "xB0Z9ZIwFCb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.pop('Origin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSwKwGENExa9",
        "outputId": "6ca5ee83-c128-446c-ea51-4a343d8e4784"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "393    1\n",
              "394    2\n",
              "395    1\n",
              "396    1\n",
              "397    1\n",
              "Name: Origin, Length: 392, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wzy-Pi0zE7JY",
        "outputId": "d5c8345f-4bb2-44d8-dc5a-1e9cacdfb0c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
              "0    18.0          8         307.0       130.0  3504.0          12.0   \n",
              "1    15.0          8         350.0       165.0  3693.0          11.5   \n",
              "2    18.0          8         318.0       150.0  3436.0          11.0   \n",
              "3    16.0          8         304.0       150.0  3433.0          12.0   \n",
              "4    17.0          8         302.0       140.0  3449.0          10.5   \n",
              "..    ...        ...           ...         ...     ...           ...   \n",
              "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
              "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
              "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
              "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
              "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
              "\n",
              "     Model Year  \n",
              "0            70  \n",
              "1            70  \n",
              "2            70  \n",
              "3            70  \n",
              "4            70  \n",
              "..          ...  \n",
              "393          82  \n",
              "394          82  \n",
              "395          82  \n",
              "396          82  \n",
              "397          82  \n",
              "\n",
              "[392 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13473a3b-ceb8-49f0-aac1-fc9f960b217c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MPG</th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693.0</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449.0</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>27.0</td>\n",
              "      <td>4</td>\n",
              "      <td>140.0</td>\n",
              "      <td>86.0</td>\n",
              "      <td>2790.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>44.0</td>\n",
              "      <td>4</td>\n",
              "      <td>97.0</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2130.0</td>\n",
              "      <td>24.6</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>32.0</td>\n",
              "      <td>4</td>\n",
              "      <td>135.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>2295.0</td>\n",
              "      <td>11.6</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>28.0</td>\n",
              "      <td>4</td>\n",
              "      <td>120.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>2625.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>31.0</td>\n",
              "      <td>4</td>\n",
              "      <td>119.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>2720.0</td>\n",
              "      <td>19.4</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>392 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13473a3b-ceb8-49f0-aac1-fc9f960b217c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13473a3b-ceb8-49f0-aac1-fc9f960b217c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13473a3b-ceb8-49f0-aac1-fc9f960b217c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋을 훈련 세트와 테스트 세트로 분할\n",
        "- 이제 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "- 테스트 세트는 모델을 최종적으로 평가할 때 사용"
      ],
      "metadata": {
        "id": "-FChiqMeFLki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)"
      ],
      "metadata": {
        "id": "tS8ToxcjFIMc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-9Lpzg2Fipm",
        "outputId": "f77f686b-bd2d-4341-903f-d786ef70a1c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(314, 78)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정규화 하기\n",
        "- dataset의 숫자들의 편차가 크기 때문에 어느정도 크기를 통일시켜 줄 필요가 있음\n",
        "\n",
        "- 특성의 스케일과 범위가 다르면 정규화(normalization)하는 것이 권장됩니다. 특성을 정규화하지 않아도 모델이 수렴할 수 있지만, 훈련시키기 어렵고 입력 단위에 의존적인 모델이 만들어집니다.\n",
        "\n",
        "노트: 의도적으로 훈련 세트만 사용하여 통계치를 생성했습니다. 이 통계는 테스트 세트를 정규화할 때에도 사용됩니다. 이는 테스트 세트를 모델이 훈련에 사용했던 것과 동일한 분포로 투영하기 위해서입니다.\n",
        "\n",
        "\n",
        "-> 통계적인 값이 필요함(avg, std 같은 것들)\n",
        "\n",
        "-> describe()를 이용해 구해줌"
      ],
      "metadata": {
        "id": "XPdxW2zpF46K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_stats = train_dataset.describe()\n",
        "train_stats.pop(\"MPG\")\n",
        "train_stats = train_stats.transpose()\n",
        "train_stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "m7XpHoAUFo8d",
        "outputId": "e9d45fc7-8f6f-4e27-8682-ac228275e929"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              count         mean         std     min      25%     50%  \\\n",
              "Cylinders     314.0     5.477707    1.699788     3.0     4.00     4.0   \n",
              "Displacement  314.0   195.318471  104.331589    68.0   105.50   151.0   \n",
              "Horsepower    314.0   104.869427   38.096214    46.0    76.25    94.5   \n",
              "Weight        314.0  2990.251592  843.898596  1649.0  2256.50  2822.5   \n",
              "Acceleration  314.0    15.559236    2.789230     8.0    13.80    15.5   \n",
              "Model Year    314.0    75.898089    3.675642    70.0    73.00    76.0   \n",
              "\n",
              "                  75%     max  \n",
              "Cylinders        8.00     8.0  \n",
              "Displacement   265.75   455.0  \n",
              "Horsepower     128.00   225.0  \n",
              "Weight        3608.00  5140.0  \n",
              "Acceleration    17.20    24.8  \n",
              "Model Year      79.00    82.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f5ebf28-4c5e-4459-b083-c2fbc29834c1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Cylinders</th>\n",
              "      <td>314.0</td>\n",
              "      <td>5.477707</td>\n",
              "      <td>1.699788</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.00</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Displacement</th>\n",
              "      <td>314.0</td>\n",
              "      <td>195.318471</td>\n",
              "      <td>104.331589</td>\n",
              "      <td>68.0</td>\n",
              "      <td>105.50</td>\n",
              "      <td>151.0</td>\n",
              "      <td>265.75</td>\n",
              "      <td>455.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Horsepower</th>\n",
              "      <td>314.0</td>\n",
              "      <td>104.869427</td>\n",
              "      <td>38.096214</td>\n",
              "      <td>46.0</td>\n",
              "      <td>76.25</td>\n",
              "      <td>94.5</td>\n",
              "      <td>128.00</td>\n",
              "      <td>225.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight</th>\n",
              "      <td>314.0</td>\n",
              "      <td>2990.251592</td>\n",
              "      <td>843.898596</td>\n",
              "      <td>1649.0</td>\n",
              "      <td>2256.50</td>\n",
              "      <td>2822.5</td>\n",
              "      <td>3608.00</td>\n",
              "      <td>5140.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Acceleration</th>\n",
              "      <td>314.0</td>\n",
              "      <td>15.559236</td>\n",
              "      <td>2.789230</td>\n",
              "      <td>8.0</td>\n",
              "      <td>13.80</td>\n",
              "      <td>15.5</td>\n",
              "      <td>17.20</td>\n",
              "      <td>24.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model Year</th>\n",
              "      <td>314.0</td>\n",
              "      <td>75.898089</td>\n",
              "      <td>3.675642</td>\n",
              "      <td>70.0</td>\n",
              "      <td>73.00</td>\n",
              "      <td>76.0</td>\n",
              "      <td>79.00</td>\n",
              "      <td>82.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f5ebf28-4c5e-4459-b083-c2fbc29834c1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8f5ebf28-4c5e-4459-b083-c2fbc29834c1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8f5ebf28-4c5e-4459-b083-c2fbc29834c1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 특성과 레이블 분리하기\n",
        "- 특성에서 타깃 값 또는 \"레이블\"을 분리합니다. 이 레이블을 예측하기 위해 모델을 훈련시킬 것입니다.\n",
        "\n",
        "ex) mpg는 정규화를 하지 말아야 하므로 제외시켜줌 "
      ],
      "metadata": {
        "id": "crBtIIUyHAbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = train_dataset.pop('MPG')\n",
        "test_labels = test_dataset.pop('MPG')"
      ],
      "metadata": {
        "id": "LPmD4b4CG9aq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 정규화"
      ],
      "metadata": {
        "id": "X28kdRx3HdwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(x):\n",
        "  return (x - train_stats['mean']) / train_stats['std']\n",
        "normed_train_data = norm(train_dataset)\n",
        "normed_test_data = norm(test_dataset)"
      ],
      "metadata": {
        "id": "RNu8Z9ufHcA9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normed_train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cs28JGVOHneh",
        "outputId": "b55bebd6-92b8-48b8-f645-0d9e81f8d977"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Cylinders  Displacement  Horsepower    Weight  Acceleration  Model Year\n",
              "146  -0.869348     -1.009459   -0.784052 -1.025303     -0.379759   -0.516397\n",
              "282  -0.869348     -0.530218   -0.442811 -0.118796      0.624102    0.843910\n",
              "69    1.483887      1.482595    1.447140  1.736877     -0.738281   -1.060519\n",
              "378  -0.869348     -0.865687   -1.099044 -1.025303     -0.308055    1.660094\n",
              "331  -0.869348     -0.942365   -0.994047 -1.001603      0.875068    1.115971"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2305b9ac-7b6e-461b-a8e6-37ee20d8b020\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cylinders</th>\n",
              "      <th>Displacement</th>\n",
              "      <th>Horsepower</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Acceleration</th>\n",
              "      <th>Model Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>-0.869348</td>\n",
              "      <td>-1.009459</td>\n",
              "      <td>-0.784052</td>\n",
              "      <td>-1.025303</td>\n",
              "      <td>-0.379759</td>\n",
              "      <td>-0.516397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>-0.869348</td>\n",
              "      <td>-0.530218</td>\n",
              "      <td>-0.442811</td>\n",
              "      <td>-0.118796</td>\n",
              "      <td>0.624102</td>\n",
              "      <td>0.843910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>1.483887</td>\n",
              "      <td>1.482595</td>\n",
              "      <td>1.447140</td>\n",
              "      <td>1.736877</td>\n",
              "      <td>-0.738281</td>\n",
              "      <td>-1.060519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>-0.869348</td>\n",
              "      <td>-0.865687</td>\n",
              "      <td>-1.099044</td>\n",
              "      <td>-1.025303</td>\n",
              "      <td>-0.308055</td>\n",
              "      <td>1.660094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>-0.869348</td>\n",
              "      <td>-0.942365</td>\n",
              "      <td>-0.994047</td>\n",
              "      <td>-1.001603</td>\n",
              "      <td>0.875068</td>\n",
              "      <td>1.115971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2305b9ac-7b6e-461b-a8e6-37ee20d8b020')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2305b9ac-7b6e-461b-a8e6-37ee20d8b020 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2305b9ac-7b6e-461b-a8e6-37ee20d8b020');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델"
      ],
      "metadata": {
        "id": "nTGEHP2RJ4gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델만들기\n",
        "- 모델을 구성해 보죠. 여기에서는 두 개의 완전 연결(densely connected) 은닉층으로 Sequential 모델을 만들겠습니다. 출력 층은 하나의 연속적인 값을 반환합니다. 나중에 두 번째 모델을 만들기 쉽도록 build_model 함수로 모델 구성 단계를 감싸겠습니다."
      ],
      "metadata": {
        "id": "9aygSWZYJ79e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normed_train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrxr8k8qH3Zl",
        "outputId": "0911d48a-d9b6-4700-cad2-5aa8e0fc986f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(314, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data feature의 size가 중요하기 때문에\n",
        "Input으로 넣어준다"
      ],
      "metadata": {
        "id": "7GzU4j21KWpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=keras.Input(shape=(normed_train_data.shape[1],))\n",
        "h=layers.Dense(64)(inputs)\n",
        "# gradient vanishing: hidden layer의 depth가 깊어질수록 학습이 잘 안되는 현상이 발생할 수 있음\n",
        "# ReLU : input이 positive면 gradient=1이므로 gradient vanishing 방지 가능\n",
        "h=layers.Activation('relu')(h)\n",
        "outputs=layers.Dense(1)(h) "
      ],
      "metadata": {
        "id": "MnswcjTMKRRF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(loss='mse', optimizer=tf.keras.optimizers.RMSprop(0.001))"
      ],
      "metadata": {
        "id": "eTfRw7lJLWi-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 확인\n",
        "\n",
        "- summary 메서드 사용해 모델에 대한 간단한 정보를 출력한다."
      ],
      "metadata": {
        "id": "-dSYhISfLuGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary() #input -> 중간레이어 -> output(결과값 1개)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXzhNceELr7U",
        "outputId": "cc4f12e8-9655-440d-8bd6-a7c25e3691e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 6)]               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                448       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 513\n",
            "Trainable params: 513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- visualize 함수 plot_model()\n",
        "   레이어 별로 input output 나옴\n",
        "\n"
      ],
      "metadata": {
        "id": "GA8IsjfKMPQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "qHxNOkGqL7cu",
        "outputId": "8a025962-2183-4f42-9434-e007d9881219"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 한 번 실행해 본다.\n",
        "train_set에서 10 sample을 하나의 배치로 만들어 model.predict()를 이용해 호출해본다."
      ],
      "metadata": {
        "id": "khXcaMaQOZUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normed_train_data[:10].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPvAgvzhOxrI",
        "outputId": "ed8356e4-8bdb-4293-b732-a08dea35b563"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습이 전혀 안 된 모델이므로 기대값과 다른 값이 나오는게 정상이다"
      ],
      "metadata": {
        "id": "yfz3JtN8PHqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch = normed_train_data[:10]\n",
        "example_result = model.predict(example_batch)\n",
        "example_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSXqY-plM4DM",
        "outputId": "7e3d026b-ef3f-40c2-ed5f-9ca3593afc48"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 375ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.17343663],\n",
              "       [ 0.06495132],\n",
              "       [-0.33456752],\n",
              "       [ 0.2860868 ],\n",
              "       [ 0.1322043 ],\n",
              "       [ 0.09557676],\n",
              "       [ 0.15003294],\n",
              "       [-0.00862686],\n",
              "       [-0.0726111 ],\n",
              "       [-0.14398897]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델훈련\n",
        "- 이 모델을 1000번의 에포크(epoch)동안 훈련함. 훈련 정확도와 검증 정확도는 history 객체에 기록 됨."
      ],
      "metadata": {
        "id": "6HpgzNACPW6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- fit(input_data, output_data, epochs,validation_split)\n",
        "- validation set : 과적합을 방지하기 위해 따로 분류해 놓은 값 (학습에 쓰이지 않는 값)"
      ],
      "metadata": {
        "id": "ijKJyhbyPqox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000\n",
        "\n",
        "history = model.fit(\n",
        "  normed_train_data, train_labels,\n",
        "  epochs=EPOCHS, validation_split = 0.2,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M_MlNclMrX1",
        "outputId": "d898603b-189f-44a4-b383-f9e17218960c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.9320 - val_loss: 8.7813\n",
            "Epoch 2/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.9524 - val_loss: 8.7825\n",
            "Epoch 3/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9461 - val_loss: 8.7778\n",
            "Epoch 4/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9455 - val_loss: 8.8161\n",
            "Epoch 5/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9330 - val_loss: 8.7957\n",
            "Epoch 6/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9308 - val_loss: 8.7781\n",
            "Epoch 7/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9812 - val_loss: 8.7652\n",
            "Epoch 8/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9060 - val_loss: 8.8108\n",
            "Epoch 9/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9375 - val_loss: 8.8267\n",
            "Epoch 10/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9772 - val_loss: 8.7901\n",
            "Epoch 11/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9355 - val_loss: 8.8137\n",
            "Epoch 12/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9311 - val_loss: 8.8589\n",
            "Epoch 13/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.9264 - val_loss: 8.8297\n",
            "Epoch 14/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9351 - val_loss: 8.8117\n",
            "Epoch 15/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9357 - val_loss: 8.8196\n",
            "Epoch 16/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9307 - val_loss: 8.7850\n",
            "Epoch 17/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9995 - val_loss: 8.7912\n",
            "Epoch 18/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9265 - val_loss: 8.7949\n",
            "Epoch 19/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9186 - val_loss: 8.7961\n",
            "Epoch 20/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9327 - val_loss: 8.7882\n",
            "Epoch 21/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9433 - val_loss: 8.8012\n",
            "Epoch 22/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9301 - val_loss: 8.8143\n",
            "Epoch 23/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8953 - val_loss: 8.8366\n",
            "Epoch 24/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9905 - val_loss: 8.8032\n",
            "Epoch 25/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9229 - val_loss: 8.8051\n",
            "Epoch 26/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9432 - val_loss: 8.8460\n",
            "Epoch 27/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9307 - val_loss: 8.8036\n",
            "Epoch 28/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9015 - val_loss: 8.8125\n",
            "Epoch 29/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9337 - val_loss: 8.8286\n",
            "Epoch 30/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9159 - val_loss: 8.7943\n",
            "Epoch 31/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9937 - val_loss: 8.8105\n",
            "Epoch 32/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9336 - val_loss: 8.8129\n",
            "Epoch 33/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9335 - val_loss: 8.8101\n",
            "Epoch 34/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9563 - val_loss: 8.7978\n",
            "Epoch 35/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9098 - val_loss: 8.7960\n",
            "Epoch 36/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.9127 - val_loss: 8.8148\n",
            "Epoch 37/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9143 - val_loss: 8.8232\n",
            "Epoch 38/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9172 - val_loss: 8.8200\n",
            "Epoch 39/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9139 - val_loss: 8.7841\n",
            "Epoch 40/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8964 - val_loss: 8.8133\n",
            "Epoch 41/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9553 - val_loss: 8.7826\n",
            "Epoch 42/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.9290 - val_loss: 8.7682\n",
            "Epoch 43/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8936 - val_loss: 8.7782\n",
            "Epoch 44/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9048 - val_loss: 8.7555\n",
            "Epoch 45/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8785 - val_loss: 8.7666\n",
            "Epoch 46/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9721 - val_loss: 8.7846\n",
            "Epoch 47/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9427 - val_loss: 8.7741\n",
            "Epoch 48/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9202 - val_loss: 8.7830\n",
            "Epoch 49/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9262 - val_loss: 8.7651\n",
            "Epoch 50/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9083 - val_loss: 8.7851\n",
            "Epoch 51/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9327 - val_loss: 8.7886\n",
            "Epoch 52/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8918 - val_loss: 8.8018\n",
            "Epoch 53/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9172 - val_loss: 8.7671\n",
            "Epoch 54/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9035 - val_loss: 8.7739\n",
            "Epoch 55/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9271 - val_loss: 8.7906\n",
            "Epoch 56/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9134 - val_loss: 8.7804\n",
            "Epoch 57/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9219 - val_loss: 8.7746\n",
            "Epoch 58/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9231 - val_loss: 8.7952\n",
            "Epoch 59/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8950 - val_loss: 8.7872\n",
            "Epoch 60/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9257 - val_loss: 8.8226\n",
            "Epoch 61/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9396 - val_loss: 8.8204\n",
            "Epoch 62/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9044 - val_loss: 8.8185\n",
            "Epoch 63/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9309 - val_loss: 8.8220\n",
            "Epoch 64/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8937 - val_loss: 8.8429\n",
            "Epoch 65/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8811 - val_loss: 8.8374\n",
            "Epoch 66/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9255 - val_loss: 8.8273\n",
            "Epoch 67/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8756 - val_loss: 8.8034\n",
            "Epoch 68/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9381 - val_loss: 8.8850\n",
            "Epoch 69/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9665 - val_loss: 8.8096\n",
            "Epoch 70/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8595 - val_loss: 8.8200\n",
            "Epoch 71/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.8977 - val_loss: 8.8211\n",
            "Epoch 72/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8638 - val_loss: 8.8188\n",
            "Epoch 73/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9330 - val_loss: 8.8220\n",
            "Epoch 74/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.9185 - val_loss: 8.8306\n",
            "Epoch 75/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8871 - val_loss: 8.8049\n",
            "Epoch 76/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8901 - val_loss: 8.8038\n",
            "Epoch 77/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9014 - val_loss: 8.7970\n",
            "Epoch 78/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9170 - val_loss: 8.8249\n",
            "Epoch 79/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8963 - val_loss: 8.8199\n",
            "Epoch 80/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9111 - val_loss: 8.8209\n",
            "Epoch 81/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8825 - val_loss: 8.7902\n",
            "Epoch 82/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8942 - val_loss: 8.8202\n",
            "Epoch 83/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8923 - val_loss: 8.7986\n",
            "Epoch 84/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8950 - val_loss: 8.8006\n",
            "Epoch 85/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8698 - val_loss: 8.8464\n",
            "Epoch 86/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9158 - val_loss: 8.8067\n",
            "Epoch 87/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8873 - val_loss: 8.8119\n",
            "Epoch 88/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8723 - val_loss: 8.8062\n",
            "Epoch 89/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8874 - val_loss: 8.8218\n",
            "Epoch 90/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8839 - val_loss: 8.8171\n",
            "Epoch 91/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9024 - val_loss: 8.8238\n",
            "Epoch 92/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9006 - val_loss: 8.8204\n",
            "Epoch 93/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8522 - val_loss: 8.8252\n",
            "Epoch 94/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9296 - val_loss: 8.8181\n",
            "Epoch 95/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8787 - val_loss: 8.8137\n",
            "Epoch 96/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8796 - val_loss: 8.8089\n",
            "Epoch 97/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8961 - val_loss: 8.8232\n",
            "Epoch 98/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8662 - val_loss: 8.8431\n",
            "Epoch 99/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9074 - val_loss: 8.8328\n",
            "Epoch 100/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8683 - val_loss: 8.8494\n",
            "Epoch 101/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8885 - val_loss: 8.8349\n",
            "Epoch 102/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8945 - val_loss: 8.8459\n",
            "Epoch 103/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8772 - val_loss: 8.8476\n",
            "Epoch 104/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8510 - val_loss: 8.8389\n",
            "Epoch 105/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8664 - val_loss: 8.8246\n",
            "Epoch 106/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9079 - val_loss: 8.8672\n",
            "Epoch 107/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8671 - val_loss: 8.8459\n",
            "Epoch 108/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.9073 - val_loss: 8.8369\n",
            "Epoch 109/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8507 - val_loss: 8.8062\n",
            "Epoch 110/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8697 - val_loss: 8.7926\n",
            "Epoch 111/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8874 - val_loss: 8.7950\n",
            "Epoch 112/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8721 - val_loss: 8.7922\n",
            "Epoch 113/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8568 - val_loss: 8.7977\n",
            "Epoch 114/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9149 - val_loss: 8.8055\n",
            "Epoch 115/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8634 - val_loss: 8.8209\n",
            "Epoch 116/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8922 - val_loss: 8.8143\n",
            "Epoch 117/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8395 - val_loss: 8.8263\n",
            "Epoch 118/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.8858 - val_loss: 8.8061\n",
            "Epoch 119/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8435 - val_loss: 8.8211\n",
            "Epoch 120/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8742 - val_loss: 8.8213\n",
            "Epoch 121/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8476 - val_loss: 8.8323\n",
            "Epoch 122/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.8518 - val_loss: 8.8776\n",
            "Epoch 123/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9067 - val_loss: 8.8715\n",
            "Epoch 124/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8461 - val_loss: 8.8336\n",
            "Epoch 125/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9231 - val_loss: 8.8348\n",
            "Epoch 126/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8687 - val_loss: 8.8643\n",
            "Epoch 127/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8554 - val_loss: 8.8220\n",
            "Epoch 128/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8615 - val_loss: 8.8152\n",
            "Epoch 129/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8664 - val_loss: 8.8145\n",
            "Epoch 130/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8462 - val_loss: 8.8505\n",
            "Epoch 131/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9060 - val_loss: 8.8771\n",
            "Epoch 132/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8449 - val_loss: 8.8300\n",
            "Epoch 133/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8695 - val_loss: 8.8223\n",
            "Epoch 134/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9080 - val_loss: 8.8418\n",
            "Epoch 135/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8534 - val_loss: 8.8228\n",
            "Epoch 136/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8653 - val_loss: 8.8168\n",
            "Epoch 137/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8658 - val_loss: 8.8172\n",
            "Epoch 138/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8650 - val_loss: 8.8136\n",
            "Epoch 139/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8692 - val_loss: 8.8308\n",
            "Epoch 140/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8570 - val_loss: 8.8338\n",
            "Epoch 141/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8428 - val_loss: 8.8520\n",
            "Epoch 142/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8304 - val_loss: 8.8340\n",
            "Epoch 143/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8816 - val_loss: 8.8544\n",
            "Epoch 144/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8309 - val_loss: 8.8588\n",
            "Epoch 145/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8176 - val_loss: 8.8607\n",
            "Epoch 146/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8917 - val_loss: 8.8530\n",
            "Epoch 147/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8129 - val_loss: 8.8317\n",
            "Epoch 148/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8423 - val_loss: 8.8493\n",
            "Epoch 149/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9259 - val_loss: 8.8612\n",
            "Epoch 150/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8461 - val_loss: 8.8341\n",
            "Epoch 151/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8272 - val_loss: 8.8586\n",
            "Epoch 152/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8883 - val_loss: 8.8374\n",
            "Epoch 153/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8448 - val_loss: 8.8395\n",
            "Epoch 154/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8783 - val_loss: 8.8345\n",
            "Epoch 155/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8624 - val_loss: 8.8448\n",
            "Epoch 156/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8649 - val_loss: 8.8569\n",
            "Epoch 157/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8625 - val_loss: 8.8606\n",
            "Epoch 158/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8096 - val_loss: 8.8888\n",
            "Epoch 159/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8247 - val_loss: 8.8457\n",
            "Epoch 160/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8591 - val_loss: 8.8757\n",
            "Epoch 161/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8495 - val_loss: 8.8692\n",
            "Epoch 162/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8517 - val_loss: 8.8781\n",
            "Epoch 163/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8735 - val_loss: 8.8560\n",
            "Epoch 164/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8243 - val_loss: 8.8575\n",
            "Epoch 165/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8710 - val_loss: 8.8332\n",
            "Epoch 166/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8216 - val_loss: 8.8300\n",
            "Epoch 167/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8194 - val_loss: 8.8669\n",
            "Epoch 168/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8663 - val_loss: 8.8565\n",
            "Epoch 169/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8542 - val_loss: 8.8355\n",
            "Epoch 170/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.9012 - val_loss: 8.8698\n",
            "Epoch 171/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8764 - val_loss: 8.8588\n",
            "Epoch 172/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8220 - val_loss: 8.8621\n",
            "Epoch 173/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8418 - val_loss: 8.8668\n",
            "Epoch 174/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8310 - val_loss: 8.8431\n",
            "Epoch 175/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8667 - val_loss: 8.8576\n",
            "Epoch 176/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8028 - val_loss: 8.8833\n",
            "Epoch 177/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8206 - val_loss: 8.9092\n",
            "Epoch 178/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8189 - val_loss: 8.8634\n",
            "Epoch 179/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8429 - val_loss: 8.8644\n",
            "Epoch 180/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8429 - val_loss: 8.8556\n",
            "Epoch 181/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8225 - val_loss: 8.8321\n",
            "Epoch 182/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8231 - val_loss: 8.8461\n",
            "Epoch 183/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8346 - val_loss: 8.8700\n",
            "Epoch 184/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8313 - val_loss: 8.8750\n",
            "Epoch 185/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8430 - val_loss: 8.8559\n",
            "Epoch 186/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8229 - val_loss: 8.8538\n",
            "Epoch 187/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8296 - val_loss: 8.8522\n",
            "Epoch 188/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8747 - val_loss: 8.8566\n",
            "Epoch 189/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8062 - val_loss: 8.8458\n",
            "Epoch 190/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8238 - val_loss: 8.8601\n",
            "Epoch 191/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.8660 - val_loss: 8.8518\n",
            "Epoch 192/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8568 - val_loss: 8.8728\n",
            "Epoch 193/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8349 - val_loss: 8.8589\n",
            "Epoch 194/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8085 - val_loss: 8.8523\n",
            "Epoch 195/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8435 - val_loss: 8.8572\n",
            "Epoch 196/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8055 - val_loss: 8.8760\n",
            "Epoch 197/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7920 - val_loss: 8.8587\n",
            "Epoch 198/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8339 - val_loss: 8.8886\n",
            "Epoch 199/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8164 - val_loss: 8.8818\n",
            "Epoch 200/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8550 - val_loss: 8.8722\n",
            "Epoch 201/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8461 - val_loss: 8.8703\n",
            "Epoch 202/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7987 - val_loss: 8.8991\n",
            "Epoch 203/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8096 - val_loss: 8.8895\n",
            "Epoch 204/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8291 - val_loss: 8.9025\n",
            "Epoch 205/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8128 - val_loss: 8.8819\n",
            "Epoch 206/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8487 - val_loss: 8.8524\n",
            "Epoch 207/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7990 - val_loss: 8.8673\n",
            "Epoch 208/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8567 - val_loss: 8.8604\n",
            "Epoch 209/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7960 - val_loss: 8.8650\n",
            "Epoch 210/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8667 - val_loss: 8.8618\n",
            "Epoch 211/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8133 - val_loss: 8.8370\n",
            "Epoch 212/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8136 - val_loss: 8.8329\n",
            "Epoch 213/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8452 - val_loss: 8.8317\n",
            "Epoch 214/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8035 - val_loss: 8.8480\n",
            "Epoch 215/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8090 - val_loss: 8.8461\n",
            "Epoch 216/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8377 - val_loss: 8.8674\n",
            "Epoch 217/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8030 - val_loss: 8.8894\n",
            "Epoch 218/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8175 - val_loss: 8.8673\n",
            "Epoch 219/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7941 - val_loss: 8.8891\n",
            "Epoch 220/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7936 - val_loss: 8.8979\n",
            "Epoch 221/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8007 - val_loss: 8.8783\n",
            "Epoch 222/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7866 - val_loss: 8.8772\n",
            "Epoch 223/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8389 - val_loss: 8.8767\n",
            "Epoch 224/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7977 - val_loss: 8.8566\n",
            "Epoch 225/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7947 - val_loss: 8.8544\n",
            "Epoch 226/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8466 - val_loss: 8.8577\n",
            "Epoch 227/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8511 - val_loss: 8.8652\n",
            "Epoch 228/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7753 - val_loss: 8.9070\n",
            "Epoch 229/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7720 - val_loss: 8.8786\n",
            "Epoch 230/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8071 - val_loss: 8.8671\n",
            "Epoch 231/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8384 - val_loss: 8.8748\n",
            "Epoch 232/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7796 - val_loss: 8.8749\n",
            "Epoch 233/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7844 - val_loss: 8.8672\n",
            "Epoch 234/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8206 - val_loss: 8.8357\n",
            "Epoch 235/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7956 - val_loss: 8.8384\n",
            "Epoch 236/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8085 - val_loss: 8.8485\n",
            "Epoch 237/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8000 - val_loss: 8.8583\n",
            "Epoch 238/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7709 - val_loss: 8.8443\n",
            "Epoch 239/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8656 - val_loss: 8.8403\n",
            "Epoch 240/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7787 - val_loss: 8.8499\n",
            "Epoch 241/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7733 - val_loss: 8.8655\n",
            "Epoch 242/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7997 - val_loss: 8.8375\n",
            "Epoch 243/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8222 - val_loss: 8.8307\n",
            "Epoch 244/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7923 - val_loss: 8.8476\n",
            "Epoch 245/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7971 - val_loss: 8.8499\n",
            "Epoch 246/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.8083 - val_loss: 8.8503\n",
            "Epoch 247/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7735 - val_loss: 8.8714\n",
            "Epoch 248/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7943 - val_loss: 8.8726\n",
            "Epoch 249/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7958 - val_loss: 8.8661\n",
            "Epoch 250/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8021 - val_loss: 8.8762\n",
            "Epoch 251/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7827 - val_loss: 8.8867\n",
            "Epoch 252/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7787 - val_loss: 8.9023\n",
            "Epoch 253/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8148 - val_loss: 8.8944\n",
            "Epoch 254/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.8000 - val_loss: 8.8682\n",
            "Epoch 255/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8134 - val_loss: 8.8627\n",
            "Epoch 256/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8192 - val_loss: 8.8854\n",
            "Epoch 257/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7555 - val_loss: 8.8885\n",
            "Epoch 258/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7515 - val_loss: 8.9184\n",
            "Epoch 259/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7666 - val_loss: 8.9261\n",
            "Epoch 260/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8144 - val_loss: 8.8865\n",
            "Epoch 261/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7674 - val_loss: 8.9037\n",
            "Epoch 262/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8033 - val_loss: 8.8888\n",
            "Epoch 263/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8181 - val_loss: 8.8871\n",
            "Epoch 264/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7821 - val_loss: 8.8899\n",
            "Epoch 265/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7663 - val_loss: 8.8528\n",
            "Epoch 266/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7986 - val_loss: 8.8581\n",
            "Epoch 267/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8019 - val_loss: 8.8639\n",
            "Epoch 268/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7967 - val_loss: 8.8614\n",
            "Epoch 269/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8237 - val_loss: 8.8779\n",
            "Epoch 270/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7571 - val_loss: 8.9056\n",
            "Epoch 271/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8040 - val_loss: 8.8705\n",
            "Epoch 272/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7662 - val_loss: 8.8763\n",
            "Epoch 273/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7927 - val_loss: 8.8771\n",
            "Epoch 274/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7933 - val_loss: 8.8818\n",
            "Epoch 275/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7844 - val_loss: 8.8621\n",
            "Epoch 276/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7857 - val_loss: 8.9021\n",
            "Epoch 277/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7837 - val_loss: 8.8755\n",
            "Epoch 278/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7548 - val_loss: 8.9060\n",
            "Epoch 279/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8052 - val_loss: 8.9137\n",
            "Epoch 280/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7806 - val_loss: 8.9042\n",
            "Epoch 281/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7775 - val_loss: 8.8981\n",
            "Epoch 282/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7763 - val_loss: 8.9155\n",
            "Epoch 283/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7617 - val_loss: 8.8832\n",
            "Epoch 284/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7742 - val_loss: 8.8848\n",
            "Epoch 285/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7802 - val_loss: 8.9227\n",
            "Epoch 286/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7917 - val_loss: 8.8819\n",
            "Epoch 287/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7828 - val_loss: 8.8984\n",
            "Epoch 288/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7756 - val_loss: 8.8734\n",
            "Epoch 289/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7689 - val_loss: 8.8550\n",
            "Epoch 290/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7595 - val_loss: 8.8792\n",
            "Epoch 291/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7435 - val_loss: 8.8887\n",
            "Epoch 292/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7909 - val_loss: 8.8926\n",
            "Epoch 293/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8223 - val_loss: 8.8679\n",
            "Epoch 294/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7697 - val_loss: 8.8832\n",
            "Epoch 295/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7628 - val_loss: 8.8603\n",
            "Epoch 296/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7753 - val_loss: 8.8594\n",
            "Epoch 297/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7611 - val_loss: 8.8394\n",
            "Epoch 298/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7661 - val_loss: 8.8464\n",
            "Epoch 299/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7403 - val_loss: 8.8488\n",
            "Epoch 300/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7885 - val_loss: 8.8590\n",
            "Epoch 301/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7795 - val_loss: 8.8869\n",
            "Epoch 302/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7592 - val_loss: 8.8710\n",
            "Epoch 303/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7762 - val_loss: 8.8585\n",
            "Epoch 304/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7743 - val_loss: 8.8620\n",
            "Epoch 305/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7459 - val_loss: 8.8675\n",
            "Epoch 306/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7442 - val_loss: 8.8766\n",
            "Epoch 307/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7380 - val_loss: 8.8901\n",
            "Epoch 308/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7715 - val_loss: 8.8795\n",
            "Epoch 309/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7635 - val_loss: 8.8869\n",
            "Epoch 310/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7561 - val_loss: 8.8726\n",
            "Epoch 311/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7703 - val_loss: 8.8948\n",
            "Epoch 312/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7689 - val_loss: 8.8951\n",
            "Epoch 313/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7329 - val_loss: 8.8982\n",
            "Epoch 314/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8129 - val_loss: 8.9076\n",
            "Epoch 315/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7763 - val_loss: 8.8835\n",
            "Epoch 316/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7920 - val_loss: 8.8872\n",
            "Epoch 317/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7847 - val_loss: 8.8976\n",
            "Epoch 318/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7392 - val_loss: 8.8822\n",
            "Epoch 319/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7709 - val_loss: 8.9072\n",
            "Epoch 320/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7532 - val_loss: 8.9020\n",
            "Epoch 321/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7621 - val_loss: 8.9175\n",
            "Epoch 322/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7444 - val_loss: 8.9029\n",
            "Epoch 323/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7889 - val_loss: 8.8888\n",
            "Epoch 324/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7692 - val_loss: 8.9008\n",
            "Epoch 325/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7686 - val_loss: 8.8901\n",
            "Epoch 326/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7318 - val_loss: 8.9162\n",
            "Epoch 327/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7377 - val_loss: 8.9057\n",
            "Epoch 328/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7383 - val_loss: 8.8871\n",
            "Epoch 329/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7715 - val_loss: 8.8768\n",
            "Epoch 330/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.8010 - val_loss: 8.8859\n",
            "Epoch 331/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7706 - val_loss: 8.8779\n",
            "Epoch 332/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7332 - val_loss: 8.9041\n",
            "Epoch 333/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7635 - val_loss: 8.8819\n",
            "Epoch 334/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7743 - val_loss: 8.8928\n",
            "Epoch 335/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7386 - val_loss: 8.9173\n",
            "Epoch 336/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7591 - val_loss: 8.9074\n",
            "Epoch 337/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7481 - val_loss: 8.9419\n",
            "Epoch 338/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7594 - val_loss: 8.9445\n",
            "Epoch 339/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7787 - val_loss: 8.9191\n",
            "Epoch 340/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7575 - val_loss: 8.8858\n",
            "Epoch 341/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7462 - val_loss: 8.8892\n",
            "Epoch 342/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7514 - val_loss: 8.9106\n",
            "Epoch 343/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7507 - val_loss: 8.9138\n",
            "Epoch 344/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7271 - val_loss: 8.9167\n",
            "Epoch 345/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7457 - val_loss: 8.9176\n",
            "Epoch 346/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7560 - val_loss: 8.9319\n",
            "Epoch 347/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7300 - val_loss: 8.8924\n",
            "Epoch 348/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7510 - val_loss: 8.9147\n",
            "Epoch 349/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8106 - val_loss: 8.9263\n",
            "Epoch 350/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7127 - val_loss: 8.8949\n",
            "Epoch 351/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7708 - val_loss: 8.9104\n",
            "Epoch 352/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7595 - val_loss: 8.9242\n",
            "Epoch 353/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7561 - val_loss: 8.8895\n",
            "Epoch 354/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.8112 - val_loss: 8.8922\n",
            "Epoch 355/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7611 - val_loss: 8.9182\n",
            "Epoch 356/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7509 - val_loss: 8.9207\n",
            "Epoch 357/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7035 - val_loss: 8.9191\n",
            "Epoch 358/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7474 - val_loss: 8.8956\n",
            "Epoch 359/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7045 - val_loss: 8.9271\n",
            "Epoch 360/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7497 - val_loss: 8.9520\n",
            "Epoch 361/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7366 - val_loss: 8.8952\n",
            "Epoch 362/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7599 - val_loss: 8.8888\n",
            "Epoch 363/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7562 - val_loss: 8.9001\n",
            "Epoch 364/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7362 - val_loss: 8.9143\n",
            "Epoch 365/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7422 - val_loss: 8.9094\n",
            "Epoch 366/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7285 - val_loss: 8.8763\n",
            "Epoch 367/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7142 - val_loss: 8.8785\n",
            "Epoch 368/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7714 - val_loss: 8.8690\n",
            "Epoch 369/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7625 - val_loss: 8.9083\n",
            "Epoch 370/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7109 - val_loss: 8.9078\n",
            "Epoch 371/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7735 - val_loss: 8.8978\n",
            "Epoch 372/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7040 - val_loss: 8.9623\n",
            "Epoch 373/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7289 - val_loss: 8.9383\n",
            "Epoch 374/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7372 - val_loss: 8.9223\n",
            "Epoch 375/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7263 - val_loss: 8.9016\n",
            "Epoch 376/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7229 - val_loss: 8.9018\n",
            "Epoch 377/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7251 - val_loss: 8.9224\n",
            "Epoch 378/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7409 - val_loss: 8.9296\n",
            "Epoch 379/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7506 - val_loss: 8.9157\n",
            "Epoch 380/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7401 - val_loss: 8.9195\n",
            "Epoch 381/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6992 - val_loss: 8.9097\n",
            "Epoch 382/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7438 - val_loss: 8.9315\n",
            "Epoch 383/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7144 - val_loss: 8.8878\n",
            "Epoch 384/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7319 - val_loss: 8.9057\n",
            "Epoch 385/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7009 - val_loss: 8.9006\n",
            "Epoch 386/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7276 - val_loss: 8.8776\n",
            "Epoch 387/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7289 - val_loss: 8.8757\n",
            "Epoch 388/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7486 - val_loss: 8.8783\n",
            "Epoch 389/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7502 - val_loss: 8.8922\n",
            "Epoch 390/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7238 - val_loss: 8.8795\n",
            "Epoch 391/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7496 - val_loss: 8.8895\n",
            "Epoch 392/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7696 - val_loss: 8.9004\n",
            "Epoch 393/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7349 - val_loss: 8.9052\n",
            "Epoch 394/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7462 - val_loss: 8.9083\n",
            "Epoch 395/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7123 - val_loss: 8.8926\n",
            "Epoch 396/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7213 - val_loss: 8.9218\n",
            "Epoch 397/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7364 - val_loss: 8.8962\n",
            "Epoch 398/1000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 5.6925 - val_loss: 8.8621\n",
            "Epoch 399/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6878 - val_loss: 8.8716\n",
            "Epoch 400/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7395 - val_loss: 8.8613\n",
            "Epoch 401/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6957 - val_loss: 8.8526\n",
            "Epoch 402/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7169 - val_loss: 8.8517\n",
            "Epoch 403/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7752 - val_loss: 8.8689\n",
            "Epoch 404/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7298 - val_loss: 8.8629\n",
            "Epoch 405/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7093 - val_loss: 8.8981\n",
            "Epoch 406/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7299 - val_loss: 8.9067\n",
            "Epoch 407/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.6979 - val_loss: 8.9180\n",
            "Epoch 408/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7495 - val_loss: 8.9168\n",
            "Epoch 409/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7098 - val_loss: 8.8959\n",
            "Epoch 410/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7373 - val_loss: 8.9098\n",
            "Epoch 411/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7320 - val_loss: 8.9088\n",
            "Epoch 412/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6885 - val_loss: 8.9271\n",
            "Epoch 413/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7372 - val_loss: 8.8995\n",
            "Epoch 414/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.7196 - val_loss: 8.9181\n",
            "Epoch 415/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7289 - val_loss: 8.9457\n",
            "Epoch 416/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7300 - val_loss: 8.9178\n",
            "Epoch 417/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.7259 - val_loss: 8.9094\n",
            "Epoch 418/1000\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 5.7140 - val_loss: 8.8918\n",
            "Epoch 419/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7240 - val_loss: 8.8944\n",
            "Epoch 420/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.7008 - val_loss: 8.9016\n",
            "Epoch 421/1000\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 5.7047 - val_loss: 8.8848\n",
            "Epoch 422/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7773 - val_loss: 8.8938\n",
            "Epoch 423/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6948 - val_loss: 8.8854\n",
            "Epoch 424/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6995 - val_loss: 8.8698\n",
            "Epoch 425/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6932 - val_loss: 8.8845\n",
            "Epoch 426/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.7312 - val_loss: 8.8922\n",
            "Epoch 427/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6996 - val_loss: 8.9308\n",
            "Epoch 428/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6879 - val_loss: 8.9062\n",
            "Epoch 429/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7472 - val_loss: 8.9413\n",
            "Epoch 430/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6968 - val_loss: 8.9196\n",
            "Epoch 431/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.6933 - val_loss: 8.9057\n",
            "Epoch 432/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7386 - val_loss: 8.9370\n",
            "Epoch 433/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6868 - val_loss: 8.9368\n",
            "Epoch 434/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7067 - val_loss: 8.9434\n",
            "Epoch 435/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7589 - val_loss: 8.9479\n",
            "Epoch 436/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.7327 - val_loss: 8.9408\n",
            "Epoch 437/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6950 - val_loss: 8.9378\n",
            "Epoch 438/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7060 - val_loss: 8.9611\n",
            "Epoch 439/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.6884 - val_loss: 8.9347\n",
            "Epoch 440/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.7047 - val_loss: 8.9485\n",
            "Epoch 441/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6895 - val_loss: 8.9753\n",
            "Epoch 442/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7142 - val_loss: 8.9770\n",
            "Epoch 443/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7692 - val_loss: 8.9369\n",
            "Epoch 444/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6833 - val_loss: 8.9473\n",
            "Epoch 445/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7174 - val_loss: 8.9537\n",
            "Epoch 446/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7433 - val_loss: 8.9413\n",
            "Epoch 447/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6708 - val_loss: 8.9184\n",
            "Epoch 448/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6977 - val_loss: 8.9154\n",
            "Epoch 449/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6821 - val_loss: 8.9347\n",
            "Epoch 450/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6833 - val_loss: 8.9443\n",
            "Epoch 451/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6894 - val_loss: 8.9445\n",
            "Epoch 452/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7345 - val_loss: 8.9245\n",
            "Epoch 453/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6718 - val_loss: 8.9184\n",
            "Epoch 454/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6834 - val_loss: 8.9300\n",
            "Epoch 455/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7015 - val_loss: 8.9191\n",
            "Epoch 456/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6871 - val_loss: 8.9150\n",
            "Epoch 457/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7752 - val_loss: 8.8942\n",
            "Epoch 458/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6781 - val_loss: 8.8875\n",
            "Epoch 459/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7029 - val_loss: 8.9167\n",
            "Epoch 460/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6784 - val_loss: 8.8938\n",
            "Epoch 461/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7033 - val_loss: 8.9092\n",
            "Epoch 462/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6959 - val_loss: 8.9184\n",
            "Epoch 463/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7042 - val_loss: 8.9100\n",
            "Epoch 464/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7222 - val_loss: 8.9353\n",
            "Epoch 465/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6810 - val_loss: 8.9316\n",
            "Epoch 466/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6968 - val_loss: 8.9313\n",
            "Epoch 467/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.7586 - val_loss: 8.8971\n",
            "Epoch 468/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6946 - val_loss: 8.9313\n",
            "Epoch 469/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6953 - val_loss: 8.9496\n",
            "Epoch 470/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6946 - val_loss: 8.9485\n",
            "Epoch 471/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6537 - val_loss: 8.9383\n",
            "Epoch 472/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6864 - val_loss: 8.9178\n",
            "Epoch 473/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7184 - val_loss: 8.9563\n",
            "Epoch 474/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6787 - val_loss: 8.9532\n",
            "Epoch 475/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7166 - val_loss: 8.9516\n",
            "Epoch 476/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6744 - val_loss: 8.9670\n",
            "Epoch 477/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6604 - val_loss: 8.9275\n",
            "Epoch 478/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.7007 - val_loss: 8.9209\n",
            "Epoch 479/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6664 - val_loss: 8.9395\n",
            "Epoch 480/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6732 - val_loss: 8.9519\n",
            "Epoch 481/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6936 - val_loss: 8.9482\n",
            "Epoch 482/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6861 - val_loss: 8.9087\n",
            "Epoch 483/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6806 - val_loss: 8.9458\n",
            "Epoch 484/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7170 - val_loss: 8.9349\n",
            "Epoch 485/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6792 - val_loss: 8.9266\n",
            "Epoch 486/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7013 - val_loss: 8.9334\n",
            "Epoch 487/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6674 - val_loss: 8.9582\n",
            "Epoch 488/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6785 - val_loss: 8.9435\n",
            "Epoch 489/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6636 - val_loss: 8.9559\n",
            "Epoch 490/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6728 - val_loss: 8.9210\n",
            "Epoch 491/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6538 - val_loss: 8.9331\n",
            "Epoch 492/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7046 - val_loss: 8.9232\n",
            "Epoch 493/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6496 - val_loss: 8.9220\n",
            "Epoch 494/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6556 - val_loss: 8.9079\n",
            "Epoch 495/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7071 - val_loss: 8.9378\n",
            "Epoch 496/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6969 - val_loss: 8.9445\n",
            "Epoch 497/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6555 - val_loss: 8.9668\n",
            "Epoch 498/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6966 - val_loss: 8.9709\n",
            "Epoch 499/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6596 - val_loss: 8.9837\n",
            "Epoch 500/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6554 - val_loss: 8.9998\n",
            "Epoch 501/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6742 - val_loss: 8.9675\n",
            "Epoch 502/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6669 - val_loss: 8.9827\n",
            "Epoch 503/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6647 - val_loss: 8.9760\n",
            "Epoch 504/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6984 - val_loss: 8.9503\n",
            "Epoch 505/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6847 - val_loss: 8.9632\n",
            "Epoch 506/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6735 - val_loss: 8.9572\n",
            "Epoch 507/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6809 - val_loss: 8.9307\n",
            "Epoch 508/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6397 - val_loss: 8.9651\n",
            "Epoch 509/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6969 - val_loss: 8.9381\n",
            "Epoch 510/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6427 - val_loss: 8.9601\n",
            "Epoch 511/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6850 - val_loss: 8.9741\n",
            "Epoch 512/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6550 - val_loss: 8.9535\n",
            "Epoch 513/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6768 - val_loss: 8.9557\n",
            "Epoch 514/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6541 - val_loss: 8.9620\n",
            "Epoch 515/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6790 - val_loss: 8.9474\n",
            "Epoch 516/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6726 - val_loss: 8.9734\n",
            "Epoch 517/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6312 - val_loss: 8.9770\n",
            "Epoch 518/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6450 - val_loss: 8.9512\n",
            "Epoch 519/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.7007 - val_loss: 8.9641\n",
            "Epoch 520/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6686 - val_loss: 8.9609\n",
            "Epoch 521/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6766 - val_loss: 8.9621\n",
            "Epoch 522/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6549 - val_loss: 8.9580\n",
            "Epoch 523/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6824 - val_loss: 8.9562\n",
            "Epoch 524/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6830 - val_loss: 8.9368\n",
            "Epoch 525/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6922 - val_loss: 8.9483\n",
            "Epoch 526/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6480 - val_loss: 8.9429\n",
            "Epoch 527/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6452 - val_loss: 8.9572\n",
            "Epoch 528/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6291 - val_loss: 8.9606\n",
            "Epoch 529/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6563 - val_loss: 8.9322\n",
            "Epoch 530/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6810 - val_loss: 8.9826\n",
            "Epoch 531/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.7112 - val_loss: 8.9477\n",
            "Epoch 532/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6260 - val_loss: 8.9630\n",
            "Epoch 533/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6780 - val_loss: 8.8986\n",
            "Epoch 534/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6623 - val_loss: 8.9058\n",
            "Epoch 535/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6703 - val_loss: 8.9517\n",
            "Epoch 536/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6480 - val_loss: 8.9381\n",
            "Epoch 537/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6449 - val_loss: 8.9198\n",
            "Epoch 538/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6805 - val_loss: 8.9328\n",
            "Epoch 539/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6699 - val_loss: 8.9455\n",
            "Epoch 540/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6629 - val_loss: 8.9393\n",
            "Epoch 541/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6588 - val_loss: 8.9213\n",
            "Epoch 542/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6329 - val_loss: 8.9173\n",
            "Epoch 543/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6638 - val_loss: 8.9188\n",
            "Epoch 544/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6453 - val_loss: 8.9232\n",
            "Epoch 545/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6640 - val_loss: 8.9366\n",
            "Epoch 546/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6461 - val_loss: 8.9304\n",
            "Epoch 547/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6664 - val_loss: 8.9093\n",
            "Epoch 548/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6346 - val_loss: 8.9084\n",
            "Epoch 549/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6380 - val_loss: 8.9744\n",
            "Epoch 550/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6650 - val_loss: 8.9462\n",
            "Epoch 551/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6219 - val_loss: 8.9745\n",
            "Epoch 552/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6701 - val_loss: 8.9341\n",
            "Epoch 553/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6340 - val_loss: 8.9395\n",
            "Epoch 554/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6908 - val_loss: 8.9232\n",
            "Epoch 555/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6015 - val_loss: 8.9417\n",
            "Epoch 556/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6471 - val_loss: 8.8913\n",
            "Epoch 557/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6594 - val_loss: 8.9266\n",
            "Epoch 558/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6768 - val_loss: 8.9210\n",
            "Epoch 559/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6295 - val_loss: 8.9067\n",
            "Epoch 560/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6389 - val_loss: 8.9524\n",
            "Epoch 561/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6373 - val_loss: 8.9683\n",
            "Epoch 562/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6998 - val_loss: 8.9964\n",
            "Epoch 563/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6619 - val_loss: 8.9707\n",
            "Epoch 564/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6390 - val_loss: 8.9837\n",
            "Epoch 565/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6357 - val_loss: 8.9584\n",
            "Epoch 566/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6184 - val_loss: 8.9540\n",
            "Epoch 567/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6514 - val_loss: 8.9522\n",
            "Epoch 568/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6813 - val_loss: 8.9610\n",
            "Epoch 569/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6429 - val_loss: 8.9377\n",
            "Epoch 570/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6268 - val_loss: 8.9542\n",
            "Epoch 571/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6692 - val_loss: 8.9458\n",
            "Epoch 572/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6523 - val_loss: 8.9587\n",
            "Epoch 573/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6466 - val_loss: 8.9760\n",
            "Epoch 574/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5990 - val_loss: 8.9426\n",
            "Epoch 575/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6332 - val_loss: 8.9535\n",
            "Epoch 576/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6904 - val_loss: 8.9470\n",
            "Epoch 577/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6225 - val_loss: 8.9642\n",
            "Epoch 578/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6205 - val_loss: 8.9052\n",
            "Epoch 579/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6659 - val_loss: 8.9292\n",
            "Epoch 580/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6390 - val_loss: 8.9511\n",
            "Epoch 581/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.6680 - val_loss: 8.9201\n",
            "Epoch 582/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6670 - val_loss: 8.9321\n",
            "Epoch 583/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6473 - val_loss: 8.9357\n",
            "Epoch 584/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6570 - val_loss: 8.9473\n",
            "Epoch 585/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6504 - val_loss: 8.9692\n",
            "Epoch 586/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6569 - val_loss: 8.9694\n",
            "Epoch 587/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6619 - val_loss: 8.9933\n",
            "Epoch 588/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6333 - val_loss: 8.9717\n",
            "Epoch 589/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6112 - val_loss: 8.9561\n",
            "Epoch 590/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6211 - val_loss: 8.9645\n",
            "Epoch 591/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6319 - val_loss: 8.9737\n",
            "Epoch 592/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6291 - val_loss: 8.9546\n",
            "Epoch 593/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6552 - val_loss: 8.9851\n",
            "Epoch 594/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6327 - val_loss: 8.9534\n",
            "Epoch 595/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6338 - val_loss: 8.9632\n",
            "Epoch 596/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6171 - val_loss: 9.0314\n",
            "Epoch 597/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6630 - val_loss: 9.0132\n",
            "Epoch 598/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6530 - val_loss: 9.0072\n",
            "Epoch 599/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6236 - val_loss: 8.9608\n",
            "Epoch 600/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6262 - val_loss: 8.9736\n",
            "Epoch 601/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.6149 - val_loss: 8.9892\n",
            "Epoch 602/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6057 - val_loss: 9.0515\n",
            "Epoch 603/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6549 - val_loss: 9.0112\n",
            "Epoch 604/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6371 - val_loss: 9.0117\n",
            "Epoch 605/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6217 - val_loss: 9.0005\n",
            "Epoch 606/1000\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5.5991 - val_loss: 9.0203\n",
            "Epoch 607/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6452 - val_loss: 9.0029\n",
            "Epoch 608/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5991 - val_loss: 9.0234\n",
            "Epoch 609/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6316 - val_loss: 8.9923\n",
            "Epoch 610/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6385 - val_loss: 8.9951\n",
            "Epoch 611/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6209 - val_loss: 8.9682\n",
            "Epoch 612/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6352 - val_loss: 8.9953\n",
            "Epoch 613/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6291 - val_loss: 8.9756\n",
            "Epoch 614/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6350 - val_loss: 8.9923\n",
            "Epoch 615/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6023 - val_loss: 8.9863\n",
            "Epoch 616/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6367 - val_loss: 9.0009\n",
            "Epoch 617/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6380 - val_loss: 8.9865\n",
            "Epoch 618/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6410 - val_loss: 9.0060\n",
            "Epoch 619/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.6519 - val_loss: 8.9680\n",
            "Epoch 620/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6203 - val_loss: 8.9497\n",
            "Epoch 621/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5904 - val_loss: 8.9819\n",
            "Epoch 622/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6131 - val_loss: 8.9730\n",
            "Epoch 623/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6269 - val_loss: 9.0240\n",
            "Epoch 624/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6276 - val_loss: 8.9672\n",
            "Epoch 625/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6563 - val_loss: 8.9766\n",
            "Epoch 626/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6086 - val_loss: 8.9856\n",
            "Epoch 627/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6382 - val_loss: 8.9526\n",
            "Epoch 628/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6009 - val_loss: 8.9855\n",
            "Epoch 629/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6689 - val_loss: 8.9628\n",
            "Epoch 630/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6186 - val_loss: 8.9415\n",
            "Epoch 631/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6462 - val_loss: 8.9005\n",
            "Epoch 632/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6078 - val_loss: 8.9132\n",
            "Epoch 633/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6594 - val_loss: 8.9424\n",
            "Epoch 634/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6065 - val_loss: 8.9609\n",
            "Epoch 635/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5854 - val_loss: 8.9526\n",
            "Epoch 636/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6854 - val_loss: 8.9610\n",
            "Epoch 637/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5873 - val_loss: 8.9532\n",
            "Epoch 638/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6206 - val_loss: 8.9617\n",
            "Epoch 639/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6234 - val_loss: 8.9590\n",
            "Epoch 640/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6429 - val_loss: 8.9881\n",
            "Epoch 641/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6007 - val_loss: 8.9724\n",
            "Epoch 642/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6603 - val_loss: 8.9732\n",
            "Epoch 643/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5819 - val_loss: 8.9769\n",
            "Epoch 644/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6227 - val_loss: 8.9938\n",
            "Epoch 645/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6062 - val_loss: 8.9953\n",
            "Epoch 646/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6471 - val_loss: 8.9860\n",
            "Epoch 647/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6120 - val_loss: 8.9696\n",
            "Epoch 648/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6347 - val_loss: 8.9737\n",
            "Epoch 649/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6239 - val_loss: 8.9795\n",
            "Epoch 650/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6076 - val_loss: 9.0338\n",
            "Epoch 651/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6134 - val_loss: 9.0087\n",
            "Epoch 652/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6162 - val_loss: 8.9814\n",
            "Epoch 653/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6102 - val_loss: 8.9997\n",
            "Epoch 654/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6139 - val_loss: 8.9933\n",
            "Epoch 655/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5882 - val_loss: 9.0312\n",
            "Epoch 656/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6095 - val_loss: 9.0330\n",
            "Epoch 657/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6310 - val_loss: 9.0319\n",
            "Epoch 658/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6226 - val_loss: 9.0168\n",
            "Epoch 659/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5827 - val_loss: 9.0142\n",
            "Epoch 660/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6016 - val_loss: 9.0404\n",
            "Epoch 661/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6539 - val_loss: 8.9866\n",
            "Epoch 662/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6054 - val_loss: 8.9864\n",
            "Epoch 663/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5913 - val_loss: 8.9912\n",
            "Epoch 664/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6275 - val_loss: 8.9611\n",
            "Epoch 665/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6611 - val_loss: 9.0040\n",
            "Epoch 666/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5911 - val_loss: 8.9837\n",
            "Epoch 667/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6197 - val_loss: 8.9739\n",
            "Epoch 668/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6142 - val_loss: 8.9688\n",
            "Epoch 669/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6331 - val_loss: 8.9495\n",
            "Epoch 670/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6025 - val_loss: 8.9383\n",
            "Epoch 671/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5870 - val_loss: 8.9290\n",
            "Epoch 672/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6224 - val_loss: 8.9472\n",
            "Epoch 673/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6015 - val_loss: 8.9762\n",
            "Epoch 674/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6137 - val_loss: 9.0035\n",
            "Epoch 675/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6123 - val_loss: 9.0099\n",
            "Epoch 676/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6624 - val_loss: 8.9741\n",
            "Epoch 677/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5792 - val_loss: 8.9504\n",
            "Epoch 678/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5880 - val_loss: 8.9704\n",
            "Epoch 679/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6022 - val_loss: 8.9905\n",
            "Epoch 680/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5819 - val_loss: 8.9909\n",
            "Epoch 681/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6162 - val_loss: 8.9813\n",
            "Epoch 682/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6019 - val_loss: 8.9920\n",
            "Epoch 683/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5866 - val_loss: 8.9842\n",
            "Epoch 684/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5882 - val_loss: 8.9982\n",
            "Epoch 685/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5895 - val_loss: 8.9584\n",
            "Epoch 686/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6632 - val_loss: 8.9438\n",
            "Epoch 687/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6110 - val_loss: 8.9870\n",
            "Epoch 688/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5803 - val_loss: 9.0220\n",
            "Epoch 689/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5935 - val_loss: 8.9918\n",
            "Epoch 690/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6115 - val_loss: 8.9716\n",
            "Epoch 691/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5737 - val_loss: 8.9791\n",
            "Epoch 692/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6389 - val_loss: 9.0143\n",
            "Epoch 693/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6012 - val_loss: 9.0174\n",
            "Epoch 694/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6169 - val_loss: 8.9988\n",
            "Epoch 695/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5740 - val_loss: 9.0119\n",
            "Epoch 696/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5867 - val_loss: 9.0145\n",
            "Epoch 697/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6533 - val_loss: 9.0070\n",
            "Epoch 698/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5887 - val_loss: 8.9873\n",
            "Epoch 699/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5638 - val_loss: 9.0447\n",
            "Epoch 700/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6399 - val_loss: 8.9952\n",
            "Epoch 701/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5771 - val_loss: 9.0079\n",
            "Epoch 702/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6148 - val_loss: 9.0030\n",
            "Epoch 703/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5860 - val_loss: 8.9922\n",
            "Epoch 704/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6237 - val_loss: 9.0062\n",
            "Epoch 705/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5842 - val_loss: 8.9762\n",
            "Epoch 706/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6032 - val_loss: 8.9972\n",
            "Epoch 707/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6319 - val_loss: 8.9905\n",
            "Epoch 708/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5845 - val_loss: 8.9957\n",
            "Epoch 709/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6261 - val_loss: 8.9737\n",
            "Epoch 710/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5997 - val_loss: 8.9957\n",
            "Epoch 711/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6483 - val_loss: 8.9904\n",
            "Epoch 712/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.6181 - val_loss: 8.9762\n",
            "Epoch 713/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5947 - val_loss: 8.9723\n",
            "Epoch 714/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6041 - val_loss: 8.9809\n",
            "Epoch 715/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6003 - val_loss: 8.9694\n",
            "Epoch 716/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5578 - val_loss: 8.9977\n",
            "Epoch 717/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6192 - val_loss: 8.9800\n",
            "Epoch 718/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5945 - val_loss: 8.9884\n",
            "Epoch 719/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5842 - val_loss: 9.0087\n",
            "Epoch 720/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5688 - val_loss: 9.0494\n",
            "Epoch 721/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5974 - val_loss: 9.0499\n",
            "Epoch 722/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5493 - val_loss: 9.0505\n",
            "Epoch 723/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6552 - val_loss: 9.0108\n",
            "Epoch 724/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5871 - val_loss: 9.0020\n",
            "Epoch 725/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5958 - val_loss: 8.9920\n",
            "Epoch 726/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5688 - val_loss: 9.0374\n",
            "Epoch 727/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5977 - val_loss: 9.0064\n",
            "Epoch 728/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5638 - val_loss: 9.0186\n",
            "Epoch 729/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5859 - val_loss: 9.0018\n",
            "Epoch 730/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5929 - val_loss: 9.0248\n",
            "Epoch 731/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5583 - val_loss: 9.0164\n",
            "Epoch 732/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6165 - val_loss: 8.9947\n",
            "Epoch 733/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6200 - val_loss: 8.9976\n",
            "Epoch 734/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6087 - val_loss: 8.9894\n",
            "Epoch 735/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5694 - val_loss: 8.9921\n",
            "Epoch 736/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5885 - val_loss: 9.0021\n",
            "Epoch 737/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5589 - val_loss: 9.0299\n",
            "Epoch 738/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5986 - val_loss: 9.0241\n",
            "Epoch 739/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.6487 - val_loss: 8.9819\n",
            "Epoch 740/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5783 - val_loss: 8.9889\n",
            "Epoch 741/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5829 - val_loss: 8.9710\n",
            "Epoch 742/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5928 - val_loss: 9.0153\n",
            "Epoch 743/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5810 - val_loss: 8.9975\n",
            "Epoch 744/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5865 - val_loss: 9.0489\n",
            "Epoch 745/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5972 - val_loss: 8.9996\n",
            "Epoch 746/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5872 - val_loss: 9.0297\n",
            "Epoch 747/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5856 - val_loss: 9.0028\n",
            "Epoch 748/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5663 - val_loss: 9.0200\n",
            "Epoch 749/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5737 - val_loss: 9.0256\n",
            "Epoch 750/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.5993 - val_loss: 8.9941\n",
            "Epoch 751/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5356 - val_loss: 9.0776\n",
            "Epoch 752/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6246 - val_loss: 9.0206\n",
            "Epoch 753/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5881 - val_loss: 9.0082\n",
            "Epoch 754/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5940 - val_loss: 9.0191\n",
            "Epoch 755/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6258 - val_loss: 8.9929\n",
            "Epoch 756/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6074 - val_loss: 9.0003\n",
            "Epoch 757/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5643 - val_loss: 8.9900\n",
            "Epoch 758/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5770 - val_loss: 9.0026\n",
            "Epoch 759/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5786 - val_loss: 9.0121\n",
            "Epoch 760/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5723 - val_loss: 8.9831\n",
            "Epoch 761/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5802 - val_loss: 9.0009\n",
            "Epoch 762/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6387 - val_loss: 8.9840\n",
            "Epoch 763/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5737 - val_loss: 8.9791\n",
            "Epoch 764/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6120 - val_loss: 8.9759\n",
            "Epoch 765/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5504 - val_loss: 9.0024\n",
            "Epoch 766/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5766 - val_loss: 8.9829\n",
            "Epoch 767/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5801 - val_loss: 8.9927\n",
            "Epoch 768/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6236 - val_loss: 9.0041\n",
            "Epoch 769/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5951 - val_loss: 9.0020\n",
            "Epoch 770/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5944 - val_loss: 9.0119\n",
            "Epoch 771/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5484 - val_loss: 9.0413\n",
            "Epoch 772/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6071 - val_loss: 9.0276\n",
            "Epoch 773/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5511 - val_loss: 9.0191\n",
            "Epoch 774/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5708 - val_loss: 9.0479\n",
            "Epoch 775/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5796 - val_loss: 9.0738\n",
            "Epoch 776/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5675 - val_loss: 9.0677\n",
            "Epoch 777/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5798 - val_loss: 9.0258\n",
            "Epoch 778/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5676 - val_loss: 9.0505\n",
            "Epoch 779/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6052 - val_loss: 9.0319\n",
            "Epoch 780/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5345 - val_loss: 9.0291\n",
            "Epoch 781/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5741 - val_loss: 9.0540\n",
            "Epoch 782/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5556 - val_loss: 9.0156\n",
            "Epoch 783/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6132 - val_loss: 9.0244\n",
            "Epoch 784/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6122 - val_loss: 9.0341\n",
            "Epoch 785/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5445 - val_loss: 9.0297\n",
            "Epoch 786/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5706 - val_loss: 9.0231\n",
            "Epoch 787/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5763 - val_loss: 9.0337\n",
            "Epoch 788/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5601 - val_loss: 9.0352\n",
            "Epoch 789/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6126 - val_loss: 9.0375\n",
            "Epoch 790/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5854 - val_loss: 8.9950\n",
            "Epoch 791/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5403 - val_loss: 9.0269\n",
            "Epoch 792/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6148 - val_loss: 9.0419\n",
            "Epoch 793/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5896 - val_loss: 9.0560\n",
            "Epoch 794/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5494 - val_loss: 9.0872\n",
            "Epoch 795/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5656 - val_loss: 9.1149\n",
            "Epoch 796/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5692 - val_loss: 9.0556\n",
            "Epoch 797/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5789 - val_loss: 9.0534\n",
            "Epoch 798/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6271 - val_loss: 9.0510\n",
            "Epoch 799/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5743 - val_loss: 9.0488\n",
            "Epoch 800/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6106 - val_loss: 9.0742\n",
            "Epoch 801/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5967 - val_loss: 9.0751\n",
            "Epoch 802/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.5697 - val_loss: 9.0564\n",
            "Epoch 803/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.5649 - val_loss: 9.0435\n",
            "Epoch 804/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5464 - val_loss: 9.0672\n",
            "Epoch 805/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6506 - val_loss: 9.0368\n",
            "Epoch 806/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5859 - val_loss: 9.0254\n",
            "Epoch 807/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5291 - val_loss: 9.0158\n",
            "Epoch 808/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5516 - val_loss: 9.0443\n",
            "Epoch 809/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5524 - val_loss: 9.0744\n",
            "Epoch 810/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5697 - val_loss: 9.1133\n",
            "Epoch 811/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5690 - val_loss: 9.0875\n",
            "Epoch 812/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5684 - val_loss: 9.0465\n",
            "Epoch 813/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5518 - val_loss: 9.0244\n",
            "Epoch 814/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5811 - val_loss: 9.0207\n",
            "Epoch 815/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5302 - val_loss: 9.0685\n",
            "Epoch 816/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5421 - val_loss: 9.1103\n",
            "Epoch 817/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5710 - val_loss: 9.0823\n",
            "Epoch 818/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5837 - val_loss: 9.0586\n",
            "Epoch 819/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.5907 - val_loss: 9.0578\n",
            "Epoch 820/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5883 - val_loss: 9.0591\n",
            "Epoch 821/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5323 - val_loss: 9.0554\n",
            "Epoch 822/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5513 - val_loss: 9.0282\n",
            "Epoch 823/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5640 - val_loss: 9.0866\n",
            "Epoch 824/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5798 - val_loss: 9.0803\n",
            "Epoch 825/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5587 - val_loss: 9.0498\n",
            "Epoch 826/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5780 - val_loss: 9.0378\n",
            "Epoch 827/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5256 - val_loss: 9.0650\n",
            "Epoch 828/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6149 - val_loss: 9.0751\n",
            "Epoch 829/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5606 - val_loss: 9.0639\n",
            "Epoch 830/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5359 - val_loss: 9.0430\n",
            "Epoch 831/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5293 - val_loss: 9.0544\n",
            "Epoch 832/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5912 - val_loss: 9.0541\n",
            "Epoch 833/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5609 - val_loss: 9.0064\n",
            "Epoch 834/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5655 - val_loss: 9.0211\n",
            "Epoch 835/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5193 - val_loss: 9.0145\n",
            "Epoch 836/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6219 - val_loss: 9.0203\n",
            "Epoch 837/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5590 - val_loss: 9.0207\n",
            "Epoch 838/1000\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 5.5904 - val_loss: 9.0421\n",
            "Epoch 839/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5665 - val_loss: 9.0442\n",
            "Epoch 840/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5688 - val_loss: 9.0601\n",
            "Epoch 841/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5460 - val_loss: 9.0602\n",
            "Epoch 842/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5562 - val_loss: 9.0573\n",
            "Epoch 843/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5468 - val_loss: 9.1230\n",
            "Epoch 844/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5982 - val_loss: 9.0354\n",
            "Epoch 845/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5609 - val_loss: 9.0447\n",
            "Epoch 846/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5335 - val_loss: 8.9919\n",
            "Epoch 847/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5574 - val_loss: 9.0094\n",
            "Epoch 848/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5734 - val_loss: 8.9917\n",
            "Epoch 849/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5384 - val_loss: 9.0190\n",
            "Epoch 850/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5159 - val_loss: 9.0237\n",
            "Epoch 851/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5232 - val_loss: 9.0702\n",
            "Epoch 852/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5486 - val_loss: 9.0747\n",
            "Epoch 853/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5326 - val_loss: 9.0418\n",
            "Epoch 854/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5716 - val_loss: 9.0285\n",
            "Epoch 855/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5669 - val_loss: 9.0345\n",
            "Epoch 856/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5585 - val_loss: 9.0293\n",
            "Epoch 857/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5311 - val_loss: 9.0727\n",
            "Epoch 858/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5896 - val_loss: 9.0089\n",
            "Epoch 859/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5227 - val_loss: 9.0552\n",
            "Epoch 860/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5282 - val_loss: 9.0487\n",
            "Epoch 861/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5332 - val_loss: 9.0473\n",
            "Epoch 862/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5585 - val_loss: 9.0846\n",
            "Epoch 863/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.6034 - val_loss: 9.0441\n",
            "Epoch 864/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5498 - val_loss: 9.0469\n",
            "Epoch 865/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5410 - val_loss: 9.0438\n",
            "Epoch 866/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5546 - val_loss: 9.0263\n",
            "Epoch 867/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5655 - val_loss: 9.0206\n",
            "Epoch 868/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5749 - val_loss: 9.0358\n",
            "Epoch 869/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5590 - val_loss: 9.0269\n",
            "Epoch 870/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5310 - val_loss: 9.0336\n",
            "Epoch 871/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5481 - val_loss: 9.0325\n",
            "Epoch 872/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5499 - val_loss: 9.0349\n",
            "Epoch 873/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.5574 - val_loss: 9.0373\n",
            "Epoch 874/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5320 - val_loss: 9.0365\n",
            "Epoch 875/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.6333 - val_loss: 9.0450\n",
            "Epoch 876/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5605 - val_loss: 9.0680\n",
            "Epoch 877/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5908 - val_loss: 9.0594\n",
            "Epoch 878/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5094 - val_loss: 9.0504\n",
            "Epoch 879/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5443 - val_loss: 9.0413\n",
            "Epoch 880/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5080 - val_loss: 9.0919\n",
            "Epoch 881/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5931 - val_loss: 9.0760\n",
            "Epoch 882/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5398 - val_loss: 9.0457\n",
            "Epoch 883/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5661 - val_loss: 9.0483\n",
            "Epoch 884/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5159 - val_loss: 9.0861\n",
            "Epoch 885/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5643 - val_loss: 9.0536\n",
            "Epoch 886/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5778 - val_loss: 9.0519\n",
            "Epoch 887/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5185 - val_loss: 9.0356\n",
            "Epoch 888/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5109 - val_loss: 9.1160\n",
            "Epoch 889/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5760 - val_loss: 9.0611\n",
            "Epoch 890/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5527 - val_loss: 9.0813\n",
            "Epoch 891/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5183 - val_loss: 9.1022\n",
            "Epoch 892/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5395 - val_loss: 9.1082\n",
            "Epoch 893/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5162 - val_loss: 9.1153\n",
            "Epoch 894/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.6270 - val_loss: 9.0768\n",
            "Epoch 895/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5722 - val_loss: 9.0925\n",
            "Epoch 896/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5310 - val_loss: 9.0677\n",
            "Epoch 897/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5969 - val_loss: 9.0151\n",
            "Epoch 898/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5375 - val_loss: 9.0422\n",
            "Epoch 899/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5276 - val_loss: 9.0755\n",
            "Epoch 900/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5320 - val_loss: 9.0709\n",
            "Epoch 901/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5327 - val_loss: 9.0893\n",
            "Epoch 902/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5450 - val_loss: 9.1126\n",
            "Epoch 903/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5438 - val_loss: 9.0487\n",
            "Epoch 904/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5450 - val_loss: 9.0335\n",
            "Epoch 905/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5265 - val_loss: 9.0715\n",
            "Epoch 906/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5377 - val_loss: 9.1066\n",
            "Epoch 907/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5220 - val_loss: 9.0912\n",
            "Epoch 908/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5077 - val_loss: 9.0518\n",
            "Epoch 909/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5532 - val_loss: 9.0502\n",
            "Epoch 910/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5049 - val_loss: 9.0782\n",
            "Epoch 911/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5325 - val_loss: 9.0736\n",
            "Epoch 912/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5195 - val_loss: 9.0609\n",
            "Epoch 913/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5270 - val_loss: 9.0712\n",
            "Epoch 914/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5189 - val_loss: 9.0771\n",
            "Epoch 915/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5811 - val_loss: 9.0822\n",
            "Epoch 916/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.6009 - val_loss: 9.0604\n",
            "Epoch 917/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5092 - val_loss: 9.1032\n",
            "Epoch 918/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5432 - val_loss: 9.0448\n",
            "Epoch 919/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5339 - val_loss: 9.0589\n",
            "Epoch 920/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5393 - val_loss: 9.0898\n",
            "Epoch 921/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5229 - val_loss: 9.0635\n",
            "Epoch 922/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5696 - val_loss: 9.0553\n",
            "Epoch 923/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5286 - val_loss: 9.0420\n",
            "Epoch 924/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5531 - val_loss: 9.0707\n",
            "Epoch 925/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5405 - val_loss: 9.0940\n",
            "Epoch 926/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5312 - val_loss: 9.0887\n",
            "Epoch 927/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5393 - val_loss: 9.0803\n",
            "Epoch 928/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5700 - val_loss: 9.0529\n",
            "Epoch 929/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.4986 - val_loss: 9.0423\n",
            "Epoch 930/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5594 - val_loss: 9.0484\n",
            "Epoch 931/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.4800 - val_loss: 9.0737\n",
            "Epoch 932/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5547 - val_loss: 9.0404\n",
            "Epoch 933/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5310 - val_loss: 9.0618\n",
            "Epoch 934/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5215 - val_loss: 9.0736\n",
            "Epoch 935/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5485 - val_loss: 9.0542\n",
            "Epoch 936/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5358 - val_loss: 9.0392\n",
            "Epoch 937/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.5388 - val_loss: 9.0759\n",
            "Epoch 938/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5345 - val_loss: 9.0936\n",
            "Epoch 939/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.4870 - val_loss: 9.0833\n",
            "Epoch 940/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5152 - val_loss: 9.0723\n",
            "Epoch 941/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5804 - val_loss: 9.0784\n",
            "Epoch 942/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5247 - val_loss: 9.1000\n",
            "Epoch 943/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5280 - val_loss: 9.0959\n",
            "Epoch 944/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5314 - val_loss: 9.1164\n",
            "Epoch 945/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5517 - val_loss: 9.1532\n",
            "Epoch 946/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5128 - val_loss: 9.1370\n",
            "Epoch 947/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5098 - val_loss: 9.0804\n",
            "Epoch 948/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5575 - val_loss: 9.0477\n",
            "Epoch 949/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5146 - val_loss: 9.0614\n",
            "Epoch 950/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5049 - val_loss: 9.0914\n",
            "Epoch 951/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5418 - val_loss: 9.0687\n",
            "Epoch 952/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5072 - val_loss: 9.0647\n",
            "Epoch 953/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5092 - val_loss: 9.0549\n",
            "Epoch 954/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5185 - val_loss: 9.0623\n",
            "Epoch 955/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5679 - val_loss: 9.0852\n",
            "Epoch 956/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5053 - val_loss: 9.0851\n",
            "Epoch 957/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5193 - val_loss: 9.0762\n",
            "Epoch 958/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5762 - val_loss: 9.0637\n",
            "Epoch 959/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.4987 - val_loss: 9.0934\n",
            "Epoch 960/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5439 - val_loss: 9.0602\n",
            "Epoch 961/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5433 - val_loss: 9.0503\n",
            "Epoch 962/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5385 - val_loss: 9.0734\n",
            "Epoch 963/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.4917 - val_loss: 9.0554\n",
            "Epoch 964/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5086 - val_loss: 9.0577\n",
            "Epoch 965/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5378 - val_loss: 9.0395\n",
            "Epoch 966/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5341 - val_loss: 9.0426\n",
            "Epoch 967/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5591 - val_loss: 9.0558\n",
            "Epoch 968/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5826 - val_loss: 9.0489\n",
            "Epoch 969/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5188 - val_loss: 9.0627\n",
            "Epoch 970/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5102 - val_loss: 9.0822\n",
            "Epoch 971/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5627 - val_loss: 9.0924\n",
            "Epoch 972/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5193 - val_loss: 9.1128\n",
            "Epoch 973/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5379 - val_loss: 9.0945\n",
            "Epoch 974/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5030 - val_loss: 9.0705\n",
            "Epoch 975/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5166 - val_loss: 9.0953\n",
            "Epoch 976/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5727 - val_loss: 9.0960\n",
            "Epoch 977/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.4912 - val_loss: 9.1151\n",
            "Epoch 978/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5721 - val_loss: 9.0701\n",
            "Epoch 979/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5329 - val_loss: 9.0786\n",
            "Epoch 980/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5459 - val_loss: 9.0657\n",
            "Epoch 981/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5042 - val_loss: 9.0542\n",
            "Epoch 982/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.4922 - val_loss: 9.0305\n",
            "Epoch 983/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5221 - val_loss: 9.0613\n",
            "Epoch 984/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5332 - val_loss: 9.0610\n",
            "Epoch 985/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5541 - val_loss: 9.0793\n",
            "Epoch 986/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5488 - val_loss: 9.0458\n",
            "Epoch 987/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5146 - val_loss: 9.0556\n",
            "Epoch 988/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5187 - val_loss: 9.0887\n",
            "Epoch 989/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5260 - val_loss: 9.1022\n",
            "Epoch 990/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5550 - val_loss: 9.0916\n",
            "Epoch 991/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5740 - val_loss: 9.1297\n",
            "Epoch 992/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.4945 - val_loss: 9.1199\n",
            "Epoch 993/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5137 - val_loss: 9.1062\n",
            "Epoch 994/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5092 - val_loss: 9.1122\n",
            "Epoch 995/1000\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5.5716 - val_loss: 9.1219\n",
            "Epoch 996/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.4982 - val_loss: 9.1016\n",
            "Epoch 997/1000\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 5.5298 - val_loss: 9.0818\n",
            "Epoch 998/1000\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 5.5293 - val_loss: 9.0992\n",
            "Epoch 999/1000\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5.5242 - val_loss: 9.1140\n",
            "Epoch 1000/1000\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5.5025 - val_loss: 9.1320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# EarlyStopping() : val loss가 어느 값 이후로 줄어들지 않으면 학습을 종료시키는 함수\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n",
        "                    validation_split = 0.2, verbose=0, callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1zCztSPPlnk",
        "outputId": "18e2fbfe-99d3-4a5b-c930-6ac07a8a35a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9a391bc460>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=model.evaluate(normed_test_data, test_labels, verbose=2) #평가 함수 evaluate()\n",
        "print('테스트 세트의 평균 절대 오차: {:5.2f} MPG'.format(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8W4fsqBRLT1",
        "outputId": "807a05a9-fb7e-48e7-8490-200172ac8812"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 - 0s - loss: 6.6021 - 24ms/epoch - 8ms/step\n",
            "테스트 세트의 평균 절대 오차:  6.60 MPG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측\n",
        "- 셈플을 사용해 MPG 값 예측"
      ],
      "metadata": {
        "id": "FU_ddJUuRwTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model.predict(normed_test_data).flatten()\n",
        "\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim([0,plt.xlim()[1]])\n",
        "plt.ylim([0,plt.ylim()[1]])\n",
        "_ = plt.plot([-100, 100], [-100, 100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "s3frasgkQ-sn",
        "outputId": "9191e47c-510e-41c3-a4ea-f48df37f6dbe"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQMAAAEGCAYAAABhHPB4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de5RV9ZXnP7uKAqqCpHj5Ki1BcHQ0KghRlIytZCWaaLQijo8xaXUx4vREJzE2Hey2o+mkl2RMR7uzelYaHyNpE0UFkWinmQRI7MREw0sRn1FBLUXwUZZoAfXY88f53fLUrfO6Vffce+69+7PWXfee33ncfWvV+Z7fb+/92z9RVQzDMOrKbYBhGNnAxMAwDMDEwDAMh4mBYRiAiYFhGI4R5TYgCRMnTtTJkyeX2wzDqHg2bNjwtqpOCtpXEWIwefJk1q9fX24zDKPiEZHtYftsmGAYBmBiYBiGw8TAMAzAxMAwaoadnXsi95sYGEYNsLNzDxfd9ofIY0wMDKPKyQnBjvetZ2AYNYtfCO66/MTIY00MDKNKyReCE6eMjzzexMAwqpBChQBMDAyj6hiKEICJgWFUFUMVAjAxMIyqYThCACYGhlEVDFcIwMTAMCqeYggBmBgYRkVTLCEAEwPDqFiKKQRgYmAYFUmxhQBMDAyj4khDCMDEwDAqirSEAEwMDKNiSFMIwMTAMCqCtIUATAwMI/OUQgjAxMAwMk2phABMDAwjs5RSCMDEwDAySamFAEwMDCNzlEMIwMTAMDJFuYQATAwMIzOUUwjAxMAwMkG5hQBKsAqziNQD64F2VT1bRKYA9wITgA3AV1V1X9p2GEY5WbmpnZtXP88bHV0c3NzIwjOOpG1GC5ANIYDS9Ay+Djzr2/4+cIuqTgPeA+aXwAbDKBsrN7Vz3YottHd0oUB7RxfXrdjCyk3tmRECSFkMROQQ4CzgdrctwFzgAXfIUqAtTRsMo9zcvPp5urp7B7R1dfey+BfPZUYIIP1hwq3AXwH7ue0JQIeq9rjt14GWoBNFZAGwAKC1tTVlMw0jPd7o6Aps39G5h6aR9UURgqhhSFJS6xmIyNnATlXdMJTzVXWJqs5S1VmTJk0qsnWGUToObm4MbBcomhCEDUMKIc1hwhzgHBHZhucwnAv8I9AsIrkeySFAYRYbRoWx8IwjaWyoH9R+1dxpRRkahA1Dbl79fEHXSU0MVPU6VT1EVScDFwFrVfUSYB1wvjvsUuChtGwwjCzQNqOFm847lgPHjga8HsHVc6dx7eePLMr1w4YhYe1hlCPP4FvAN0XkT3g+hDvKYINhlJRTpk6gaVQ9TSPrWXblyUUTAggfhoS1h5F6ngGAqv4a+LX7/DIQvTa0YRSZYjjYhkra4cOFZxzJdSu2DBgqNDbUs/CMwgSnJGJgGOUk52DL3Sw5BxuQuiCUIo8g9xuGK3YmBkbVE+VgS1MMSplQ1DajZdi/xeYmGFVPsRxshZClzMKkmBgYVU+xHGxJqUQhABMDowYIivMPxcGWhEoVAjCfgVEDFMvBBtmYfZhWZMTEwKgJkjjY4m6yqKjEKVMnlEwI0oqMmBgYBslusqjZh02j6ksyNEgzMmJiYNQcQT2AsJvs2vueBDxBKMXswzjSjIyYGBg1RVgPIF8IcvSq9vcQDm5upD3gpivW7MMkhNlQjMiIRROMmiKsB1AvEnpOrhue9uzDJKQZGbGegVFThHWne1VpqBO6+zT0vNyYfPEvnmNH5x4ETwiKOekojmJGRvIxMTBqirBudnNjAx/u6wk44+PzYODsw3LlERQj9TgIGyYYNUVYN1sEunuDewW5bnglJxQlwXoGRtWTHz2YN7OFdc/tGtDNvmbZ5tDzbzrv2JLlEZQT6xkYVU1QfcDlG9o5/ahJHNzcyBsdXdy8+nmaRg52DAKMa2qoCSEA6xkYVU5Y9OCnf3iV3KAgyIeQo7dPBwjBGx1dzFm8tixFUtLGxMCoasKiB8HegcF07umhp0/7haBcRVJKgQ0TjKpmuMk4/oSiYlUhziomBkZFsXJTO3MWr2XKokeYs3ht7NoAQdGDsPSioHZ/QlE5iqSUEhMDo2IYymIhuTLlLc2NCNDS3Mgls1sDw4uXzG6NLGde6iIppcZ8BkbFMNQZe0FJOrMOGz8oi++UqRN47KV3QhOKilWFOKuYGBgVQzG76fkCkSShKM1U4CxgYmBUDGnN2CskszCtVOAsYD4Do2JIY8ZetacYF4L1DIyKodjddBOCgZgYGBXFcLrp/jkKB4wdTZ8qu/f29AtBOZdgywKRYiAi/5TgGp2qen2R7DGMfop5c+ZXONrRuQfwwoc5Iajm7MIkxPkMzgU2xLzmpWmgUZus3NTOwgeeHJBTsPCBJwflFCRNQgoKSwKs2Ngeur+asguTEDdMuEVVl0YdICLjimiPYQDwnZ9vHVRfoLtX+c7Pt9I2o4WVm9q5cdVWOrq6+/dHPc3jwpLVnl2YhMiegareGneBJMcYRqG891F3aHuuS+8XghxhT/MDXGZhPrmwZLVnFyYhUgxE5BgROce3fYuI3OleJ6RvnmEMJqzLnyP/ab6zcw99Onieoj8sWcol2LJKnM9gMfC2b/sM4BFgHfDttIwyjObGhtD2uK67/2meCx/u3tvD1XOnDZijcNN5x/YPJ4LmMPj31wJxPoODVPUx33anqi4HEJEr0zPLqHVuPOcYFt7/5IBqxQ11wtnHH8Q9j79Gb8CTHgY+zYPyCKIqGVdzdmES4sRgP/+Gqs72be5ffHMMwyMowej0oyaxfEN7qBCMa2rghi8dQ9uMFksoGgJxYvCGiJykqo/7G0VkNvBGemYZxuAn9ZzFawN9BfUi/MMFx/cfu/SxbXz34Wfo6VMmjhlZUxGB4RAnBt8ClonIXcBG1zYTuBS4MEW7DGMQYTd1n+oAIbhx1db+smZv797HdSu2sH77u4MqItfykCCISDFQ1SdE5CTgKuAy17wVmK2qb0WdKyKjgUeBUe57HlDVG0RkCnAvMAEvaemrqrpvWL/CqAniZi3u7NzDdx9+ZlB9w6ACqEmyC2stPTnJrMWDgaeA61V1nqp+O04IHHuBuap6PDAdONMNL76Pl8w0DXgPmD9E240aIyr8l/MR9IQsjxYkEFHZhUOpqlTpxOUZfBu4Dy/l+BERuSLphdVjt9tscC8F5gIPuPalQFuhRhu1SVj4z7+uwcQxIxNfL8qXUIvpyXE+gwuB6ar6kYhMAP4duC3pxUWkHm8oMA34Z+AloENVc4vavQ4E9rtEZAGwAKC1tTXpVxpVTlyFovxy5uDVMwzqL0RlF9ZienLcMGGvqn4EoKrvJDh+AKraq6rTgUOAE4GjCjh3iarOUtVZkyZNKuRrjRohKHxYSAHUqOzCWkxPjusZHC4iq9xnAab6tlHVc4JPG4iqdojIOuBkoFlERrjewSFA9Q7CjNSIyiNIWgA1yhlY7cVPg4gTg3Pztn+Q9MIiMgnodkLQCHwOz3m4DjgfL6JwKfBQcnMNI75CUVgUoJBIQLUXPw1CNCSba9gXFjkOz0FYjze8uE9V/05EDscTgvHAJuArqro36lqzZs3S9evXp2KnkW3yb+wFpx7O0t9vixSCoCd6rc0zCENENqjqrMB9UWIgIk9FXVhVjxumbYkwMahuwp7k16/cMiA/ALyxasOIOu6ef1JgivGcxWsDcxFamhv53aK56f2ICiFKDOKGCX14jtifAT8HqteVagyboSTphJUbW7/93UFCAN4/49jRI0LnGtRiFKBYxBU3mQ5cDIzBE4S/B44B2lV1e/rmGZXCUJN0wuL59zz+WuhKye/sDk9YrcUoQLGIDRWq6nOqeoOqnoDXO/gJcE3qlhkVxVCTdMKe2GEzEyH6xrYiJUMntlS6iLQAFwFfxksfvgZ4MGW7jApjqN3zsPkG9SKBgiAQeWPXYhSgWMSVSv8NXk2D+4DLgXfcrpEiMl5V303ZPqNCGOrSZ2Hx/C8ceyAPbmwf5Dy8ZHZr7I1d60VKhkpcz+AwPJ/NlbjUYEcuw/PwlOwyKoxCk3T8zsZPNjYwuqGOjo+6B4QPG0bUMXb0CN7Zva//CQ9exMCe+sUnbgrz5BLZYVQ4hXTP8yMIHV3dNDbUc8uF0wdMOsoPH9pCJ+kSl2dwoKruiLxAgmOGi+UZVBdhuQAHjh1N06j60IQiyyEYPlF5BnHRhH9LcP0kxxhGP2FOxR2deyJrFloOQbrE+QyOF5HOiP0CRO03jEGEORsFIouXDtVJaSQjzmdQH7XfMIZCkLMR4Cq3CGqO/IzGXHXkWppJWEoKqk9gGMUgV3PgQLfkmeCthuxf0yAoo3H5hnbmzWyp6YVO0iQ26cgw0uCUqRNoGlVP08j6wKFBWEbjuud2mbMwJaxnYJScJPUIgnwDYM7CNEnUMxCRqcDrqrpXRE4DjgN+oqodaRpnVAfXr9zSvyRaHTBm9Ah6+jSyHkEY5ixMj6TDhOXALBGZBizBq070M+CLaRlmVC5+x1/TyHo+3Pdxd78P6NzTw+eP3j8wahC1wrI5C9MlqRj0qWqPiHwZ+JGq/khENqVpmJEuaS0Qkp8l6BcCP2ue3RXYHjUMmDfT5hykSVKfQbeIXIxXs/Bh1xa8ZraRedJcICTqye4nbIpy1DBg+Yb2ql7EpNwkFYPL8Sob/72qvuKWSPvX9Mwy0iTNBUKSOvjqRQLbg+oR5Kj2RUzKTaJhgqo+A/wv3/YreJWOjQokzbTesCzBfC4+6dDA9tww4BvLNgfut2hCeiTqGYjIHBH5pYi8ICIvi8grIvJy2sYZ6ZBmabCgJ3udeIlF4PUIvjK7le+1HRt6jbYZXmJRWjYawSR1IN6BV+FoAxA/IDQyTZoLhLTNaOH9rm6++/Az9PQpE8eM5Pqzji7Y8VeLi5iUm6Ri8L6q/iJVS4ySkWZpsJ2de1j6+22MHFHHzyImHfmJimxY+bLSkVQM1onIzcAKvKXWAVDVjalYZQyLtMKGccRlFobZGlWwxG7+0pFUDE5y7/6iCLnl1Y0MkaQaUBoVgwoVgpxgBTkbc1EDE4LSkjSacHrahhjFISps6O96xx0Th7/3ccDY0fSpsntvT2IhCJrC7MeiBqUn6dyETwI3AKe6pt8Af6eq76dlmDE0wm6i9o4upix6JDL05z83aqiRfzPv6NwDeNOQkwwNkiQmWdSg9CRNOroT+AC4wL06gf+bllHG0Im6iXLZhsHpPh+fG5ehGHYzr9jY3n/+nMVrmbLoEeYsXjsoazDuqW9Rg/KQVAymulWVXnav72Bl0jNJVAZfDoVBguC/AeMyFKOSlpKkOkcJlhUsKR9JxaBLRD6T2xCROdgirJkkV0UoVw0oDIXQikFxGYoHuApF+Rzc3Jgo1TlsCbRbL5zO7xbNNSEoE0mjCX8BLHW+AwHeBS5LyyhjePhDckMpLx5VeHRn5x76AiYZ5XoW1yRII7YcgmySNJqwGa9S8li3bRWRK4SwTL7Tj5oUujJR2DkLTj2ci277A7v39nD13Gms2Ng+6PywcGH+0MByCLJH3FqLX1HVu0Xkm3ntAKjqD1O0zSgCQU/h/CrDYYk+/nNyS5758wj8BUxzWBpx5RLXM/iEe98vYF/4UkxGpsh/Cs9ZvDY2z8B/jj+haP5npnDNss2h3fu2GS2s3/5uf5mzehErSlIhxK2b8C/u469U9Xf+fc6JaFQghUxhzheC2//jldjsxuUb2vuLl/SqsnxDO7MOG2+CkHGSRhN+lLDNqACSTmHOTzFesbE9NlKQZuEUI13ifAYnA6cAk/L8BmMBW22pQkkyrg+aa5CkR2HrIVYucT2DkcAYPNHYz/fqBM6POlFEDhWRdSLyjIhsFZGvu/bxrlDKi+593PB/hlEI+bkI+XkGSx/bximL1/Lyrg9pGlnffyMn6VGkWTjFSJfIJdn7DxI5TFW3F3RhkYOAg1R1o4jsh1cYpQ0vP+FdVV0sIouAcar6rahr2ZLsw2PlpnZuXLWVjq5uAMY1NXDDl44JHMMvfWwbN67aOsA73NhQz03neZWJgnoUfiEJmoSUf4xRPoazJHuO20Wk2XfBcSKyOuoEVX0zV+9AVT8AngVagHOBpe6wpXgCYaTEyk3tLLz/yX4hAHjvo24WPvDkoDkDOzv38N2HnxkUJvJHGqJ6FBDf6zCyS9KewSZVnRHXFnH+ZOBR4FPAq6ra7NoFeC+3nXfOAmABQGtr68zt2wvqmBiOsAxEP/UinDvjYDa/1sHLuz4MPEaAVxaf1b9drgIqxvAoRs+gT0RafRc8jIR5BiIyBm9Fpm/kZy6qp0SB11HVJao6S1VnTZo0KaGZRj5JHHe9qqzY2M6r73zExDEjA4/xj/nTXHfBKB9JxeBvgN+KyL+KyN14T/nr4k4SkQY8Ifipqq5wzW85f0LOr7CzcLONpBTiuOvrU64/6+jASUT+SIOFD6uTpHMT/l1ETgBmu6ZvqOrbUee4IcAdwLN5acur8FZmWuzeHyrYaiOQoK776UdN4u4/vJro/D6STSKy8GF1EpdncJSqPueEAOAN994qIq0xBVHnAF8FtohIbirbX+OJwH0iMh/YjlcsxRgmYXUNR41I2vn7eJWjuElEUbMajcolrmdwLXAF8A8B+yILoqrqbxlcQyPHZxNZZ4SS3wv4cG9PYNc9ybqHOcJWOcrHJiNVJ3FzE65w71YQNUME9QKGgwCXxKxy5MfqEVQnccOE86L2+5yCRglJutIxwCdG1ocui57j4OZGvtd2bEHhQqtHUH3EDRO+5N73x5ujsNZtnw48hreoilFikjrqGhvqaaivI25FPH/twmKupWBUFpHeJVW9XFUvBxqAo1V1nqrOA45xbUYZiHLU5Zw0ucy/932Zh1HXs3ChkdTVfKiqvunbfgtoDTvYSJeoCsi5Qqe5wqJhxUtz5Bx/YX6H4fojjMohqRisEZHVInKZiFwGPAL8Kj2zjChy+f9h5IYRYcVL83sPbTNa+sOK+YS1G9VH0qSjq0Tky3y8otISVX0wPbOMOOKKj+bqEUQVL/XTGzJHJazdqD6SlkoH2Ah8oKq/EpEmEdnPzUY0ykRcFeO44qV+WkISiVoskahmSLrW4hV4MwjHA1PxpiL/GEseKjn54b95M1tY99yuwCrGccVL/VgikZG0Z/A14ETgcQBVfVFE9k/NKiOQoPDf8g3t/eP+QouX+rFEIiOpGOxV1X259RJEZARWKr3kRIX/Tpk6YcDQ4Jplmwtedt0SiWqbpGLwGxH5a6BRRD4H/E/g5+mZVXskyf6LWm59KMVLDcNP0tDit4BdwBbgSuDfgOvTMqrWSFosJCzZaESdDBCCqGNtZqERRqwYiEg9Xk2C21T1v6rq+e6zDROKRNLsv6BkIwHq6mSAEIQdaw5BI4rYYYKq9orI865+QbIqGUZBJO3S+5187R1djKgT6uqEu+efNEAI8o81h6CRhKQ+g3HAVhF5AuivmKmq56RiVY1RSLGQthktg5yF+ULgP9ZufiMpScXgb1O1osZZeMaRLHzgSbp7Px55NdRLYJc+aKUjwygGcfUMRgP/A5iG5zy8Q1V7SmFYzZHvgQnwyOzs3MPZP/otuz7YiwLXLNtsXX+jaMQ5EJcCs/CE4AsElz8zhsnNq5+nu2/g3d/dpwMciDkh2OmEAKxEuVFcIhdREZEtqnqs+zwCeEJVTwg9ISWqfXm1KYseCc3gys0ZGFEn9PQFH1UvQp+qOQmNWIaziEp/ZQwbHqRHWOxf+LieQJgQgDez0BYzMYZLnBgcLyKd7vUBcFzus4h0xpxrJCQsf2AoiRxWncgYKnFlz+pVdax77aeqI3yfx5bKyGqnbUYL82Z+XGCkXmRYEz8s5dgYCoXUMzCKiH8uQnNTA7v39PQXEokqKNLUUMe4T4zijY4u6kQCj7WUY2MomBiUgfypyO99FF+0NIcCv1s0N/A6YCnHxtAxMSgDhax7kE9Xd1//Z0s5NoqJiUEZKOaY3lKOjWKRfFVOo2gMZ0w/rsmWqzDSwcSgDEStexBFQ71ww5eOScEiwzAxKAu5dQ8OjFngBLwMRHHvN59/vA0JjNQwn0GZOGXqBJpG1dM00nu9vXvfoGNyKyMZRimwnkEZyJ+GfP1ZR1tVIqPsWM+gxETVI7AQoVFOTAxKSJQQWIjQKDcmBikQVPY8aakywygXJgZFJmjVo0XLn2JsYwO79/aYEBiZxcSgyASlGu/p6WPvB3tZduXJJgRGZklNDETkTuBsYKeqfsq1jQeWAZOBbcAFqvpeWjaUgvwhQVCVY/AmGJkQGFkmzdDiXcCZeW2LgDWqegSwxm1XLEErIUnIsba0uZF1UhMDVX0UeDev+Vy8Iqu497a0vr8UBA0JgioRWM6AUQmU2mdwgKq+6T7vAA4IO1BEFgALAFpbW0tgWuFEzT7MlS1rsZwBo0IomwNRVVVEQkv6qOoSYAl41ZFLZlgBhPkIBMxZaFQcpU5HfktEDgJw7ztL/P1FJWz24VVzp5kQGBVHqcVgFXCp+3wp8FCJv7+o5M8+FODqudO49vPmHzAqjzRDi/cApwETReR14AZgMXCfiMwHtgMXpPX9pcI/+9ASioxKJjUxUNWLQ3Z9Nq3vLDW2CKpRTdgU5iFiQmBUGyYGQ8CEwKhGTAwKxITAqFZMDArAhMCoZkwMEmJCYFQ7JgYJMCEwagETgxhMCIxawcQgAhMCo5YwMQjBhMCoNUwMAjAhMGoRE4M8TAiMWsXEwIcJgVHLmBg4TAiMWsfEABMCwwATAxMCw3DUtBiYEBjGx9SsGJgQGMZAalIMTAgMYzA1JwYmBIYRTE2JgQmBYYRTM2JgQmAY0dSEGJgQGEY8VS8GJgSGkYyqFgMTAsNITtWKgQmBYRRGVYqBCYFhFE7ViYEJgWEMjaoSAxMCwxg6VSMGJgSGMTyqQgxMCAxj+FS8GJgQGEZxqGgxMCEwjOJRsWJgQmAYxaUixcCEwDCKT8WJgQmBYaRDRYmBCYFhpEfFiIEJgWGkS1nEQETOFJHnReRPIrIo7vieXjUhMIyUKbkYiEg98M/AF4CjgYtF5Oioc15+e7cJgWGkTDl6BicCf1LVl1V1H3AvcG7UCd29akJgGCkzogzf2QK85tt+HTgp/yARWQAscJt7Tzp8wtMlsK3YTATeLrcRQ6AS7a5Em6H0dh8WtqMcYpAIVV0CLAEQkfWqOqvMJhWM2V06KtFmyJbd5RgmtAOH+rYPcW2GYZSRcojBH4EjRGSKiIwELgJWlcEOwzB8lHyYoKo9InIVsBqoB+5U1a0xpy1J37JUMLtLRyXaDBmyW1S13DYYhpEBKiYD0TCMdDExMAwDyLgYFJq2XE5E5E4R2SkiT/vaxovIL0XkRfc+rpw25iMih4rIOhF5RkS2isjXXXvW7R4tIk+IyJPO7u+49iki8rj7f1nmHNSZQkTqRWSTiDzstjNjc2bFYChpy2XmLuDMvLZFwBpVPQJY47azRA9wraoeDcwGvub+xlm3ey8wV1WPB6YDZ4rIbOD7wC2qOg14D5hfRhvD+DrwrG87MzZnVgwYQtpyOVHVR4F385rPBZa6z0uBtpIaFYOqvqmqG93nD/D+SVvIvt2qqrvdZoN7KTAXeMC1Z85uETkEOAu43W0LGbI5y2IQlLbcUiZbhsoBqvqm+7wDOKCcxkQhIpOBGcDjVIDdrru9GdgJ/BJ4CehQ1R53SBb/X24F/groc9sTyJDNWRaDqkK9GG4m47giMgZYDnxDVTv9+7Jqt6r2qup0vAzWE4GjymxSJCJyNrBTVTeU25YwMjs3gepIW35LRA5S1TdF5CC8p1imEJEGPCH4qaqucM2ZtzuHqnaIyDrgZKBZREa4J23W/l/mAOeIyBeB0cBY4B/JkM1Z7hlUQ9ryKuBS9/lS4KEy2jIIN2a9A3hWVX/o25V1uyeJSLP73Ah8Ds/fsQ443x2WKbtV9TpVPURVJ+P9L69V1UvIks2qmtkX8EXgBbzx4N+U254YW+8B3gS68cZ+8/HGhGuAF4FfAePLbWeezZ/BGwI8BWx2ry9WgN3HAZuc3U8D33bthwNPAH8C7gdGldvWEPtPAx7Oms2WjmwYBpDtYYJhGCXExMAwDMDEwDAMh4mBYRiAiYFhGA4TA8MwABODTCIiE0Rks3vtEJF23/awp7iKyA0iclNe23QReTbinBtF5C+H+90R198mIltEZJbb/rWIvOoSo3LHrBSR3e7zZBHpcn+TZ0TkxyJS5/YdISIPi8hLIrLBTdM+1e270E0Xfjit31KpmBhkEFV9R1Wnq5d7/2O8Ka7T3WufiAw3jfwe4MK8totcezk5XVXX+7Y78NJ4cRmHB+Ud/5L7Gx2HN829TURGA48AS1R1qqrOBK7GS+5BVZcB/z3dn1GZmBhUCCJyl3v6PQ787/wntYg87WYeIiJfccU/NovIv7jaEP2o6gvAeyLiX7zmAuAeEblCRP7oCocsF5GmAFt+7XuCTxSRbe5zvYjc7M5/SkSudO0Hicijzp6nReS/JPzZ9+KJFMB5wIqgg9TL638MmAZcAvxeVVf59j+tqncl/M6axcSgsjgEOEVVvxl2gIj8Z7yn/hz31OzFu0HyuQd3o7nCIO+q6ovAClX9tHqFQ56lsGIb84H3VfXTwKeBK0RkCvDfgNXOnuPx0p6TsAY41YnZRcCyoIOcYH0W2AIcA2wswGbDkeVZi8Zg7lfV3phjPgvMBP7ohtuNBM86XAY8JiLXMnCI8CkR+R7QDIzBK2mflM8Dx4lIbuLNJ4Ej8Cad3elmSK5U1aRi0Av81tnXqKrbfC4EgKmupoECD6nqL0Tkc/4DRORBZ8MLqnpeAb+l5jAxqCw+9H3uYWDPbrR7F2Cpql4XdSFVfU1EXgH+DJiHNwUYvPJtbar6pIhchjepJh//d4/2tQtwtaoOEhDnwDsLuEtEfqiqP4myz8e9wIPAjQH7cj4DP1uBU3MbqvplN6T5QcLvq1lsmFC5bANOABCRE4Aprn0NcL6I7O/2jReRsMU27wFuAV5W1ddd237Am+4pHjS8yH33TPf5fF/7auAv3LmIyH8SkU+4739LVW/DK/l1QgG/8z+Am0ju3PwZMEdEztH8nWMAAADHSURBVPG1DfJ7GIOxnkHlshz4cxHZileq7AUAVX1GRK4H/p8LtXUDXwO2B1zjfuCf8LztOf7WXW+Xe98v4LwfAPeJt1L2I77224HJwEYXEtyFV9PvNGChiHQDu4E/T/oj1ZtWm/iprqpdrqrQD0XkVuAt4APge0mvUavYFGYjE7iIxCxVTX15chE5DfhLVT077e+qJGyYYGSFXcCaXMgyLUTkQuD/4JUlN3xYz8AwDMB6BoZhOEwMDMMATAwMw3CYGBiGAcD/B96GeOnJsoA8AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "조금이지만 직선에 가까운 형태로 모델이 학습 됨"
      ],
      "metadata": {
        "id": "KTwBLudManlh"
      }
    }
  ]
}