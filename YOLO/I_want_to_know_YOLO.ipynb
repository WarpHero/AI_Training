{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHZWBvcFfadA",
        "outputId": "0a05d7f6-a49e-4c17-d066-24a706193554"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.24-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.10-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.24-py3-none-any.whl (877 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.7/877.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.10-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.24 ultralytics-thop-2.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dIUMv2XfHds",
        "outputId": "76a6f8de-3f65-4483-ce1a-f2023fa606c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(YOLO)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0o_QT1dofOy2",
        "outputId": "d4ea4083-9f75-45d3-aa23-8d7938890772"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class YOLO in module ultralytics.models.yolo.model:\n",
            "\n",
            "class YOLO(ultralytics.engine.model.Model)\n",
            " |  YOLO(model='yolo11n.pt', task=None, verbose=False)\n",
            " |  \n",
            " |  YOLO (You Only Look Once) object detection model.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      YOLO\n",
            " |      ultralytics.engine.model.Model\n",
            " |      torch.nn.modules.module.Module\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, model='yolo11n.pt', task=None, verbose=False)\n",
            " |      Initialize YOLO model, switching to YOLOWorld if model filename contains '-world'.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  task_map\n",
            " |      Map head to model, trainer, validator, and predictor classes.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from ultralytics.engine.model.Model:\n",
            " |  \n",
            " |  __call__(self, source: Union[str, pathlib.Path, int, PIL.Image.Image, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, **kwargs) -> list\n",
            " |      Alias for the predict method, enabling the model instance to be callable for predictions.\n",
            " |      \n",
            " |      This method simplifies the process of making predictions by allowing the model instance to be called\n",
            " |      directly with the required arguments.\n",
            " |      \n",
            " |      Args:\n",
            " |          source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): The source of\n",
            " |              the image(s) to make predictions on. Can be a file path, URL, PIL image, numpy array, PyTorch\n",
            " |              tensor, or a list/tuple of these.\n",
            " |          stream (bool): If True, treat the input source as a continuous stream for predictions.\n",
            " |          **kwargs (Any): Additional keyword arguments to configure the prediction process.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (List[ultralytics.engine.results.Results]): A list of prediction results, each encapsulated in a\n",
            " |              Results object.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> results = model(\"https://ultralytics.com/images/bus.jpg\")\n",
            " |          >>> for r in results:\n",
            " |          ...     print(f\"Detected {len(r)} objects in image\")\n",
            " |  \n",
            " |  add_callback(self, event: str, func) -> None\n",
            " |      Adds a callback function for a specified event.\n",
            " |      \n",
            " |      This method allows registering custom callback functions that are triggered on specific events during\n",
            " |      model operations such as training or inference. Callbacks provide a way to extend and customize the\n",
            " |      behavior of the model at various stages of its lifecycle.\n",
            " |      \n",
            " |      Args:\n",
            " |          event (str): The name of the event to attach the callback to. Must be a valid event name recognized\n",
            " |              by the Ultralytics framework.\n",
            " |          func (Callable): The callback function to be registered. This function will be called when the\n",
            " |              specified event occurs.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the event name is not recognized or is invalid.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> def on_train_start(trainer):\n",
            " |          ...     print(\"Training is starting!\")\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> model.add_callback(\"on_train_start\", on_train_start)\n",
            " |          >>> model.train(data=\"coco8.yaml\", epochs=1)\n",
            " |  \n",
            " |  benchmark(self, **kwargs)\n",
            " |      Benchmarks the model across various export formats to evaluate performance.\n",
            " |      \n",
            " |      This method assesses the model's performance in different export formats, such as ONNX, TorchScript, etc.\n",
            " |      It uses the 'benchmark' function from the ultralytics.utils.benchmarks module. The benchmarking is\n",
            " |      configured using a combination of default configuration values, model-specific arguments, method-specific\n",
            " |      defaults, and any additional user-provided keyword arguments.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs (Any): Arbitrary keyword arguments to customize the benchmarking process. These are combined with\n",
            " |              default configurations, model-specific arguments, and method defaults. Common options include:\n",
            " |              - data (str): Path to the dataset for benchmarking.\n",
            " |              - imgsz (int | List[int]): Image size for benchmarking.\n",
            " |              - half (bool): Whether to use half-precision (FP16) mode.\n",
            " |              - int8 (bool): Whether to use int8 precision mode.\n",
            " |              - device (str): Device to run the benchmark on (e.g., 'cpu', 'cuda').\n",
            " |              - verbose (bool): Whether to print detailed benchmark information.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (Dict): A dictionary containing the results of the benchmarking process, including metrics for\n",
            " |              different export formats.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> results = model.benchmark(data=\"coco8.yaml\", imgsz=640, half=True)\n",
            " |          >>> print(results)\n",
            " |  \n",
            " |  clear_callback(self, event: str) -> None\n",
            " |      Clears all callback functions registered for a specified event.\n",
            " |      \n",
            " |      This method removes all custom and default callback functions associated with the given event.\n",
            " |      It resets the callback list for the specified event to an empty list, effectively removing all\n",
            " |      registered callbacks for that event.\n",
            " |      \n",
            " |      Args:\n",
            " |          event (str): The name of the event for which to clear the callbacks. This should be a valid event name\n",
            " |              recognized by the Ultralytics callback system.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> model.add_callback(\"on_train_start\", lambda: print(\"Training started\"))\n",
            " |          >>> model.clear_callback(\"on_train_start\")\n",
            " |          >>> # All callbacks for 'on_train_start' are now removed\n",
            " |      \n",
            " |      Notes:\n",
            " |          - This method affects both custom callbacks added by the user and default callbacks\n",
            " |            provided by the Ultralytics framework.\n",
            " |          - After calling this method, no callbacks will be executed for the specified event\n",
            " |            until new ones are added.\n",
            " |          - Use with caution as it removes all callbacks, including essential ones that might\n",
            " |            be required for proper functioning of certain operations.\n",
            " |  \n",
            " |  embed(self, source: Union[str, pathlib.Path, int, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, **kwargs) -> list\n",
            " |      Generates image embeddings based on the provided source.\n",
            " |      \n",
            " |      This method is a wrapper around the 'predict()' method, focusing on generating embeddings from an image\n",
            " |      source. It allows customization of the embedding process through various keyword arguments.\n",
            " |      \n",
            " |      Args:\n",
            " |          source (str | Path | int | List | Tuple | np.ndarray | torch.Tensor): The source of the image for\n",
            " |              generating embeddings. Can be a file path, URL, PIL image, numpy array, etc.\n",
            " |          stream (bool): If True, predictions are streamed.\n",
            " |          **kwargs (Any): Additional keyword arguments for configuring the embedding process.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (List[torch.Tensor]): A list containing the image embeddings.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> image = \"https://ultralytics.com/images/bus.jpg\"\n",
            " |          >>> embeddings = model.embed(image)\n",
            " |          >>> print(embeddings[0].shape)\n",
            " |  \n",
            " |  export(self, **kwargs) -> str\n",
            " |      Exports the model to a different format suitable for deployment.\n",
            " |      \n",
            " |      This method facilitates the export of the model to various formats (e.g., ONNX, TorchScript) for deployment\n",
            " |      purposes. It uses the 'Exporter' class for the export process, combining model-specific overrides, method\n",
            " |      defaults, and any additional arguments provided.\n",
            " |      \n",
            " |      Args:\n",
            " |          **kwargs (Dict): Arbitrary keyword arguments to customize the export process. These are combined with\n",
            " |              the model's overrides and method defaults. Common arguments include:\n",
            " |              format (str): Export format (e.g., 'onnx', 'engine', 'coreml').\n",
            " |              half (bool): Export model in half-precision.\n",
            " |              int8 (bool): Export model in int8 precision.\n",
            " |              device (str): Device to run the export on.\n",
            " |              workspace (int): Maximum memory workspace size for TensorRT engines.\n",
            " |              nms (bool): Add Non-Maximum Suppression (NMS) module to model.\n",
            " |              simplify (bool): Simplify ONNX model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (str): The path to the exported model file.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |          ValueError: If an unsupported export format is specified.\n",
            " |          RuntimeError: If the export process fails due to errors.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> model.export(format=\"onnx\", dynamic=True, simplify=True)\n",
            " |          'path/to/exported/model.onnx'\n",
            " |  \n",
            " |  fuse(self)\n",
            " |      Fuses Conv2d and BatchNorm2d layers in the model for optimized inference.\n",
            " |      \n",
            " |      This method iterates through the model's modules and fuses consecutive Conv2d and BatchNorm2d layers\n",
            " |      into a single layer. This fusion can significantly improve inference speed by reducing the number of\n",
            " |      operations and memory accesses required during forward passes.\n",
            " |      \n",
            " |      The fusion process typically involves folding the BatchNorm2d parameters (mean, variance, weight, and\n",
            " |      bias) into the preceding Conv2d layer's weights and biases. This results in a single Conv2d layer that\n",
            " |      performs both convolution and normalization in one step.\n",
            " |      \n",
            " |      Raises:\n",
            " |          TypeError: If the model is not a PyTorch nn.Module.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = Model(\"yolo11n.pt\")\n",
            " |          >>> model.fuse()\n",
            " |          >>> # Model is now fused and ready for optimized inference\n",
            " |  \n",
            " |  info(self, detailed: bool = False, verbose: bool = True)\n",
            " |      Logs or returns model information.\n",
            " |      \n",
            " |      This method provides an overview or detailed information about the model, depending on the arguments\n",
            " |      passed. It can control the verbosity of the output and return the information as a list.\n",
            " |      \n",
            " |      Args:\n",
            " |          detailed (bool): If True, shows detailed information about the model layers and parameters.\n",
            " |          verbose (bool): If True, prints the information. If False, returns the information as a list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (List[str]): A list of strings containing various types of information about the model, including\n",
            " |              model summary, layer details, and parameter counts. Empty if verbose is True.\n",
            " |      \n",
            " |      Raises:\n",
            " |          TypeError: If the model is not a PyTorch model.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = Model(\"yolo11n.pt\")\n",
            " |          >>> model.info()  # Prints model summary\n",
            " |          >>> info_list = model.info(detailed=True, verbose=False)  # Returns detailed info as a list\n",
            " |  \n",
            " |  load(self, weights: Union[str, pathlib.Path] = 'yolo11n.pt') -> 'Model'\n",
            " |      Loads parameters from the specified weights file into the model.\n",
            " |      \n",
            " |      This method supports loading weights from a file or directly from a weights object. It matches parameters by\n",
            " |      name and shape and transfers them to the model.\n",
            " |      \n",
            " |      Args:\n",
            " |          weights (Union[str, Path]): Path to the weights file or a weights object.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (Model): The instance of the class with loaded weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = Model()\n",
            " |          >>> model.load(\"yolo11n.pt\")\n",
            " |          >>> model.load(Path(\"path/to/weights.pt\"))\n",
            " |  \n",
            " |  predict(self, source: Union[str, pathlib.Path, int, PIL.Image.Image, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, predictor=None, **kwargs) -> List[ultralytics.engine.results.Results]\n",
            " |      Performs predictions on the given image source using the YOLO model.\n",
            " |      \n",
            " |      This method facilitates the prediction process, allowing various configurations through keyword arguments.\n",
            " |      It supports predictions with custom predictors or the default predictor method. The method handles different\n",
            " |      types of image sources and can operate in a streaming mode.\n",
            " |      \n",
            " |      Args:\n",
            " |          source (str | Path | int | PIL.Image | np.ndarray | torch.Tensor | List | Tuple): The source\n",
            " |              of the image(s) to make predictions on. Accepts various types including file paths, URLs, PIL\n",
            " |              images, numpy arrays, and torch tensors.\n",
            " |          stream (bool): If True, treats the input source as a continuous stream for predictions.\n",
            " |          predictor (BasePredictor | None): An instance of a custom predictor class for making predictions.\n",
            " |              If None, the method uses a default predictor.\n",
            " |          **kwargs (Any): Additional keyword arguments for configuring the prediction process.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (List[ultralytics.engine.results.Results]): A list of prediction results, each encapsulated in a\n",
            " |              Results object.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> results = model.predict(source=\"path/to/image.jpg\", conf=0.25)\n",
            " |          >>> for r in results:\n",
            " |          ...     print(r.boxes.data)  # print detection bounding boxes\n",
            " |      \n",
            " |      Notes:\n",
            " |          - If 'source' is not provided, it defaults to the ASSETS constant with a warning.\n",
            " |          - The method sets up a new predictor if not already present and updates its arguments with each call.\n",
            " |          - For SAM-type models, 'prompts' can be passed as a keyword argument.\n",
            " |  \n",
            " |  reset_callbacks(self) -> None\n",
            " |      Resets all callbacks to their default functions.\n",
            " |      \n",
            " |      This method reinstates the default callback functions for all events, removing any custom callbacks that were\n",
            " |      previously added. It iterates through all default callback events and replaces the current callbacks with the\n",
            " |      default ones.\n",
            " |      \n",
            " |      The default callbacks are defined in the 'callbacks.default_callbacks' dictionary, which contains predefined\n",
            " |      functions for various events in the model's lifecycle, such as on_train_start, on_epoch_end, etc.\n",
            " |      \n",
            " |      This method is useful when you want to revert to the original set of callbacks after making custom\n",
            " |      modifications, ensuring consistent behavior across different runs or experiments.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> model.add_callback(\"on_train_start\", custom_function)\n",
            " |          >>> model.reset_callbacks()\n",
            " |          # All callbacks are now reset to their default functions\n",
            " |  \n",
            " |  reset_weights(self) -> 'Model'\n",
            " |      Resets the model's weights to their initial state.\n",
            " |      \n",
            " |      This method iterates through all modules in the model and resets their parameters if they have a\n",
            " |      'reset_parameters' method. It also ensures that all parameters have 'requires_grad' set to True,\n",
            " |      enabling them to be updated during training.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (Model): The instance of the class with reset weights.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = Model(\"yolo11n.pt\")\n",
            " |          >>> model.reset_weights()\n",
            " |  \n",
            " |  save(self, filename: Union[str, pathlib.Path] = 'saved_model.pt') -> None\n",
            " |      Saves the current model state to a file.\n",
            " |      \n",
            " |      This method exports the model's checkpoint (ckpt) to the specified filename. It includes metadata such as\n",
            " |      the date, Ultralytics version, license information, and a link to the documentation.\n",
            " |      \n",
            " |      Args:\n",
            " |          filename (Union[str, Path]): The name of the file to save the model to.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = Model(\"yolo11n.pt\")\n",
            " |          >>> model.save(\"my_model.pt\")\n",
            " |  \n",
            " |  track(self, source: Union[str, pathlib.Path, int, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, persist: bool = False, **kwargs) -> List[ultralytics.engine.results.Results]\n",
            " |      Conducts object tracking on the specified input source using the registered trackers.\n",
            " |      \n",
            " |      This method performs object tracking using the model's predictors and optionally registered trackers. It handles\n",
            " |      various input sources such as file paths or video streams, and supports customization through keyword arguments.\n",
            " |      The method registers trackers if not already present and can persist them between calls.\n",
            " |      \n",
            " |      Args:\n",
            " |          source (Union[str, Path, int, List, Tuple, np.ndarray, torch.Tensor], optional): Input source for object\n",
            " |              tracking. Can be a file path, URL, or video stream.\n",
            " |          stream (bool): If True, treats the input source as a continuous video stream. Defaults to False.\n",
            " |          persist (bool): If True, persists trackers between different calls to this method. Defaults to False.\n",
            " |          **kwargs (Any): Additional keyword arguments for configuring the tracking process.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (List[ultralytics.engine.results.Results]): A list of tracking results, each a Results object.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the predictor does not have registered trackers.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> results = model.track(source=\"path/to/video.mp4\", show=True)\n",
            " |          >>> for r in results:\n",
            " |          ...     print(r.boxes.id)  # print tracking IDs\n",
            " |      \n",
            " |      Notes:\n",
            " |          - This method sets a default confidence threshold of 0.1 for ByteTrack-based tracking.\n",
            " |          - The tracking mode is explicitly set in the keyword arguments.\n",
            " |          - Batch size is set to 1 for tracking in videos.\n",
            " |  \n",
            " |  train(self, trainer=None, **kwargs)\n",
            " |      Trains the model using the specified dataset and training configuration.\n",
            " |      \n",
            " |      This method facilitates model training with a range of customizable settings. It supports training with a\n",
            " |      custom trainer or the default training approach. The method handles scenarios such as resuming training\n",
            " |      from a checkpoint, integrating with Ultralytics HUB, and updating model and configuration after training.\n",
            " |      \n",
            " |      When using Ultralytics HUB, if the session has a loaded model, the method prioritizes HUB training\n",
            " |      arguments and warns if local arguments are provided. It checks for pip updates and combines default\n",
            " |      configurations, method-specific defaults, and user-provided arguments to configure the training process.\n",
            " |      \n",
            " |      Args:\n",
            " |          trainer (BaseTrainer | None): Custom trainer instance for model training. If None, uses default.\n",
            " |          **kwargs (Any): Arbitrary keyword arguments for training configuration. Common options include:\n",
            " |              data (str): Path to dataset configuration file.\n",
            " |              epochs (int): Number of training epochs.\n",
            " |              batch_size (int): Batch size for training.\n",
            " |              imgsz (int): Input image size.\n",
            " |              device (str): Device to run training on (e.g., 'cuda', 'cpu').\n",
            " |              workers (int): Number of worker threads for data loading.\n",
            " |              optimizer (str): Optimizer to use for training.\n",
            " |              lr0 (float): Initial learning rate.\n",
            " |              patience (int): Epochs to wait for no observable improvement for early stopping of training.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (Dict | None): Training metrics if available and training is successful; otherwise, None.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |          PermissionError: If there is a permission issue with the HUB session.\n",
            " |          ModuleNotFoundError: If the HUB SDK is not installed.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> results = model.train(data=\"coco8.yaml\", epochs=3)\n",
            " |  \n",
            " |  tune(self, use_ray=False, iterations=10, *args, **kwargs)\n",
            " |      Conducts hyperparameter tuning for the model, with an option to use Ray Tune.\n",
            " |      \n",
            " |      This method supports two modes of hyperparameter tuning: using Ray Tune or a custom tuning method.\n",
            " |      When Ray Tune is enabled, it leverages the 'run_ray_tune' function from the ultralytics.utils.tuner module.\n",
            " |      Otherwise, it uses the internal 'Tuner' class for tuning. The method combines default, overridden, and\n",
            " |      custom arguments to configure the tuning process.\n",
            " |      \n",
            " |      Args:\n",
            " |          use_ray (bool): If True, uses Ray Tune for hyperparameter tuning. Defaults to False.\n",
            " |          iterations (int): The number of tuning iterations to perform. Defaults to 10.\n",
            " |          *args (List): Variable length argument list for additional arguments.\n",
            " |          **kwargs (Dict): Arbitrary keyword arguments. These are combined with the model's overrides and defaults.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (Dict): A dictionary containing the results of the hyperparameter search.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> results = model.tune(use_ray=True, iterations=20)\n",
            " |          >>> print(results)\n",
            " |  \n",
            " |  val(self, validator=None, **kwargs)\n",
            " |      Validates the model using a specified dataset and validation configuration.\n",
            " |      \n",
            " |      This method facilitates the model validation process, allowing for customization through various settings. It\n",
            " |      supports validation with a custom validator or the default validation approach. The method combines default\n",
            " |      configurations, method-specific defaults, and user-provided arguments to configure the validation process.\n",
            " |      \n",
            " |      Args:\n",
            " |          validator (ultralytics.engine.validator.BaseValidator | None): An instance of a custom validator class for\n",
            " |              validating the model.\n",
            " |          **kwargs (Any): Arbitrary keyword arguments for customizing the validation process.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (ultralytics.utils.metrics.DetMetrics): Validation metrics obtained from the validation process.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AssertionError: If the model is not a PyTorch model.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> results = model.val(data=\"coco8.yaml\", imgsz=640)\n",
            " |          >>> print(results.box.map)  # Print mAP50-95\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from ultralytics.engine.model.Model:\n",
            " |  \n",
            " |  is_hub_model(model: str) -> bool\n",
            " |      Check if the provided model is an Ultralytics HUB model.\n",
            " |      \n",
            " |      This static method determines whether the given model string represents a valid Ultralytics HUB model\n",
            " |      identifier.\n",
            " |      \n",
            " |      Args:\n",
            " |          model (str): The model string to check.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (bool): True if the model is a valid Ultralytics HUB model, False otherwise.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> Model.is_hub_model(\"https://hub.ultralytics.com/models/MODEL\")\n",
            " |          True\n",
            " |          >>> Model.is_hub_model(\"yolo11n.pt\")\n",
            " |          False\n",
            " |  \n",
            " |  is_triton_model(model: str) -> bool\n",
            " |      Checks if the given model string is a Triton Server URL.\n",
            " |      \n",
            " |      This static method determines whether the provided model string represents a valid Triton Server URL by\n",
            " |      parsing its components using urllib.parse.urlsplit().\n",
            " |      \n",
            " |      Args:\n",
            " |          model (str): The model string to be checked.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (bool): True if the model string is a valid Triton Server URL, False otherwise.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> Model.is_triton_model(\"http://localhost:8000/v2/models/yolov8n\")\n",
            " |          True\n",
            " |          >>> Model.is_triton_model(\"yolo11n.pt\")\n",
            " |          False\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from ultralytics.engine.model.Model:\n",
            " |  \n",
            " |  device\n",
            " |      Retrieves the device on which the model's parameters are allocated.\n",
            " |      \n",
            " |      This property determines the device (CPU or GPU) where the model's parameters are currently stored. It is\n",
            " |      applicable only to models that are instances of nn.Module.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (torch.device): The device (CPU/GPU) of the model.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the model is not a PyTorch nn.Module instance.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> print(model.device)\n",
            " |          device(type='cuda', index=0)  # if CUDA is available\n",
            " |          >>> model = model.to(\"cpu\")\n",
            " |          >>> print(model.device)\n",
            " |          device(type='cpu')\n",
            " |  \n",
            " |  names\n",
            " |      Retrieves the class names associated with the loaded model.\n",
            " |      \n",
            " |      This property returns the class names if they are defined in the model. It checks the class names for validity\n",
            " |      using the 'check_class_names' function from the ultralytics.nn.autobackend module. If the predictor is not\n",
            " |      initialized, it sets it up before retrieving the names.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (Dict[int, str]): A dict of class names associated with the model.\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the model or predictor does not have a 'names' attribute.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> print(model.names)\n",
            " |          {0: 'person', 1: 'bicycle', 2: 'car', ...}\n",
            " |  \n",
            " |  transforms\n",
            " |      Retrieves the transformations applied to the input data of the loaded model.\n",
            " |      \n",
            " |      This property returns the transformations if they are defined in the model. The transforms\n",
            " |      typically include preprocessing steps like resizing, normalization, and data augmentation\n",
            " |      that are applied to input data before it is fed into the model.\n",
            " |      \n",
            " |      Returns:\n",
            " |          (object | None): The transform object of the model if available, otherwise None.\n",
            " |      \n",
            " |      Examples:\n",
            " |          >>> model = YOLO(\"yolo11n.pt\")\n",
            " |          >>> transforms = model.transforms\n",
            " |          >>> if transforms:\n",
            " |          ...     print(f\"Model transforms: {transforms}\")\n",
            " |          ... else:\n",
            " |          ...     print(\"No transforms defined for this model.\")\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __dir__(self)\n",
            " |      Default dir() implementation.\n",
            " |  \n",
            " |  __getattr__(self, name: str) -> Any\n",
            " |      # On the return type:\n",
            " |      # We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\n",
            " |      # This is done for better interop with various type checkers for the end users.\n",
            " |      # Having a stricter return type doesn't play nicely with `register_buffer()` and forces\n",
            " |      # people to excessively use type-ignores, asserts, casts, etc.\n",
            " |      # See full discussion on the problems with returning `Union` here\n",
            " |      # https://github.com/microsoft/pyright/issues/4213\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
            " |      Add a child module to the current module.\n",
            " |      \n",
            " |      The module can be accessed as an attribute using the given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the child module. The child module can be\n",
            " |              accessed from this module using the given name\n",
            " |          module (Module): child module to be added to the module.\n",
            " |  \n",
            " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
            " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
            " |      \n",
            " |      Typical use includes initializing the parameters of a model\n",
            " |      (see also :ref:`nn-init-doc`).\n",
            " |      \n",
            " |      Args:\n",
            " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> @torch.no_grad()\n",
            " |          >>> def init_weights(m):\n",
            " |          >>>     print(m)\n",
            " |          >>>     if type(m) == nn.Linear:\n",
            " |          >>>         m.weight.fill_(1.0)\n",
            " |          >>>         print(m.weight)\n",
            " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
            " |          >>> net.apply(init_weights)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[1., 1.],\n",
            " |                  [1., 1.]], requires_grad=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[1., 1.],\n",
            " |                  [1., 1.]], requires_grad=True)\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |  \n",
            " |  bfloat16(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
            " |      Return an iterator over module buffers.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          torch.Tensor: module buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for buf in model.buffers():\n",
            " |          >>>     print(type(buf), buf.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  children(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Return an iterator over immediate children modules.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a child module\n",
            " |  \n",
            " |  compile(self, *args, **kwargs)\n",
            " |      Compile this Module's forward using :func:`torch.compile`.\n",
            " |      \n",
            " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
            " |      to :func:`torch.compile`.\n",
            " |      \n",
            " |      See :func:`torch.compile` for details on the arguments for this function.\n",
            " |  \n",
            " |  cpu(self: ~T) -> ~T\n",
            " |      Move all model parameters and buffers to the CPU.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the GPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on GPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  double(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  eval(self: ~T) -> ~T\n",
            " |      Set the module in evaluation mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  extra_repr(self) -> str\n",
            " |      Set the extra representation of the module.\n",
            " |      \n",
            " |      To print customized extra information, you should re-implement\n",
            " |      this method in your own modules. Both single-line and multi-line\n",
            " |      strings are acceptable.\n",
            " |  \n",
            " |  float(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  forward = _forward_unimplemented(self, *input: Any) -> None\n",
            " |      Define the computation performed at every call.\n",
            " |      \n",
            " |      Should be overridden by all subclasses.\n",
            " |      \n",
            " |      .. note::\n",
            " |          Although the recipe for forward pass needs to be defined within\n",
            " |          this function, one should call the :class:`Module` instance afterwards\n",
            " |          instead of this since the former takes care of running the\n",
            " |          registered hooks while the latter silently ignores them.\n",
            " |  \n",
            " |  get_buffer(self, target: str) -> 'Tensor'\n",
            " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the buffer\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.Tensor: The buffer referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not a\n",
            " |              buffer\n",
            " |  \n",
            " |  get_extra_state(self) -> Any\n",
            " |      Return any extra state to include in the module's state_dict.\n",
            " |      \n",
            " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
            " |      if you need to store extra state. This function is called when building the\n",
            " |      module's `state_dict()`.\n",
            " |      \n",
            " |      Note that extra state should be picklable to ensure working serialization\n",
            " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
            " |      for serializing Tensors; other objects may break backwards compatibility if\n",
            " |      their serialized pickled form changes.\n",
            " |      \n",
            " |      Returns:\n",
            " |          object: Any extra state to store in the module's state_dict\n",
            " |  \n",
            " |  get_parameter(self, target: str) -> 'Parameter'\n",
            " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the Parameter\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Parameter``\n",
            " |  \n",
            " |  get_submodule(self, target: str) -> 'Module'\n",
            " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
            " |      looks like this:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          A(\n",
            " |              (net_b): Module(\n",
            " |                  (net_c): Module(\n",
            " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
            " |                  )\n",
            " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
            " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
            " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
            " |      \n",
            " |      To check whether or not we have the ``linear`` submodule, we\n",
            " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
            " |      we have the ``conv`` submodule, we would call\n",
            " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
            " |      \n",
            " |      The runtime of ``get_submodule`` is bounded by the degree\n",
            " |      of module nesting in ``target``. A query against\n",
            " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
            " |      the number of transitive modules. So, for a simple check to see\n",
            " |      if some submodule exists, ``get_submodule`` should always be\n",
            " |      used.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the submodule\n",
            " |              to look for. (See above example for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Module: The submodule referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Module``\n",
            " |  \n",
            " |  half(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the IPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on IPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)\n",
            " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
            " |      \n",
            " |      If :attr:`strict` is ``True``, then\n",
            " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
            " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
            " |          the call to :attr:`load_state_dict` unless\n",
            " |          :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
            " |      \n",
            " |      Args:\n",
            " |          state_dict (dict): a dict containing parameters and\n",
            " |              persistent buffers.\n",
            " |          strict (bool, optional): whether to strictly enforce that the keys\n",
            " |              in :attr:`state_dict` match the keys returned by this module's\n",
            " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
            " |          assign (bool, optional): When ``False``, the properties of the tensors\n",
            " |              in the current module are preserved while when ``True``, the\n",
            " |              properties of the Tensors in the state dict are preserved. The only\n",
            " |              exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`s\n",
            " |              for which the value from the module is preserved.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
            " |              * **missing_keys** is a list of str containing any keys that are expected\n",
            " |                  by this module but missing from the provided ``state_dict``.\n",
            " |              * **unexpected_keys** is a list of str containing the keys that are not\n",
            " |                  expected by this module but present in the provided ``state_dict``.\n",
            " |      \n",
            " |      Note:\n",
            " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
            " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
            " |          ``RuntimeError``.\n",
            " |  \n",
            " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Return an iterator over all modules in the network.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a module in the network\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.modules()):\n",
            " |          ...     print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
            " |  \n",
            " |  mtia(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the MTIA.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on MTIA while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
            " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all buffer names.\n",
            " |          recurse (bool, optional): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module. Defaults to True.\n",
            " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, buf in self.named_buffers():\n",
            " |          >>>     if name in ['running_var']:\n",
            " |          >>>         print(buf.size())\n",
            " |  \n",
            " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
            " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Module): Tuple containing a name and child module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, module in model.named_children():\n",
            " |          >>>     if name in ['conv4', 'conv5']:\n",
            " |          >>>         print(module)\n",
            " |  \n",
            " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
            " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          memo: a memo to store the set of modules already added to the result\n",
            " |          prefix: a prefix that will be added to the name of the module\n",
            " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
            " |              or not\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Module): Tuple of name and module\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.named_modules()):\n",
            " |          ...     print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> ('', Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          ))\n",
            " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
            " |  \n",
            " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
            " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all parameter names.\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
            " |              parameters in the result. Defaults to True.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (str, Parameter): Tuple containing the name and parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for name, param in self.named_parameters():\n",
            " |          >>>     if name in ['bias']:\n",
            " |          >>>         print(param.size())\n",
            " |  \n",
            " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
            " |      Return an iterator over module parameters.\n",
            " |      \n",
            " |      This is typically passed to an optimizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Parameter: module parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> for param in model.parameters():\n",
            " |          >>>     print(type(param), param.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward hook on the module.\n",
            " |      \n",
            " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
            " |      the behavior of this function will change in future versions.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
            " |      Add a buffer to the module.\n",
            " |      \n",
            " |      This is typically used to register a buffer that should not to be\n",
            " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
            " |      is not a parameter, but is part of the module's state. Buffers, by\n",
            " |      default, are persistent and will be saved alongside parameters. This\n",
            " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
            " |      only difference between a persistent buffer and a non-persistent buffer\n",
            " |      is that the latter will not be a part of this module's\n",
            " |      :attr:`state_dict`.\n",
            " |      \n",
            " |      Buffers can be accessed as attributes using given names.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the buffer. The buffer can be accessed\n",
            " |              from this module using the given name\n",
            " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
            " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
            " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
            " |          persistent (bool): whether the buffer is part of this module's\n",
            " |              :attr:`state_dict`.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
            " |  \n",
            " |  register_forward_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a forward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time after :func:`forward` has computed an output.\n",
            " |      \n",
            " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
            " |      the positional arguments given to the module. Keyword arguments won't be\n",
            " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
            " |      output. It can modify the input inplace but it will not have effect on\n",
            " |      forward since this is called after :func:`forward` is called. The hook\n",
            " |      should have the following signature::\n",
            " |      \n",
            " |          hook(module, args, output) -> None or modified output\n",
            " |      \n",
            " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
            " |      ``kwargs`` given to the forward function and be expected to return the\n",
            " |      output possibly modified. The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, args, kwargs, output) -> None or modified output\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user defined hook to be registered.\n",
            " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
            " |              before all existing ``forward`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
            " |              this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``forward`` hooks registered with\n",
            " |              :func:`register_module_forward_hook` will fire before all hooks\n",
            " |              registered by this method.\n",
            " |              Default: ``False``\n",
            " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
            " |              kwargs given to the forward function.\n",
            " |              Default: ``False``\n",
            " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
            " |              whether an exception is raised while calling the Module.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a forward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time before :func:`forward` is invoked.\n",
            " |      \n",
            " |      \n",
            " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
            " |      the positional arguments given to the module. Keyword arguments won't be\n",
            " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
            " |      input. User can either return a tuple or a single modified value in the\n",
            " |      hook. We will wrap the value into a tuple if a single value is returned\n",
            " |      (unless that value is already a tuple). The hook should have the\n",
            " |      following signature::\n",
            " |      \n",
            " |          hook(module, args) -> None or modified input\n",
            " |      \n",
            " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
            " |      kwargs given to the forward function. And if the hook modifies the\n",
            " |      input, both the args and kwargs should be returned. The hook should have\n",
            " |      the following signature::\n",
            " |      \n",
            " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``forward_pre`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
            " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``forward_pre`` hooks registered with\n",
            " |              :func:`register_module_forward_pre_hook` will fire before all\n",
            " |              hooks registered by this method.\n",
            " |              Default: ``False``\n",
            " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
            " |              given to the forward function.\n",
            " |              Default: ``False``\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients with respect to a module\n",
            " |      are computed, i.e. the hook will execute if and only if the gradients with\n",
            " |      respect to module outputs are computed. The hook should have the following\n",
            " |      signature::\n",
            " |      \n",
            " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
            " |      \n",
            " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
            " |      with respect to the inputs and outputs respectively. The hook should\n",
            " |      not modify its arguments, but it can optionally return a new gradient with\n",
            " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
            " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
            " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
            " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
            " |      arguments.\n",
            " |      \n",
            " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
            " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
            " |      of each Tensor returned by the Module's forward function.\n",
            " |      \n",
            " |      .. warning ::\n",
            " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
            " |          will raise an error.\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user-defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``backward`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
            " |              this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``backward`` hooks registered with\n",
            " |              :func:`register_module_full_backward_hook` will fire before\n",
            " |              all hooks registered by this method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
            " |      Register a backward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients for the module are computed.\n",
            " |      The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
            " |      \n",
            " |      The :attr:`grad_output` is a tuple. The hook should\n",
            " |      not modify its arguments, but it can optionally return a new gradient with\n",
            " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
            " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
            " |      all non-Tensor arguments.\n",
            " |      \n",
            " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
            " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
            " |      of each Tensor returned by the Module's forward function.\n",
            " |      \n",
            " |      .. warning ::\n",
            " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
            " |          will raise an error.\n",
            " |      \n",
            " |      Args:\n",
            " |          hook (Callable): The user-defined hook to be registered.\n",
            " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
            " |              all existing ``backward_pre`` hooks on this\n",
            " |              :class:`torch.nn.modules.Module`. Otherwise, the provided\n",
            " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
            " |              on this :class:`torch.nn.modules.Module`. Note that global\n",
            " |              ``backward_pre`` hooks registered with\n",
            " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
            " |              all hooks registered by this method.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_load_state_dict_post_hook(self, hook)\n",
            " |      Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, incompatible_keys) -> None\n",
            " |      \n",
            " |      The ``module`` argument is the current module that this hook is registered\n",
            " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
            " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
            " |      is a ``list`` of ``str`` containing the missing keys and\n",
            " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
            " |      \n",
            " |      The given incompatible_keys can be modified inplace if needed.\n",
            " |      \n",
            " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
            " |      ``strict=True`` are affected by modifications the hook makes to\n",
            " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
            " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
            " |      clearing out both missing and unexpected keys will avoid an error.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_load_state_dict_pre_hook(self, hook)\n",
            " |      Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\n",
            " |      \n",
            " |      Arguments:\n",
            " |          hook (Callable): Callable hook that will be invoked before\n",
            " |              loading the state dict.\n",
            " |  \n",
            " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
            " |      Alias for :func:`add_module`.\n",
            " |  \n",
            " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
            " |      Add a parameter to the module.\n",
            " |      \n",
            " |      The parameter can be accessed as an attribute using given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (str): name of the parameter. The parameter can be accessed\n",
            " |              from this module using the given name\n",
            " |          param (Parameter or None): parameter to be added to the module. If\n",
            " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
            " |              are ignored. If ``None``, the parameter is **not** included in the\n",
            " |              module's :attr:`state_dict`.\n",
            " |  \n",
            " |  register_state_dict_post_hook(self, hook)\n",
            " |      Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, state_dict, prefix, local_metadata) -> None\n",
            " |      \n",
            " |      The registered hooks can modify the ``state_dict`` inplace.\n",
            " |  \n",
            " |  register_state_dict_pre_hook(self, hook)\n",
            " |      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
            " |      \n",
            " |      It should have the following signature::\n",
            " |          hook(module, prefix, keep_vars) -> None\n",
            " |      \n",
            " |      The registered hooks can be used to perform pre-processing before the ``state_dict``\n",
            " |      call is made.\n",
            " |  \n",
            " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
            " |      Change if autograd should record operations on parameters in this module.\n",
            " |      \n",
            " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
            " |      in-place.\n",
            " |      \n",
            " |      This method is helpful for freezing part of the module for finetuning\n",
            " |      or training parts of a model individually (e.g., GAN training).\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Args:\n",
            " |          requires_grad (bool): whether autograd should record operations on\n",
            " |                                parameters in this module. Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  set_extra_state(self, state: Any) -> None\n",
            " |      Set extra state contained in the loaded `state_dict`.\n",
            " |      \n",
            " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
            " |      found within the `state_dict`. Implement this function and a corresponding\n",
            " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
            " |      `state_dict`.\n",
            " |      \n",
            " |      Args:\n",
            " |          state (dict): Extra state from the `state_dict`\n",
            " |  \n",
            " |  set_submodule(self, target: str, module: 'Module') -> None\n",
            " |      Set the submodule given by ``target`` if it exists, otherwise throw an error.\n",
            " |      \n",
            " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
            " |      looks like this:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          A(\n",
            " |              (net_b): Module(\n",
            " |                  (net_c): Module(\n",
            " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
            " |                  )\n",
            " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
            " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
            " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
            " |      \n",
            " |      To overide the ``Conv2d`` with a new submodule ``Linear``, you\n",
            " |      would call\n",
            " |      ``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the submodule\n",
            " |              to look for. (See above example for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |          module: The module to set the submodule to.\n",
            " |      \n",
            " |      Raises:\n",
            " |          ValueError: If the target string is empty\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Module``\n",
            " |  \n",
            " |  share_memory(self: ~T) -> ~T\n",
            " |      See :meth:`torch.Tensor.share_memory_`.\n",
            " |  \n",
            " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
            " |      Return a dictionary containing references to the whole state of the module.\n",
            " |      \n",
            " |      Both parameters and persistent buffers (e.g. running averages) are\n",
            " |      included. Keys are corresponding parameter and buffer names.\n",
            " |      Parameters and buffers set to ``None`` are not included.\n",
            " |      \n",
            " |      .. note::\n",
            " |          The returned object is a shallow copy. It contains references\n",
            " |          to the module's parameters and buffers.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          Currently ``state_dict()`` also accepts positional arguments for\n",
            " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
            " |          this is being deprecated and keyword arguments will be enforced in\n",
            " |          future releases.\n",
            " |      \n",
            " |      .. warning::\n",
            " |          Please avoid the use of argument ``destination`` as it is not\n",
            " |          designed for end-users.\n",
            " |      \n",
            " |      Args:\n",
            " |          destination (dict, optional): If provided, the state of module will\n",
            " |              be updated into the dict and the same object is returned.\n",
            " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
            " |              Default: ``None``.\n",
            " |          prefix (str, optional): a prefix added to parameter and buffer\n",
            " |              names to compose the keys in state_dict. Default: ``''``.\n",
            " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
            " |              returned in the state dict are detached from autograd. If it's\n",
            " |              set to ``True``, detaching will not be performed.\n",
            " |              Default: ``False``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          dict:\n",
            " |              a dictionary containing a whole state of the module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
            " |          >>> module.state_dict().keys()\n",
            " |          ['bias', 'weight']\n",
            " |  \n",
            " |  to(self, *args, **kwargs)\n",
            " |      Move and/or cast the parameters and buffers.\n",
            " |      \n",
            " |      This can be called as\n",
            " |      \n",
            " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(dtype, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(tensor, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(memory_format=torch.channels_last)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
            " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
            " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
            " |      (if given). The integral parameters and buffers will be moved\n",
            " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
            " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
            " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
            " |      pinned memory to CUDA devices.\n",
            " |      \n",
            " |      See below for examples.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): the desired device of the parameters\n",
            " |              and buffers in this module\n",
            " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
            " |              the parameters and buffers in this module\n",
            " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
            " |              dtype and device for all parameters and buffers in this module\n",
            " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
            " |              format for 4D parameters and buffers in this module (keyword\n",
            " |              only argument)\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
            " |          >>> linear = nn.Linear(2, 2)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]])\n",
            " |          >>> linear.to(torch.double)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
            " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
            " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
            " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
            " |          >>> cpu = torch.device(\"cpu\")\n",
            " |          >>> linear.to(cpu)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
            " |      \n",
            " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
            " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
            " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
            " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
            " |  \n",
            " |  to_empty(self: ~T, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T\n",
            " |      Move the parameters and buffers to the specified device without copying storage.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): The desired device of the parameters\n",
            " |              and buffers in this module.\n",
            " |          recurse (bool): Whether parameters and buffers of submodules should\n",
            " |              be recursively moved to the specified device.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
            " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          dst_type (type or string): the desired type\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Move all model parameters and buffers to the XPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on XPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  zero_grad(self, set_to_none: bool = True) -> None\n",
            " |      Reset gradients of all model parameters.\n",
            " |      \n",
            " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
            " |      \n",
            " |      Args:\n",
            " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
            " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  T_destination = ~T_destination\n",
            " |  \n",
            " |  call_super_init = False\n",
            " |  \n",
            " |  dump_patches = False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(YOLO))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdGKChypfnb2",
        "outputId": "ce312129-353f-44cd-bb7f-87c6c6ab263c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_call_impl', '_check_is_pytorch_model', '_compiled_call_impl', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_load', '_load_from_state_dict', '_maybe_warn_non_full_backward_hook', '_named_members', '_new', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_reset_ckpt_args', '_save_to_state_dict', '_slow_forward', '_smart_load', '_version', '_wrapped_call_impl', 'add_callback', 'add_module', 'apply', 'benchmark', 'bfloat16', 'buffers', 'call_super_init', 'children', 'clear_callback', 'compile', 'cpu', 'cuda', 'device', 'double', 'dump_patches', 'embed', 'eval', 'export', 'extra_repr', 'float', 'forward', 'fuse', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'info', 'ipu', 'is_hub_model', 'is_triton_model', 'load', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'names', 'parameters', 'predict', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_callbacks', 'reset_weights', 'save', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'task_map', 'to', 'to_empty', 'track', 'train', 'transforms', 'tune', 'type', 'val', 'xpu', 'zero_grad']\n"
          ]
        }
      ]
    }
  ]
}